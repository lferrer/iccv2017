I0403 15:11:36.833699 16844 caffe.cpp:211] Use CPU.
I0403 15:11:36.833950 16844 solver.cpp:44] Initializing solver from parameters: 
train_net: "mini_train_test.prototxt"
base_lr: 0.0001
display: 20
max_iter: 20000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 5000
snapshot: 5000
snapshot_prefix: "c3d_ucf101_finetune_whole"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I0403 15:11:36.834056 16844 solver.cpp:77] Creating training net from train_net file: mini_train_test.prototxt
I0403 15:11:36.834506 16844 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0403 15:11:36.834813 16844 net.cpp:51] Initializing net from parameters: 
name: "C3D-Mini"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "triplet"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 112
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
  }
  data_param {
    source: "/home/lferrer/Documents/LMDB_MINI/train"
    batch_size: 5
    backend: LMDB
  }
}
layer {
  name: "slicer"
  type: "Slice"
  bottom: "triplet"
  top: "anchor_stacked"
  top: "positive_stacked"
  top: "negative_stacked"
  slice_param {
    slice_dim: 1
    slice_point: 48
    slice_point: 96
  }
}
layer {
  name: "reshape_anchor"
  type: "Reshape"
  bottom: "anchor_stacked"
  top: "anchor"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "reshape_positive"
  type: "Reshape"
  bottom: "positive_stacked"
  top: "positive"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "reshape_negative"
  type: "Reshape"
  bottom: "negative_stacked"
  top: "negative"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution3D"
  bottom: "anchor"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution3d_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    kernel_depth: 3
    stride: 1
    temporal_stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    temporal_pad: 1
  }
}
layer {
  name: "relu1a"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "pool1"
  type: "Pooling3D"
  bottom: "conv1a"
  top: "pool1"
  pooling3d_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    kernel_depth: 1
    temporal_stride: 1
  }
}
layer {
  name: "conv2a"
  type: "Convolution3D"
  bottom: "pool1"
  top: "conv2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution3d_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    kernel_depth: 3
    stride: 1
    temporal_stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    temporal_pad: 1
  }
}
layer {
  name: "relu2a"
  type: "ReLU"
  bottom: "conv2a"
  top: "conv2a"
}
layer {
  name: "pool2"
  type: "Pooling3D"
  bottom: "conv2a"
  top: "pool2"
  pooling3d_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    kernel_depth: 2
    temporal_stride: 2
  }
}
layer {
  name: "conv3a"
  type: "Convolution3D"
  bottom: "pool2"
  top: "conv3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution3d_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    kernel_depth: 3
    stride: 1
    temporal_stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    temporal_pad: 1
  }
}
layer {
  name: "relu3a"
  type: "ReLU"
  bottom: "conv3a"
  top: "conv3a"
}
layer {
  name: "pool3"
  type: "Pooling3D"
  bottom: "conv3a"
  top: "pool3"
  pooling3d_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    kernel_depth: 2
    temporal_stride: 2
  }
}
layer {
  name: "conv4a"
  type: "Convolution3D"
  bottom: "pool3"
  top: "conv4a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution3d_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    kernel_depth: 3
    stride: 1
    temporal_stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    temporal_pad: 1
  }
}
layer {
  name: "relu4a"
  type: "ReLU"
  bottom: "conv4a"
  top: "conv4a"
}
layer {
  name: "pool4"
  type: "Pooling3D"
  bottom: "conv4a"
  top: "pool4"
  pooling3d_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    kernel_depth: 2
    temporal_stride: 2
  }
}
layer {
  name: "conv5a"
  type: "Convolution3D"
  bottom: "pool4"
  top: "conv5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution3d_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    kernel_depth: 3
    stride: 1
    temporal_stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    temporal_pad: 1
  }
}
layer {
  name: "relu5a"
  type: "ReLU"
  bottom: "conv5a"
  top: "conv5a"
}
layer {
  name: "pool5"
  type: "Pooling3D"
  bottom: "conv5a"
  top: "pool5"
  pooling3d_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    kernel_depth: 2
    temporal_stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 101
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "silence"
  type: "Silence"
  bottom: "positive"
  bottom: "negative"
}
layer {
  name: "loss"
  type: "TripletLoss"
  bottom: "fc8"
  bottom: "fc8"
  bottom: "fc8"
  top: "loss"
  threshold_param {
    threshold: 1
  }
}
I0403 15:11:36.834946 16844 layer_factory.hpp:77] Creating layer data
I0403 15:11:36.835073 16844 db_lmdb.cpp:35] Opened lmdb /home/lferrer/Documents/LMDB_MINI/train
I0403 15:11:36.835093 16844 net.cpp:84] Creating Layer data
I0403 15:11:36.835103 16844 net.cpp:380] data -> triplet
I0403 15:11:36.838579 16844 data_layer.cpp:45] output data size: 5,144,112,112
I0403 15:11:36.839000 16844 net.cpp:122] Setting up data
I0403 15:11:36.839040 16844 net.cpp:129] Top shape: 5 144 112 112 (9031680)
I0403 15:11:36.839045 16844 net.cpp:137] Memory required for data: 36126720
I0403 15:11:36.839054 16844 layer_factory.hpp:77] Creating layer slicer
I0403 15:11:36.839066 16844 net.cpp:84] Creating Layer slicer
I0403 15:11:36.839069 16844 net.cpp:406] slicer <- triplet
I0403 15:11:36.839076 16844 net.cpp:380] slicer -> anchor_stacked
I0403 15:11:36.839089 16844 net.cpp:380] slicer -> positive_stacked
I0403 15:11:36.839109 16844 net.cpp:380] slicer -> negative_stacked
I0403 15:11:36.839118 16844 net.cpp:122] Setting up slicer
I0403 15:11:36.839120 16844 net.cpp:129] Top shape: 5 48 112 112 (3010560)
I0403 15:11:36.839136 16844 net.cpp:129] Top shape: 5 48 112 112 (3010560)
I0403 15:11:36.839154 16844 net.cpp:129] Top shape: 5 48 112 112 (3010560)
I0403 15:11:36.839155 16844 net.cpp:137] Memory required for data: 72253440
I0403 15:11:36.839157 16844 layer_factory.hpp:77] Creating layer reshape_anchor
I0403 15:11:36.839164 16844 net.cpp:84] Creating Layer reshape_anchor
I0403 15:11:36.839166 16844 net.cpp:406] reshape_anchor <- anchor_stacked
I0403 15:11:36.839177 16844 net.cpp:380] reshape_anchor -> anchor
I0403 15:11:36.839192 16844 net.cpp:122] Setting up reshape_anchor
I0403 15:11:36.839196 16844 net.cpp:129] Top shape: 5 3 16 112 112 (3010560)
I0403 15:11:36.839198 16844 net.cpp:137] Memory required for data: 84295680
I0403 15:11:36.839200 16844 layer_factory.hpp:77] Creating layer reshape_positive
I0403 15:11:36.839205 16844 net.cpp:84] Creating Layer reshape_positive
I0403 15:11:36.839208 16844 net.cpp:406] reshape_positive <- positive_stacked
I0403 15:11:36.839211 16844 net.cpp:380] reshape_positive -> positive
I0403 15:11:36.839216 16844 net.cpp:122] Setting up reshape_positive
I0403 15:11:36.839233 16844 net.cpp:129] Top shape: 5 3 16 112 112 (3010560)
I0403 15:11:36.839236 16844 net.cpp:137] Memory required for data: 96337920
I0403 15:11:36.839237 16844 layer_factory.hpp:77] Creating layer reshape_negative
I0403 15:11:36.839257 16844 net.cpp:84] Creating Layer reshape_negative
I0403 15:11:36.839259 16844 net.cpp:406] reshape_negative <- negative_stacked
I0403 15:11:36.839263 16844 net.cpp:380] reshape_negative -> negative
I0403 15:11:36.839267 16844 net.cpp:122] Setting up reshape_negative
I0403 15:11:36.839270 16844 net.cpp:129] Top shape: 5 3 16 112 112 (3010560)
I0403 15:11:36.839272 16844 net.cpp:137] Memory required for data: 108380160
I0403 15:11:36.839274 16844 layer_factory.hpp:77] Creating layer conv1a
I0403 15:11:36.839283 16844 net.cpp:84] Creating Layer conv1a
I0403 15:11:36.839285 16844 net.cpp:406] conv1a <- anchor
I0403 15:11:36.839289 16844 net.cpp:380] conv1a -> conv1a
I0403 15:11:36.839819 16844 net.cpp:122] Setting up conv1a
I0403 15:11:36.839829 16844 net.cpp:129] Top shape: 5 64 16 112 112 (64225280)
I0403 15:11:36.839831 16844 net.cpp:137] Memory required for data: 365281280
I0403 15:11:36.839846 16844 layer_factory.hpp:77] Creating layer relu1a
I0403 15:11:36.839855 16844 net.cpp:84] Creating Layer relu1a
I0403 15:11:36.839859 16844 net.cpp:406] relu1a <- conv1a
I0403 15:11:36.839861 16844 net.cpp:367] relu1a -> conv1a (in-place)
I0403 15:11:36.839874 16844 net.cpp:122] Setting up relu1a
I0403 15:11:36.839876 16844 net.cpp:129] Top shape: 5 64 16 112 112 (64225280)
I0403 15:11:36.839879 16844 net.cpp:137] Memory required for data: 622182400
I0403 15:11:36.839880 16844 layer_factory.hpp:77] Creating layer pool1
I0403 15:11:36.839884 16844 net.cpp:84] Creating Layer pool1
I0403 15:11:36.839886 16844 net.cpp:406] pool1 <- conv1a
I0403 15:11:36.839891 16844 net.cpp:380] pool1 -> pool1
I0403 15:11:36.839902 16844 net.cpp:122] Setting up pool1
I0403 15:11:36.839906 16844 net.cpp:129] Top shape: 5 64 16 56 56 (16056320)
I0403 15:11:36.839908 16844 net.cpp:137] Memory required for data: 686407680
I0403 15:11:36.839910 16844 layer_factory.hpp:77] Creating layer conv2a
I0403 15:11:36.839918 16844 net.cpp:84] Creating Layer conv2a
I0403 15:11:36.839920 16844 net.cpp:406] conv2a <- pool1
I0403 15:11:36.839925 16844 net.cpp:380] conv2a -> conv2a
I0403 15:11:36.844040 16844 net.cpp:122] Setting up conv2a
I0403 15:11:36.844081 16844 net.cpp:129] Top shape: 5 128 16 56 56 (32112640)
I0403 15:11:36.844087 16844 net.cpp:137] Memory required for data: 814858240
I0403 15:11:36.844117 16844 layer_factory.hpp:77] Creating layer relu2a
I0403 15:11:36.844127 16844 net.cpp:84] Creating Layer relu2a
I0403 15:11:36.844132 16844 net.cpp:406] relu2a <- conv2a
I0403 15:11:36.844174 16844 net.cpp:367] relu2a -> conv2a (in-place)
I0403 15:11:36.844187 16844 net.cpp:122] Setting up relu2a
I0403 15:11:36.844192 16844 net.cpp:129] Top shape: 5 128 16 56 56 (32112640)
I0403 15:11:36.844194 16844 net.cpp:137] Memory required for data: 943308800
I0403 15:11:36.844197 16844 layer_factory.hpp:77] Creating layer pool2
I0403 15:11:36.844203 16844 net.cpp:84] Creating Layer pool2
I0403 15:11:36.844207 16844 net.cpp:406] pool2 <- conv2a
I0403 15:11:36.844231 16844 net.cpp:380] pool2 -> pool2
I0403 15:11:36.844247 16844 net.cpp:122] Setting up pool2
I0403 15:11:36.844252 16844 net.cpp:129] Top shape: 5 128 8 28 28 (4014080)
I0403 15:11:36.844255 16844 net.cpp:137] Memory required for data: 959365120
I0403 15:11:36.844257 16844 layer_factory.hpp:77] Creating layer conv3a
I0403 15:11:36.844267 16844 net.cpp:84] Creating Layer conv3a
I0403 15:11:36.844271 16844 net.cpp:406] conv3a <- pool2
I0403 15:11:36.844276 16844 net.cpp:380] conv3a -> conv3a
I0403 15:11:36.861227 16844 net.cpp:122] Setting up conv3a
I0403 15:11:36.861265 16844 net.cpp:129] Top shape: 5 256 8 28 28 (8028160)
I0403 15:11:36.861285 16844 net.cpp:137] Memory required for data: 991477760
I0403 15:11:36.861304 16844 layer_factory.hpp:77] Creating layer relu3a
I0403 15:11:36.861316 16844 net.cpp:84] Creating Layer relu3a
I0403 15:11:36.861320 16844 net.cpp:406] relu3a <- conv3a
I0403 15:11:36.861326 16844 net.cpp:367] relu3a -> conv3a (in-place)
I0403 15:11:36.861337 16844 net.cpp:122] Setting up relu3a
I0403 15:11:36.861341 16844 net.cpp:129] Top shape: 5 256 8 28 28 (8028160)
I0403 15:11:36.861361 16844 net.cpp:137] Memory required for data: 1023590400
I0403 15:11:36.861364 16844 layer_factory.hpp:77] Creating layer pool3
I0403 15:11:36.861371 16844 net.cpp:84] Creating Layer pool3
I0403 15:11:36.861372 16844 net.cpp:406] pool3 <- conv3a
I0403 15:11:36.861377 16844 net.cpp:380] pool3 -> pool3
I0403 15:11:36.861384 16844 net.cpp:122] Setting up pool3
I0403 15:11:36.861392 16844 net.cpp:129] Top shape: 5 256 4 14 14 (1003520)
I0403 15:11:36.861394 16844 net.cpp:137] Memory required for data: 1027604480
I0403 15:11:36.861397 16844 layer_factory.hpp:77] Creating layer conv4a
I0403 15:11:36.861403 16844 net.cpp:84] Creating Layer conv4a
I0403 15:11:36.861405 16844 net.cpp:406] conv4a <- pool3
I0403 15:11:36.861410 16844 net.cpp:380] conv4a -> conv4a
I0403 15:11:36.893450 16844 net.cpp:122] Setting up conv4a
I0403 15:11:36.893468 16844 net.cpp:129] Top shape: 5 256 4 14 14 (1003520)
I0403 15:11:36.893471 16844 net.cpp:137] Memory required for data: 1031618560
I0403 15:11:36.893482 16844 layer_factory.hpp:77] Creating layer relu4a
I0403 15:11:36.893491 16844 net.cpp:84] Creating Layer relu4a
I0403 15:11:36.893496 16844 net.cpp:406] relu4a <- conv4a
I0403 15:11:36.893501 16844 net.cpp:367] relu4a -> conv4a (in-place)
I0403 15:11:36.893507 16844 net.cpp:122] Setting up relu4a
I0403 15:11:36.893510 16844 net.cpp:129] Top shape: 5 256 4 14 14 (1003520)
I0403 15:11:36.893512 16844 net.cpp:137] Memory required for data: 1035632640
I0403 15:11:36.893514 16844 layer_factory.hpp:77] Creating layer pool4
I0403 15:11:36.893539 16844 net.cpp:84] Creating Layer pool4
I0403 15:11:36.893543 16844 net.cpp:406] pool4 <- conv4a
I0403 15:11:36.893546 16844 net.cpp:380] pool4 -> pool4
I0403 15:11:36.893553 16844 net.cpp:122] Setting up pool4
I0403 15:11:36.893555 16844 net.cpp:129] Top shape: 5 256 2 7 7 (125440)
I0403 15:11:36.893558 16844 net.cpp:137] Memory required for data: 1036134400
I0403 15:11:36.893559 16844 layer_factory.hpp:77] Creating layer conv5a
I0403 15:11:36.893584 16844 net.cpp:84] Creating Layer conv5a
I0403 15:11:36.893585 16844 net.cpp:406] conv5a <- pool4
I0403 15:11:36.893604 16844 net.cpp:380] conv5a -> conv5a
I0403 15:11:36.925590 16844 net.cpp:122] Setting up conv5a
I0403 15:11:36.925627 16844 net.cpp:129] Top shape: 5 256 2 7 7 (125440)
I0403 15:11:36.925631 16844 net.cpp:137] Memory required for data: 1036636160
I0403 15:11:36.925647 16844 layer_factory.hpp:77] Creating layer relu5a
I0403 15:11:36.925657 16844 net.cpp:84] Creating Layer relu5a
I0403 15:11:36.925660 16844 net.cpp:406] relu5a <- conv5a
I0403 15:11:36.925668 16844 net.cpp:367] relu5a -> conv5a (in-place)
I0403 15:11:36.925690 16844 net.cpp:122] Setting up relu5a
I0403 15:11:36.925695 16844 net.cpp:129] Top shape: 5 256 2 7 7 (125440)
I0403 15:11:36.925698 16844 net.cpp:137] Memory required for data: 1037137920
I0403 15:11:36.925699 16844 layer_factory.hpp:77] Creating layer pool5
I0403 15:11:36.925704 16844 net.cpp:84] Creating Layer pool5
I0403 15:11:36.925707 16844 net.cpp:406] pool5 <- conv5a
I0403 15:11:36.925714 16844 net.cpp:380] pool5 -> pool5
I0403 15:11:36.925735 16844 net.cpp:122] Setting up pool5
I0403 15:11:36.925740 16844 net.cpp:129] Top shape: 5 256 1 4 4 (20480)
I0403 15:11:36.925742 16844 net.cpp:137] Memory required for data: 1037219840
I0403 15:11:36.925745 16844 layer_factory.hpp:77] Creating layer fc6
I0403 15:11:36.925752 16844 net.cpp:84] Creating Layer fc6
I0403 15:11:36.925755 16844 net.cpp:406] fc6 <- pool5
I0403 15:11:36.925760 16844 net.cpp:380] fc6 -> fc6
I0403 15:11:37.066059 16844 net.cpp:122] Setting up fc6
I0403 15:11:37.066078 16844 net.cpp:129] Top shape: 5 2048 (10240)
I0403 15:11:37.066082 16844 net.cpp:137] Memory required for data: 1037260800
I0403 15:11:37.066090 16844 layer_factory.hpp:77] Creating layer relu6
I0403 15:11:37.066098 16844 net.cpp:84] Creating Layer relu6
I0403 15:11:37.066102 16844 net.cpp:406] relu6 <- fc6
I0403 15:11:37.066107 16844 net.cpp:367] relu6 -> fc6 (in-place)
I0403 15:11:37.066114 16844 net.cpp:122] Setting up relu6
I0403 15:11:37.066118 16844 net.cpp:129] Top shape: 5 2048 (10240)
I0403 15:11:37.066143 16844 net.cpp:137] Memory required for data: 1037301760
I0403 15:11:37.066148 16844 layer_factory.hpp:77] Creating layer drop6
I0403 15:11:37.066160 16844 net.cpp:84] Creating Layer drop6
I0403 15:11:37.066174 16844 net.cpp:406] drop6 <- fc6
I0403 15:11:37.066181 16844 net.cpp:367] drop6 -> fc6 (in-place)
I0403 15:11:37.066192 16844 net.cpp:122] Setting up drop6
I0403 15:11:37.066195 16844 net.cpp:129] Top shape: 5 2048 (10240)
I0403 15:11:37.066197 16844 net.cpp:137] Memory required for data: 1037342720
I0403 15:11:37.066200 16844 layer_factory.hpp:77] Creating layer fc7
I0403 15:11:37.066205 16844 net.cpp:84] Creating Layer fc7
I0403 15:11:37.066207 16844 net.cpp:406] fc7 <- fc6
I0403 15:11:37.066210 16844 net.cpp:380] fc7 -> fc7
I0403 15:11:37.138207 16844 net.cpp:122] Setting up fc7
I0403 15:11:37.138226 16844 net.cpp:129] Top shape: 5 2048 (10240)
I0403 15:11:37.138229 16844 net.cpp:137] Memory required for data: 1037383680
I0403 15:11:37.138237 16844 layer_factory.hpp:77] Creating layer relu7
I0403 15:11:37.138244 16844 net.cpp:84] Creating Layer relu7
I0403 15:11:37.138267 16844 net.cpp:406] relu7 <- fc7
I0403 15:11:37.138272 16844 net.cpp:367] relu7 -> fc7 (in-place)
I0403 15:11:37.138290 16844 net.cpp:122] Setting up relu7
I0403 15:11:37.138293 16844 net.cpp:129] Top shape: 5 2048 (10240)
I0403 15:11:37.138295 16844 net.cpp:137] Memory required for data: 1037424640
I0403 15:11:37.138314 16844 layer_factory.hpp:77] Creating layer drop7
I0403 15:11:37.138319 16844 net.cpp:84] Creating Layer drop7
I0403 15:11:37.138334 16844 net.cpp:406] drop7 <- fc7
I0403 15:11:37.138339 16844 net.cpp:367] drop7 -> fc7 (in-place)
I0403 15:11:37.138344 16844 net.cpp:122] Setting up drop7
I0403 15:11:37.138346 16844 net.cpp:129] Top shape: 5 2048 (10240)
I0403 15:11:37.138347 16844 net.cpp:137] Memory required for data: 1037465600
I0403 15:11:37.138350 16844 layer_factory.hpp:77] Creating layer fc8
I0403 15:11:37.138357 16844 net.cpp:84] Creating Layer fc8
I0403 15:11:37.138371 16844 net.cpp:406] fc8 <- fc7
I0403 15:11:37.138376 16844 net.cpp:380] fc8 -> fc8
I0403 15:11:37.143138 16844 net.cpp:122] Setting up fc8
I0403 15:11:37.143182 16844 net.cpp:129] Top shape: 5 101 (505)
I0403 15:11:37.143203 16844 net.cpp:137] Memory required for data: 1037467620
I0403 15:11:37.143224 16844 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0403 15:11:37.143236 16844 net.cpp:84] Creating Layer fc8_fc8_0_split
I0403 15:11:37.143242 16844 net.cpp:406] fc8_fc8_0_split <- fc8
I0403 15:11:37.143260 16844 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0403 15:11:37.143267 16844 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0403 15:11:37.143285 16844 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_2
I0403 15:11:37.143292 16844 net.cpp:122] Setting up fc8_fc8_0_split
I0403 15:11:37.143313 16844 net.cpp:129] Top shape: 5 101 (505)
I0403 15:11:37.143316 16844 net.cpp:129] Top shape: 5 101 (505)
I0403 15:11:37.143317 16844 net.cpp:129] Top shape: 5 101 (505)
I0403 15:11:37.143319 16844 net.cpp:137] Memory required for data: 1037473680
I0403 15:11:37.143321 16844 layer_factory.hpp:77] Creating layer silence
I0403 15:11:37.143326 16844 net.cpp:84] Creating Layer silence
I0403 15:11:37.143327 16844 net.cpp:406] silence <- positive
I0403 15:11:37.143332 16844 net.cpp:406] silence <- negative
I0403 15:11:37.143369 16844 net.cpp:122] Setting up silence
I0403 15:11:37.143383 16844 net.cpp:137] Memory required for data: 1037473680
I0403 15:11:37.143385 16844 layer_factory.hpp:77] Creating layer loss
I0403 15:11:37.143393 16844 net.cpp:84] Creating Layer loss
I0403 15:11:37.143395 16844 net.cpp:406] loss <- fc8_fc8_0_split_0
I0403 15:11:37.143399 16844 net.cpp:406] loss <- fc8_fc8_0_split_1
I0403 15:11:37.143401 16844 net.cpp:406] loss <- fc8_fc8_0_split_2
I0403 15:11:37.143416 16844 net.cpp:380] loss -> loss
I0403 15:11:37.143431 16844 net.cpp:122] Setting up loss
I0403 15:11:37.143471 16844 net.cpp:129] Top shape: (1)
I0403 15:11:37.143473 16844 net.cpp:132]     with loss weight 1
I0403 15:11:37.143530 16844 net.cpp:137] Memory required for data: 1037473684
I0403 15:11:37.143534 16844 net.cpp:198] loss needs backward computation.
I0403 15:11:37.143537 16844 net.cpp:200] silence does not need backward computation.
I0403 15:11:37.143539 16844 net.cpp:198] fc8_fc8_0_split needs backward computation.
I0403 15:11:37.143543 16844 net.cpp:198] fc8 needs backward computation.
I0403 15:11:37.143544 16844 net.cpp:198] drop7 needs backward computation.
I0403 15:11:37.143546 16844 net.cpp:198] relu7 needs backward computation.
I0403 15:11:37.143561 16844 net.cpp:198] fc7 needs backward computation.
I0403 15:11:37.143564 16844 net.cpp:198] drop6 needs backward computation.
I0403 15:11:37.143566 16844 net.cpp:198] relu6 needs backward computation.
I0403 15:11:37.143568 16844 net.cpp:198] fc6 needs backward computation.
I0403 15:11:37.143570 16844 net.cpp:198] pool5 needs backward computation.
I0403 15:11:37.143573 16844 net.cpp:198] relu5a needs backward computation.
I0403 15:11:37.143574 16844 net.cpp:198] conv5a needs backward computation.
I0403 15:11:37.143576 16844 net.cpp:198] pool4 needs backward computation.
I0403 15:11:37.143579 16844 net.cpp:198] relu4a needs backward computation.
I0403 15:11:37.143581 16844 net.cpp:198] conv4a needs backward computation.
I0403 15:11:37.143584 16844 net.cpp:198] pool3 needs backward computation.
I0403 15:11:37.143585 16844 net.cpp:198] relu3a needs backward computation.
I0403 15:11:37.143587 16844 net.cpp:198] conv3a needs backward computation.
I0403 15:11:37.143590 16844 net.cpp:198] pool2 needs backward computation.
I0403 15:11:37.143592 16844 net.cpp:198] relu2a needs backward computation.
I0403 15:11:37.143594 16844 net.cpp:198] conv2a needs backward computation.
I0403 15:11:37.143596 16844 net.cpp:198] pool1 needs backward computation.
I0403 15:11:37.143599 16844 net.cpp:198] relu1a needs backward computation.
I0403 15:11:37.143600 16844 net.cpp:198] conv1a needs backward computation.
I0403 15:11:37.143604 16844 net.cpp:200] reshape_negative does not need backward computation.
I0403 15:11:37.143605 16844 net.cpp:200] reshape_positive does not need backward computation.
I0403 15:11:37.143620 16844 net.cpp:200] reshape_anchor does not need backward computation.
I0403 15:11:37.143623 16844 net.cpp:200] slicer does not need backward computation.
I0403 15:11:37.143640 16844 net.cpp:200] data does not need backward computation.
I0403 15:11:37.143641 16844 net.cpp:242] This network produces output loss
I0403 15:11:37.143685 16844 net.cpp:255] Network initialization done.
I0403 15:11:37.143769 16844 solver.cpp:56] Solver scaffolding done.
I0403 15:11:37.143816 16844 caffe.cpp:155] Finetuning from ../c3d_ucf101_finetune_whole_iter_20000
I0403 15:11:44.082697 16844 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: ../c3d_ucf101_finetune_whole_iter_20000
I0403 15:11:44.602413 16844 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
F0403 15:11:44.603904 16844 blob.cpp:496] Check failed: count_ == proto.data_size() (1728 vs. 0) 
*** Check failure stack trace: ***
    @     0x7f02f91455cd  google::LogMessage::Fail()
    @     0x7f02f9147433  google::LogMessage::SendToLog()
    @     0x7f02f914515b  google::LogMessage::Flush()
    @     0x7f02f9147e1e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f02f96fc5d2  caffe::Blob<>::FromProto()
    @     0x7f02f96a4a6f  caffe::Net<>::CopyTrainedLayersFrom()
    @     0x7f02f96af775  caffe::Net<>::CopyTrainedLayersFromBinaryProto()
    @     0x7f02f96af80e  caffe::Net<>::CopyTrainedLayersFrom()
    @           0x40aea4  CopyLayers()
    @           0x40c3da  train()
    @           0x408110  main
    @     0x7f02f8098830  __libc_start_main
    @           0x408939  _start
    @              (nil)  (unknown)
