I0502 21:16:44.838032 28586 caffe.cpp:217] Using GPUs 2
I0502 21:16:45.005380 28586 caffe.cpp:222] GPU 2: GeForce GTX TITAN X
I0502 21:16:45.873329 28586 solver.cpp:48] Initializing solver from parameters: 
test_iter: 500
test_interval: 200
base_lr: 1e-05
display: 50
max_iter: 20000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
stepsize: 2000
snapshot: 200
snapshot_prefix: "three_stream_triplet_loss"
solver_mode: GPU
device_id: 2
net: "../../models/three_stream_triplet_loss.prototxt"
train_state {
  level: 0
  stage: ""
}
test_state {
  stage: "test-on-val"
}
I0502 21:16:45.873548 28586 solver.cpp:91] Creating training net from net file: ../../models/three_stream_triplet_loss.prototxt
I0502 21:16:45.888907 28586 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0502 21:16:45.890101 28586 net.cpp:58] Initializing net from parameters: 
name: "C3D-Three-Streams"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "triplet"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 112
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
  }
  data_param {
    source: "/data/leo-data/Synthetic/LMDB/Triplets/train"
    batch_size: 18
    backend: LMDB
  }
}
layer {
  name: "slicer"
  type: "Slice"
  bottom: "triplet"
  top: "anchor_stacked"
  top: "positive_stacked"
  top: "negative_stacked"
  slice_param {
    slice_dim: 1
    slice_point: 48
    slice_point: 96
  }
}
layer {
  name: "reshape_anchor"
  type: "Reshape"
  bottom: "anchor_stacked"
  top: "anchor"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "reshape_positive"
  type: "Reshape"
  bottom: "positive_stacked"
  top: "positive"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "reshape_negative"
  type: "Reshape"
  bottom: "negative_stacked"
  top: "negative"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "conv1a"
  type: "NdConvolution"
  bottom: "anchor"
  top: "conv1a"
  param {
    name: "conv1a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "pool1"
  type: "NdPooling"
  bottom: "conv1a"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a"
  type: "NdConvolution"
  bottom: "pool1"
  top: "conv2a"
  param {
    name: "conv2a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a"
  type: "ReLU"
  bottom: "conv2a"
  top: "conv2a"
}
layer {
  name: "pool2"
  type: "NdPooling"
  bottom: "conv2a"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a"
  type: "NdConvolution"
  bottom: "pool2"
  top: "conv3a"
  param {
    name: "conv3a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a"
  type: "ReLU"
  bottom: "conv3a"
  top: "conv3a"
}
layer {
  name: "pool3"
  type: "NdPooling"
  bottom: "conv3a"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a"
  type: "NdConvolution"
  bottom: "pool3"
  top: "conv4a"
  param {
    name: "conv4a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a"
  type: "ReLU"
  bottom: "conv4a"
  top: "conv4a"
}
layer {
  name: "pool4"
  type: "NdPooling"
  bottom: "conv4a"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a"
  type: "NdConvolution"
  bottom: "pool4"
  top: "conv5a"
  param {
    name: "conv5a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a"
  type: "ReLU"
  bottom: "conv5a"
  top: "conv5a"
}
layer {
  name: "pool5"
  type: "NdPooling"
  bottom: "conv5a"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv1a_pos"
  type: "NdConvolution"
  bottom: "positive"
  top: "conv1a_pos"
  param {
    name: "conv1a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a_pos"
  type: "ReLU"
  bottom: "conv1a_pos"
  top: "conv1a_pos"
}
layer {
  name: "pool1_pos"
  type: "NdPooling"
  bottom: "conv1a_pos"
  top: "pool1_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a_pos"
  type: "NdConvolution"
  bottom: "pool1_pos"
  top: "conv2a_pos"
  param {
    name: "conv2a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a_pos"
  type: "ReLU"
  bottom: "conv2a_pos"
  top: "conv2a_pos"
}
layer {
  name: "pool2_pos"
  type: "NdPooling"
  bottom: "conv2a_pos"
  top: "pool2_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a_pos"
  type: "NdConvolution"
  bottom: "pool2_pos"
  top: "conv3a_pos"
  param {
    name: "conv3a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a_pos"
  type: "ReLU"
  bottom: "conv3a_pos"
  top: "conv3a_pos"
}
layer {
  name: "pool3_pos"
  type: "NdPooling"
  bottom: "conv3a_pos"
  top: "pool3_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a_pos"
  type: "NdConvolution"
  bottom: "pool3_pos"
  top: "conv4a_pos"
  param {
    name: "conv4a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a_pos"
  type: "ReLU"
  bottom: "conv4a_pos"
  top: "conv4a_pos"
}
layer {
  name: "pool4_pos"
  type: "NdPooling"
  bottom: "conv4a_pos"
  top: "pool4_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a_pos"
  type: "NdConvolution"
  bottom: "pool4_pos"
  top: "conv5a_pos"
  param {
    name: "conv5a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a_pos"
  type: "ReLU"
  bottom: "conv5a_pos"
  top: "conv5a_pos"
}
layer {
  name: "pool5_pos"
  type: "NdPooling"
  bottom: "conv5a_pos"
  top: "pool5_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6_pos"
  type: "InnerProduct"
  bottom: "pool5_pos"
  top: "fc6_pos"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6_pos"
  type: "ReLU"
  bottom: "fc6_pos"
  top: "fc6_pos"
}
layer {
  name: "drop6_pos"
  type: "Dropout"
  bottom: "fc6_pos"
  top: "fc6_pos"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_pos"
  type: "InnerProduct"
  bottom: "fc6_pos"
  top: "fc7_pos"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7_pos"
  type: "ReLU"
  bottom: "fc7_pos"
  top: "fc7_pos"
}
layer {
  name: "drop7_pos"
  type: "Dropout"
  bottom: "fc7_pos"
  top: "fc7_pos"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv1a_neg"
  type: "NdConvolution"
  bottom: "negative"
  top: "conv1a_neg"
  param {
    name: "conv1a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a_neg"
  type: "ReLU"
  bottom: "conv1a_neg"
  top: "conv1a_neg"
}
layer {
  name: "pool1_neg"
  type: "NdPooling"
  bottom: "conv1a_neg"
  top: "pool1_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a_neg"
  type: "NdConvolution"
  bottom: "pool1_neg"
  top: "conv2a_neg"
  param {
    name: "conv2a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a_neg"
  type: "ReLU"
  bottom: "conv2a_neg"
  top: "conv2a_neg"
}
layer {
  name: "pool2_neg"
  type: "NdPooling"
  bottom: "conv2a_neg"
  top: "pool2_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a_neg"
  type: "NdConvolution"
  bottom: "pool2_neg"
  top: "conv3a_neg"
  param {
    name: "conv3a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a_neg"
  type: "ReLU"
  bottom: "conv3a_neg"
  top: "conv3a_neg"
}
layer {
  name: "pool3_neg"
  type: "NdPooling"
  bottom: "conv3a_neg"
  top: "pool3_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a_neg"
  type: "NdConvolution"
  bottom: "pool3_neg"
  top: "conv4a_neg"
  param {
    name: "conv4a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a_neg"
  type: "ReLU"
  bottom: "conv4a_neg"
  top: "conv4a_neg"
}
layer {
  name: "pool4_neg"
  type: "NdPooling"
  bottom: "conv4a_neg"
  top: "pool4_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a_neg"
  type: "NdConvolution"
  bottom: "pool4_neg"
  top: "conv5a_neg"
  param {
    name: "conv5a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a_neg"
  type: "ReLU"
  bottom: "conv5a_neg"
  top: "conv5a_neg"
}
layer {
  name: "pool5_neg"
  type: "NdPooling"
  bottom: "conv5a_neg"
  top: "pool5_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6_neg"
  type: "InnerProduct"
  bottom: "pool5_neg"
  top: "fc6_neg"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6_neg"
  type: "ReLU"
  bottom: "fc6_neg"
  top: "fc6_neg"
}
layer {
  name: "drop6_neg"
  type: "Dropout"
  bottom: "fc6_neg"
  top: "fc6_neg"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_neg"
  type: "InnerProduct"
  bottom: "fc6_neg"
  top: "fc7_neg"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7_neg"
  type: "ReLU"
  bottom: "fc7_neg"
  top: "fc7_neg"
}
layer {
  name: "drop7_neg"
  type: "Dropout"
  bottom: "fc7_neg"
  top: "fc7_neg"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "TripletLoss"
  bottom: "fc7"
  bottom: "fc7_pos"
  bottom: "fc7_neg"
  top: "loss"
  threshold_param {
    threshold: 1
  }
}
I0502 21:16:45.890583 28586 layer_factory.hpp:77] Creating layer data
I0502 21:16:45.891367 28586 net.cpp:100] Creating Layer data
I0502 21:16:45.891386 28586 net.cpp:408] data -> triplet
I0502 21:16:45.895603 28595 db_lmdb.cpp:35] Opened lmdb /data/leo-data/Synthetic/LMDB/Triplets/train
I0502 21:16:45.931249 28586 data_layer.cpp:41] output data size: 18,144,112,112
I0502 21:16:46.235407 28586 net.cpp:150] Setting up data
I0502 21:16:46.235483 28586 net.cpp:157] Top shape: 18 144 112 112 (32514048)
I0502 21:16:46.235488 28586 net.cpp:165] Memory required for data: 130056192
I0502 21:16:46.235503 28586 layer_factory.hpp:77] Creating layer slicer
I0502 21:16:46.235540 28586 net.cpp:100] Creating Layer slicer
I0502 21:16:46.235551 28586 net.cpp:434] slicer <- triplet
I0502 21:16:46.235564 28586 net.cpp:408] slicer -> anchor_stacked
I0502 21:16:46.235577 28586 net.cpp:408] slicer -> positive_stacked
I0502 21:16:46.235585 28586 net.cpp:408] slicer -> negative_stacked
I0502 21:16:46.235726 28586 net.cpp:150] Setting up slicer
I0502 21:16:46.235743 28586 net.cpp:157] Top shape: 18 48 112 112 (10838016)
I0502 21:16:46.235750 28586 net.cpp:157] Top shape: 18 48 112 112 (10838016)
I0502 21:16:46.235834 28586 net.cpp:157] Top shape: 18 48 112 112 (10838016)
I0502 21:16:46.235844 28586 net.cpp:165] Memory required for data: 260112384
I0502 21:16:46.235853 28586 layer_factory.hpp:77] Creating layer reshape_anchor
I0502 21:16:46.235889 28586 net.cpp:100] Creating Layer reshape_anchor
I0502 21:16:46.235900 28586 net.cpp:434] reshape_anchor <- anchor_stacked
I0502 21:16:46.235918 28586 net.cpp:408] reshape_anchor -> anchor
I0502 21:16:46.235980 28586 net.cpp:150] Setting up reshape_anchor
I0502 21:16:46.235994 28586 net.cpp:157] Top shape: 18 3 16 112 112 (10838016)
I0502 21:16:46.235999 28586 net.cpp:165] Memory required for data: 303464448
I0502 21:16:46.236014 28586 layer_factory.hpp:77] Creating layer reshape_positive
I0502 21:16:46.236035 28586 net.cpp:100] Creating Layer reshape_positive
I0502 21:16:46.236043 28586 net.cpp:434] reshape_positive <- positive_stacked
I0502 21:16:46.236052 28586 net.cpp:408] reshape_positive -> positive
I0502 21:16:46.236115 28586 net.cpp:150] Setting up reshape_positive
I0502 21:16:46.236127 28586 net.cpp:157] Top shape: 18 3 16 112 112 (10838016)
I0502 21:16:46.236132 28586 net.cpp:165] Memory required for data: 346816512
I0502 21:16:46.236140 28586 layer_factory.hpp:77] Creating layer reshape_negative
I0502 21:16:46.236152 28586 net.cpp:100] Creating Layer reshape_negative
I0502 21:16:46.236161 28586 net.cpp:434] reshape_negative <- negative_stacked
I0502 21:16:46.236171 28586 net.cpp:408] reshape_negative -> negative
I0502 21:16:46.236202 28586 net.cpp:150] Setting up reshape_negative
I0502 21:16:46.236212 28586 net.cpp:157] Top shape: 18 3 16 112 112 (10838016)
I0502 21:16:46.236218 28586 net.cpp:165] Memory required for data: 390168576
I0502 21:16:46.236225 28586 layer_factory.hpp:77] Creating layer conv1a
I0502 21:16:46.236245 28586 net.cpp:100] Creating Layer conv1a
I0502 21:16:46.236251 28586 net.cpp:434] conv1a <- anchor
I0502 21:16:46.236261 28586 net.cpp:408] conv1a -> conv1a
I0502 21:16:46.978195 28586 net.cpp:150] Setting up conv1a
I0502 21:16:46.978247 28586 net.cpp:157] Top shape: 18 64 16 112 112 (231211008)
I0502 21:16:46.978255 28586 net.cpp:165] Memory required for data: 1315012608
I0502 21:16:46.978296 28586 layer_factory.hpp:77] Creating layer relu1a
I0502 21:16:46.978322 28586 net.cpp:100] Creating Layer relu1a
I0502 21:16:46.978332 28586 net.cpp:434] relu1a <- conv1a
I0502 21:16:46.978345 28586 net.cpp:395] relu1a -> conv1a (in-place)
I0502 21:16:46.990324 28586 net.cpp:150] Setting up relu1a
I0502 21:16:46.990365 28586 net.cpp:157] Top shape: 18 64 16 112 112 (231211008)
I0502 21:16:46.990371 28586 net.cpp:165] Memory required for data: 2239856640
I0502 21:16:46.990378 28586 layer_factory.hpp:77] Creating layer pool1
I0502 21:16:46.990401 28586 net.cpp:100] Creating Layer pool1
I0502 21:16:46.990425 28586 net.cpp:434] pool1 <- conv1a
I0502 21:16:46.990445 28586 net.cpp:408] pool1 -> pool1
I0502 21:16:46.990967 28586 net.cpp:150] Setting up pool1
I0502 21:16:46.990988 28586 net.cpp:157] Top shape: 18 64 16 56 56 (57802752)
I0502 21:16:46.990995 28586 net.cpp:165] Memory required for data: 2471067648
I0502 21:16:46.991005 28586 layer_factory.hpp:77] Creating layer conv2a
I0502 21:16:46.991026 28586 net.cpp:100] Creating Layer conv2a
I0502 21:16:46.991034 28586 net.cpp:434] conv2a <- pool1
I0502 21:16:46.991046 28586 net.cpp:408] conv2a -> conv2a
I0502 21:16:47.023344 28586 net.cpp:150] Setting up conv2a
I0502 21:16:47.023385 28586 net.cpp:157] Top shape: 18 128 16 56 56 (115605504)
I0502 21:16:47.023392 28586 net.cpp:165] Memory required for data: 2933489664
I0502 21:16:47.023414 28586 layer_factory.hpp:77] Creating layer relu2a
I0502 21:16:47.023432 28586 net.cpp:100] Creating Layer relu2a
I0502 21:16:47.023442 28586 net.cpp:434] relu2a <- conv2a
I0502 21:16:47.023452 28586 net.cpp:395] relu2a -> conv2a (in-place)
I0502 21:16:47.025040 28586 net.cpp:150] Setting up relu2a
I0502 21:16:47.025063 28586 net.cpp:157] Top shape: 18 128 16 56 56 (115605504)
I0502 21:16:47.025070 28586 net.cpp:165] Memory required for data: 3395911680
I0502 21:16:47.025076 28586 layer_factory.hpp:77] Creating layer pool2
I0502 21:16:47.025090 28586 net.cpp:100] Creating Layer pool2
I0502 21:16:47.025097 28586 net.cpp:434] pool2 <- conv2a
I0502 21:16:47.025107 28586 net.cpp:408] pool2 -> pool2
I0502 21:16:47.025485 28586 net.cpp:150] Setting up pool2
I0502 21:16:47.025501 28586 net.cpp:157] Top shape: 18 128 8 28 28 (14450688)
I0502 21:16:47.025509 28586 net.cpp:165] Memory required for data: 3453714432
I0502 21:16:47.025516 28586 layer_factory.hpp:77] Creating layer conv3a
I0502 21:16:47.025530 28586 net.cpp:100] Creating Layer conv3a
I0502 21:16:47.025537 28586 net.cpp:434] conv3a <- pool2
I0502 21:16:47.025550 28586 net.cpp:408] conv3a -> conv3a
I0502 21:16:47.077766 28586 net.cpp:150] Setting up conv3a
I0502 21:16:47.077802 28586 net.cpp:157] Top shape: 18 256 8 28 28 (28901376)
I0502 21:16:47.077808 28586 net.cpp:165] Memory required for data: 3569319936
I0502 21:16:47.077826 28586 layer_factory.hpp:77] Creating layer relu3a
I0502 21:16:47.077867 28586 net.cpp:100] Creating Layer relu3a
I0502 21:16:47.077875 28586 net.cpp:434] relu3a <- conv3a
I0502 21:16:47.077885 28586 net.cpp:395] relu3a -> conv3a (in-place)
I0502 21:16:47.079927 28586 net.cpp:150] Setting up relu3a
I0502 21:16:47.079958 28586 net.cpp:157] Top shape: 18 256 8 28 28 (28901376)
I0502 21:16:47.079964 28586 net.cpp:165] Memory required for data: 3684925440
I0502 21:16:47.079968 28586 layer_factory.hpp:77] Creating layer pool3
I0502 21:16:47.079979 28586 net.cpp:100] Creating Layer pool3
I0502 21:16:47.079984 28586 net.cpp:434] pool3 <- conv3a
I0502 21:16:47.079993 28586 net.cpp:408] pool3 -> pool3
I0502 21:16:47.080230 28586 net.cpp:150] Setting up pool3
I0502 21:16:47.080242 28586 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0502 21:16:47.080245 28586 net.cpp:165] Memory required for data: 3699376128
I0502 21:16:47.080276 28586 layer_factory.hpp:77] Creating layer conv4a
I0502 21:16:47.080304 28586 net.cpp:100] Creating Layer conv4a
I0502 21:16:47.080314 28586 net.cpp:434] conv4a <- pool3
I0502 21:16:47.080327 28586 net.cpp:408] conv4a -> conv4a
I0502 21:16:47.151888 28586 net.cpp:150] Setting up conv4a
I0502 21:16:47.151928 28586 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0502 21:16:47.151935 28586 net.cpp:165] Memory required for data: 3713826816
I0502 21:16:47.151947 28586 layer_factory.hpp:77] Creating layer relu4a
I0502 21:16:47.151957 28586 net.cpp:100] Creating Layer relu4a
I0502 21:16:47.151962 28586 net.cpp:434] relu4a <- conv4a
I0502 21:16:47.151969 28586 net.cpp:395] relu4a -> conv4a (in-place)
I0502 21:16:47.152166 28586 net.cpp:150] Setting up relu4a
I0502 21:16:47.152178 28586 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0502 21:16:47.152181 28586 net.cpp:165] Memory required for data: 3728277504
I0502 21:16:47.152184 28586 layer_factory.hpp:77] Creating layer pool4
I0502 21:16:47.152197 28586 net.cpp:100] Creating Layer pool4
I0502 21:16:47.152202 28586 net.cpp:434] pool4 <- conv4a
I0502 21:16:47.152209 28586 net.cpp:408] pool4 -> pool4
I0502 21:16:47.153138 28586 net.cpp:150] Setting up pool4
I0502 21:16:47.153151 28586 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0502 21:16:47.153156 28586 net.cpp:165] Memory required for data: 3730083840
I0502 21:16:47.153159 28586 layer_factory.hpp:77] Creating layer conv5a
I0502 21:16:47.153236 28586 net.cpp:100] Creating Layer conv5a
I0502 21:16:47.153254 28586 net.cpp:434] conv5a <- pool4
I0502 21:16:47.153271 28586 net.cpp:408] conv5a -> conv5a
I0502 21:16:47.248181 28586 net.cpp:150] Setting up conv5a
I0502 21:16:47.248224 28586 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0502 21:16:47.248229 28586 net.cpp:165] Memory required for data: 3731890176
I0502 21:16:47.248247 28586 layer_factory.hpp:77] Creating layer relu5a
I0502 21:16:47.248266 28586 net.cpp:100] Creating Layer relu5a
I0502 21:16:47.248283 28586 net.cpp:434] relu5a <- conv5a
I0502 21:16:47.248296 28586 net.cpp:395] relu5a -> conv5a (in-place)
I0502 21:16:47.248556 28586 net.cpp:150] Setting up relu5a
I0502 21:16:47.248570 28586 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0502 21:16:47.248579 28586 net.cpp:165] Memory required for data: 3733696512
I0502 21:16:47.248589 28586 layer_factory.hpp:77] Creating layer pool5
I0502 21:16:47.248606 28586 net.cpp:100] Creating Layer pool5
I0502 21:16:47.248613 28586 net.cpp:434] pool5 <- conv5a
I0502 21:16:47.248625 28586 net.cpp:408] pool5 -> pool5
I0502 21:16:47.252776 28586 net.cpp:150] Setting up pool5
I0502 21:16:47.252794 28586 net.cpp:157] Top shape: 18 256 1 4 4 (73728)
I0502 21:16:47.252799 28586 net.cpp:165] Memory required for data: 3733991424
I0502 21:16:47.252810 28586 layer_factory.hpp:77] Creating layer fc6
I0502 21:16:47.252837 28586 net.cpp:100] Creating Layer fc6
I0502 21:16:47.252845 28586 net.cpp:434] fc6 <- pool5
I0502 21:16:47.252852 28586 net.cpp:408] fc6 -> fc6
I0502 21:16:47.631119 28586 net.cpp:150] Setting up fc6
I0502 21:16:47.631171 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:47.631176 28586 net.cpp:165] Memory required for data: 3734138880
I0502 21:16:47.631232 28586 layer_factory.hpp:77] Creating layer relu6
I0502 21:16:47.631247 28586 net.cpp:100] Creating Layer relu6
I0502 21:16:47.631252 28586 net.cpp:434] relu6 <- fc6
I0502 21:16:47.631258 28586 net.cpp:395] relu6 -> fc6 (in-place)
I0502 21:16:47.631536 28586 net.cpp:150] Setting up relu6
I0502 21:16:47.631547 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:47.631551 28586 net.cpp:165] Memory required for data: 3734286336
I0502 21:16:47.631554 28586 layer_factory.hpp:77] Creating layer drop6
I0502 21:16:47.631604 28586 net.cpp:100] Creating Layer drop6
I0502 21:16:47.631633 28586 net.cpp:434] drop6 <- fc6
I0502 21:16:47.631649 28586 net.cpp:395] drop6 -> fc6 (in-place)
I0502 21:16:47.631745 28586 net.cpp:150] Setting up drop6
I0502 21:16:47.631757 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:47.631763 28586 net.cpp:165] Memory required for data: 3734433792
I0502 21:16:47.631769 28586 layer_factory.hpp:77] Creating layer fc7
I0502 21:16:47.631784 28586 net.cpp:100] Creating Layer fc7
I0502 21:16:47.631789 28586 net.cpp:434] fc7 <- fc6
I0502 21:16:47.631798 28586 net.cpp:408] fc7 -> fc7
I0502 21:16:47.762655 28586 net.cpp:150] Setting up fc7
I0502 21:16:47.762684 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:47.762688 28586 net.cpp:165] Memory required for data: 3734581248
I0502 21:16:47.762701 28586 layer_factory.hpp:77] Creating layer relu7
I0502 21:16:47.762711 28586 net.cpp:100] Creating Layer relu7
I0502 21:16:47.762715 28586 net.cpp:434] relu7 <- fc7
I0502 21:16:47.762723 28586 net.cpp:395] relu7 -> fc7 (in-place)
I0502 21:16:47.768829 28586 net.cpp:150] Setting up relu7
I0502 21:16:47.768870 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:47.768878 28586 net.cpp:165] Memory required for data: 3734728704
I0502 21:16:47.768887 28586 layer_factory.hpp:77] Creating layer drop7
I0502 21:16:47.768906 28586 net.cpp:100] Creating Layer drop7
I0502 21:16:47.768926 28586 net.cpp:434] drop7 <- fc7
I0502 21:16:47.768946 28586 net.cpp:395] drop7 -> fc7 (in-place)
I0502 21:16:47.769039 28586 net.cpp:150] Setting up drop7
I0502 21:16:47.769062 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:47.769073 28586 net.cpp:165] Memory required for data: 3734876160
I0502 21:16:47.769083 28586 layer_factory.hpp:77] Creating layer conv1a_pos
I0502 21:16:47.769114 28586 net.cpp:100] Creating Layer conv1a_pos
I0502 21:16:47.769127 28586 net.cpp:434] conv1a_pos <- positive
I0502 21:16:47.769150 28586 net.cpp:408] conv1a_pos -> conv1a_pos
I0502 21:16:47.775952 28586 net.cpp:150] Setting up conv1a_pos
I0502 21:16:47.775988 28586 net.cpp:157] Top shape: 18 64 16 112 112 (231211008)
I0502 21:16:47.775995 28586 net.cpp:165] Memory required for data: 4659720192
I0502 21:16:47.776003 28586 net.cpp:493] Sharing parameters 'conv1a_w' owned by layer 'conv1a', param index 0
I0502 21:16:47.776013 28586 net.cpp:493] Sharing parameters 'conv1a_b' owned by layer 'conv1a', param index 1
I0502 21:16:47.776020 28586 layer_factory.hpp:77] Creating layer relu1a_pos
I0502 21:16:47.776031 28586 net.cpp:100] Creating Layer relu1a_pos
I0502 21:16:47.776041 28586 net.cpp:434] relu1a_pos <- conv1a_pos
I0502 21:16:47.776057 28586 net.cpp:395] relu1a_pos -> conv1a_pos (in-place)
I0502 21:16:47.776526 28586 net.cpp:150] Setting up relu1a_pos
I0502 21:16:47.776554 28586 net.cpp:157] Top shape: 18 64 16 112 112 (231211008)
I0502 21:16:47.776567 28586 net.cpp:165] Memory required for data: 5584564224
I0502 21:16:47.776634 28586 layer_factory.hpp:77] Creating layer pool1_pos
I0502 21:16:47.776655 28586 net.cpp:100] Creating Layer pool1_pos
I0502 21:16:47.776660 28586 net.cpp:434] pool1_pos <- conv1a_pos
I0502 21:16:47.776670 28586 net.cpp:408] pool1_pos -> pool1_pos
I0502 21:16:47.777746 28586 net.cpp:150] Setting up pool1_pos
I0502 21:16:47.777763 28586 net.cpp:157] Top shape: 18 64 16 56 56 (57802752)
I0502 21:16:47.777766 28586 net.cpp:165] Memory required for data: 5815775232
I0502 21:16:47.777770 28586 layer_factory.hpp:77] Creating layer conv2a_pos
I0502 21:16:47.777782 28586 net.cpp:100] Creating Layer conv2a_pos
I0502 21:16:47.777811 28586 net.cpp:434] conv2a_pos <- pool1_pos
I0502 21:16:47.777819 28586 net.cpp:408] conv2a_pos -> conv2a_pos
I0502 21:16:47.786950 28586 net.cpp:150] Setting up conv2a_pos
I0502 21:16:47.786967 28586 net.cpp:157] Top shape: 18 128 16 56 56 (115605504)
I0502 21:16:47.786970 28586 net.cpp:165] Memory required for data: 6278197248
I0502 21:16:47.786979 28586 net.cpp:493] Sharing parameters 'conv2a_w' owned by layer 'conv2a', param index 0
I0502 21:16:47.786984 28586 net.cpp:493] Sharing parameters 'conv2a_b' owned by layer 'conv2a', param index 1
I0502 21:16:47.786988 28586 layer_factory.hpp:77] Creating layer relu2a_pos
I0502 21:16:47.786994 28586 net.cpp:100] Creating Layer relu2a_pos
I0502 21:16:47.786998 28586 net.cpp:434] relu2a_pos <- conv2a_pos
I0502 21:16:47.787003 28586 net.cpp:395] relu2a_pos -> conv2a_pos (in-place)
I0502 21:16:47.787185 28586 net.cpp:150] Setting up relu2a_pos
I0502 21:16:47.787195 28586 net.cpp:157] Top shape: 18 128 16 56 56 (115605504)
I0502 21:16:47.787199 28586 net.cpp:165] Memory required for data: 6740619264
I0502 21:16:47.787202 28586 layer_factory.hpp:77] Creating layer pool2_pos
I0502 21:16:47.787212 28586 net.cpp:100] Creating Layer pool2_pos
I0502 21:16:47.787227 28586 net.cpp:434] pool2_pos <- conv2a_pos
I0502 21:16:47.787243 28586 net.cpp:408] pool2_pos -> pool2_pos
I0502 21:16:47.788952 28586 net.cpp:150] Setting up pool2_pos
I0502 21:16:47.788978 28586 net.cpp:157] Top shape: 18 128 8 28 28 (14450688)
I0502 21:16:47.788985 28586 net.cpp:165] Memory required for data: 6798422016
I0502 21:16:47.788991 28586 layer_factory.hpp:77] Creating layer conv3a_pos
I0502 21:16:47.789016 28586 net.cpp:100] Creating Layer conv3a_pos
I0502 21:16:47.789033 28586 net.cpp:434] conv3a_pos <- pool2_pos
I0502 21:16:47.789050 28586 net.cpp:408] conv3a_pos -> conv3a_pos
I0502 21:16:47.835774 28586 net.cpp:150] Setting up conv3a_pos
I0502 21:16:47.835799 28586 net.cpp:157] Top shape: 18 256 8 28 28 (28901376)
I0502 21:16:47.835805 28586 net.cpp:165] Memory required for data: 6914027520
I0502 21:16:47.835811 28586 net.cpp:493] Sharing parameters 'conv3a_w' owned by layer 'conv3a', param index 0
I0502 21:16:47.835819 28586 net.cpp:493] Sharing parameters 'conv3a_b' owned by layer 'conv3a', param index 1
I0502 21:16:47.835824 28586 layer_factory.hpp:77] Creating layer relu3a_pos
I0502 21:16:47.835836 28586 net.cpp:100] Creating Layer relu3a_pos
I0502 21:16:47.835841 28586 net.cpp:434] relu3a_pos <- conv3a_pos
I0502 21:16:47.835853 28586 net.cpp:395] relu3a_pos -> conv3a_pos (in-place)
I0502 21:16:47.837541 28586 net.cpp:150] Setting up relu3a_pos
I0502 21:16:47.837574 28586 net.cpp:157] Top shape: 18 256 8 28 28 (28901376)
I0502 21:16:47.837579 28586 net.cpp:165] Memory required for data: 7029633024
I0502 21:16:47.837586 28586 layer_factory.hpp:77] Creating layer pool3_pos
I0502 21:16:47.837595 28586 net.cpp:100] Creating Layer pool3_pos
I0502 21:16:47.837600 28586 net.cpp:434] pool3_pos <- conv3a_pos
I0502 21:16:47.837610 28586 net.cpp:408] pool3_pos -> pool3_pos
I0502 21:16:47.838976 28586 net.cpp:150] Setting up pool3_pos
I0502 21:16:47.838996 28586 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0502 21:16:47.839001 28586 net.cpp:165] Memory required for data: 7044083712
I0502 21:16:47.839006 28586 layer_factory.hpp:77] Creating layer conv4a_pos
I0502 21:16:47.839020 28586 net.cpp:100] Creating Layer conv4a_pos
I0502 21:16:47.839025 28586 net.cpp:434] conv4a_pos <- pool3_pos
I0502 21:16:47.839036 28586 net.cpp:408] conv4a_pos -> conv4a_pos
I0502 21:16:47.910971 28586 net.cpp:150] Setting up conv4a_pos
I0502 21:16:47.910993 28586 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0502 21:16:47.911000 28586 net.cpp:165] Memory required for data: 7058534400
I0502 21:16:47.911006 28586 net.cpp:493] Sharing parameters 'conv4a_w' owned by layer 'conv4a', param index 0
I0502 21:16:47.911011 28586 net.cpp:493] Sharing parameters 'conv4a_b' owned by layer 'conv4a', param index 1
I0502 21:16:47.911015 28586 layer_factory.hpp:77] Creating layer relu4a_pos
I0502 21:16:47.911025 28586 net.cpp:100] Creating Layer relu4a_pos
I0502 21:16:47.911056 28586 net.cpp:434] relu4a_pos <- conv4a_pos
I0502 21:16:47.911063 28586 net.cpp:395] relu4a_pos -> conv4a_pos (in-place)
I0502 21:16:47.912091 28586 net.cpp:150] Setting up relu4a_pos
I0502 21:16:47.912106 28586 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0502 21:16:47.912109 28586 net.cpp:165] Memory required for data: 7072985088
I0502 21:16:47.912114 28586 layer_factory.hpp:77] Creating layer pool4_pos
I0502 21:16:47.912124 28586 net.cpp:100] Creating Layer pool4_pos
I0502 21:16:47.912128 28586 net.cpp:434] pool4_pos <- conv4a_pos
I0502 21:16:47.912134 28586 net.cpp:408] pool4_pos -> pool4_pos
I0502 21:16:47.912374 28586 net.cpp:150] Setting up pool4_pos
I0502 21:16:47.912384 28586 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0502 21:16:47.912387 28586 net.cpp:165] Memory required for data: 7074791424
I0502 21:16:47.912391 28586 layer_factory.hpp:77] Creating layer conv5a_pos
I0502 21:16:47.912401 28586 net.cpp:100] Creating Layer conv5a_pos
I0502 21:16:47.912406 28586 net.cpp:434] conv5a_pos <- pool4_pos
I0502 21:16:47.912415 28586 net.cpp:408] conv5a_pos -> conv5a_pos
I0502 21:16:47.967763 28586 net.cpp:150] Setting up conv5a_pos
I0502 21:16:47.967780 28586 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0502 21:16:47.967795 28586 net.cpp:165] Memory required for data: 7076597760
I0502 21:16:47.967800 28586 net.cpp:493] Sharing parameters 'conv5a_w' owned by layer 'conv5a', param index 0
I0502 21:16:47.967804 28586 net.cpp:493] Sharing parameters 'conv5a_b' owned by layer 'conv5a', param index 1
I0502 21:16:47.967808 28586 layer_factory.hpp:77] Creating layer relu5a_pos
I0502 21:16:47.967813 28586 net.cpp:100] Creating Layer relu5a_pos
I0502 21:16:47.967820 28586 net.cpp:434] relu5a_pos <- conv5a_pos
I0502 21:16:47.967824 28586 net.cpp:395] relu5a_pos -> conv5a_pos (in-place)
I0502 21:16:47.968744 28586 net.cpp:150] Setting up relu5a_pos
I0502 21:16:47.968755 28586 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0502 21:16:47.968770 28586 net.cpp:165] Memory required for data: 7078404096
I0502 21:16:47.968773 28586 layer_factory.hpp:77] Creating layer pool5_pos
I0502 21:16:47.968780 28586 net.cpp:100] Creating Layer pool5_pos
I0502 21:16:47.968786 28586 net.cpp:434] pool5_pos <- conv5a_pos
I0502 21:16:47.968791 28586 net.cpp:408] pool5_pos -> pool5_pos
I0502 21:16:47.969027 28586 net.cpp:150] Setting up pool5_pos
I0502 21:16:47.969034 28586 net.cpp:157] Top shape: 18 256 1 4 4 (73728)
I0502 21:16:47.969038 28586 net.cpp:165] Memory required for data: 7078699008
I0502 21:16:47.969040 28586 layer_factory.hpp:77] Creating layer fc6_pos
I0502 21:16:47.969053 28586 net.cpp:100] Creating Layer fc6_pos
I0502 21:16:47.969055 28586 net.cpp:434] fc6_pos <- pool5_pos
I0502 21:16:47.969061 28586 net.cpp:408] fc6_pos -> fc6_pos
I0502 21:16:48.279754 28586 net.cpp:150] Setting up fc6_pos
I0502 21:16:48.279794 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:48.279799 28586 net.cpp:165] Memory required for data: 7078846464
I0502 21:16:48.279806 28586 net.cpp:493] Sharing parameters 'fc6_w' owned by layer 'fc6', param index 0
I0502 21:16:48.279814 28586 net.cpp:493] Sharing parameters 'fc6_b' owned by layer 'fc6', param index 1
I0502 21:16:48.279816 28586 layer_factory.hpp:77] Creating layer relu6_pos
I0502 21:16:48.279826 28586 net.cpp:100] Creating Layer relu6_pos
I0502 21:16:48.279831 28586 net.cpp:434] relu6_pos <- fc6_pos
I0502 21:16:48.279846 28586 net.cpp:395] relu6_pos -> fc6_pos (in-place)
I0502 21:16:48.281141 28586 net.cpp:150] Setting up relu6_pos
I0502 21:16:48.281155 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:48.281159 28586 net.cpp:165] Memory required for data: 7078993920
I0502 21:16:48.281167 28586 layer_factory.hpp:77] Creating layer drop6_pos
I0502 21:16:48.281173 28586 net.cpp:100] Creating Layer drop6_pos
I0502 21:16:48.281177 28586 net.cpp:434] drop6_pos <- fc6_pos
I0502 21:16:48.281184 28586 net.cpp:395] drop6_pos -> fc6_pos (in-place)
I0502 21:16:48.281225 28586 net.cpp:150] Setting up drop6_pos
I0502 21:16:48.281235 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:48.281258 28586 net.cpp:165] Memory required for data: 7079141376
I0502 21:16:48.281261 28586 layer_factory.hpp:77] Creating layer fc7_pos
I0502 21:16:48.281272 28586 net.cpp:100] Creating Layer fc7_pos
I0502 21:16:48.281276 28586 net.cpp:434] fc7_pos <- fc6_pos
I0502 21:16:48.281283 28586 net.cpp:408] fc7_pos -> fc7_pos
I0502 21:16:48.448781 28586 net.cpp:150] Setting up fc7_pos
I0502 21:16:48.448822 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:48.448827 28586 net.cpp:165] Memory required for data: 7079288832
I0502 21:16:48.448834 28586 net.cpp:493] Sharing parameters 'fc7_w' owned by layer 'fc7', param index 0
I0502 21:16:48.448844 28586 net.cpp:493] Sharing parameters 'fc7_b' owned by layer 'fc7', param index 1
I0502 21:16:48.448849 28586 layer_factory.hpp:77] Creating layer relu7_pos
I0502 21:16:48.448860 28586 net.cpp:100] Creating Layer relu7_pos
I0502 21:16:48.448868 28586 net.cpp:434] relu7_pos <- fc7_pos
I0502 21:16:48.448879 28586 net.cpp:395] relu7_pos -> fc7_pos (in-place)
I0502 21:16:48.449162 28586 net.cpp:150] Setting up relu7_pos
I0502 21:16:48.449172 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:48.449175 28586 net.cpp:165] Memory required for data: 7079436288
I0502 21:16:48.449179 28586 layer_factory.hpp:77] Creating layer drop7_pos
I0502 21:16:48.449187 28586 net.cpp:100] Creating Layer drop7_pos
I0502 21:16:48.449192 28586 net.cpp:434] drop7_pos <- fc7_pos
I0502 21:16:48.449198 28586 net.cpp:395] drop7_pos -> fc7_pos (in-place)
I0502 21:16:48.449230 28586 net.cpp:150] Setting up drop7_pos
I0502 21:16:48.449239 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:48.449241 28586 net.cpp:165] Memory required for data: 7079583744
I0502 21:16:48.449245 28586 layer_factory.hpp:77] Creating layer conv1a_neg
I0502 21:16:48.449264 28586 net.cpp:100] Creating Layer conv1a_neg
I0502 21:16:48.449267 28586 net.cpp:434] conv1a_neg <- negative
I0502 21:16:48.449275 28586 net.cpp:408] conv1a_neg -> conv1a_neg
I0502 21:16:48.452795 28586 net.cpp:150] Setting up conv1a_neg
I0502 21:16:48.452812 28586 net.cpp:157] Top shape: 18 64 16 112 112 (231211008)
I0502 21:16:48.452816 28586 net.cpp:165] Memory required for data: 8004427776
I0502 21:16:48.452823 28586 net.cpp:493] Sharing parameters 'conv1a_w' owned by layer 'conv1a', param index 0
I0502 21:16:48.452828 28586 net.cpp:493] Sharing parameters 'conv1a_b' owned by layer 'conv1a', param index 1
I0502 21:16:48.452833 28586 layer_factory.hpp:77] Creating layer relu1a_neg
I0502 21:16:48.452841 28586 net.cpp:100] Creating Layer relu1a_neg
I0502 21:16:48.452846 28586 net.cpp:434] relu1a_neg <- conv1a_neg
I0502 21:16:48.452850 28586 net.cpp:395] relu1a_neg -> conv1a_neg (in-place)
I0502 21:16:48.453807 28586 net.cpp:150] Setting up relu1a_neg
I0502 21:16:48.453824 28586 net.cpp:157] Top shape: 18 64 16 112 112 (231211008)
I0502 21:16:48.453826 28586 net.cpp:165] Memory required for data: 8929271808
I0502 21:16:48.453830 28586 layer_factory.hpp:77] Creating layer pool1_neg
I0502 21:16:48.453837 28586 net.cpp:100] Creating Layer pool1_neg
I0502 21:16:48.453841 28586 net.cpp:434] pool1_neg <- conv1a_neg
I0502 21:16:48.453850 28586 net.cpp:408] pool1_neg -> pool1_neg
I0502 21:16:48.454098 28586 net.cpp:150] Setting up pool1_neg
I0502 21:16:48.454110 28586 net.cpp:157] Top shape: 18 64 16 56 56 (57802752)
I0502 21:16:48.454113 28586 net.cpp:165] Memory required for data: 9160482816
I0502 21:16:48.454116 28586 layer_factory.hpp:77] Creating layer conv2a_neg
I0502 21:16:48.454131 28586 net.cpp:100] Creating Layer conv2a_neg
I0502 21:16:48.454136 28586 net.cpp:434] conv2a_neg <- pool1_neg
I0502 21:16:48.454144 28586 net.cpp:408] conv2a_neg -> conv2a_neg
I0502 21:16:48.465934 28586 net.cpp:150] Setting up conv2a_neg
I0502 21:16:48.465952 28586 net.cpp:157] Top shape: 18 128 16 56 56 (115605504)
I0502 21:16:48.465962 28586 net.cpp:165] Memory required for data: 9622904832
I0502 21:16:48.465967 28586 net.cpp:493] Sharing parameters 'conv2a_w' owned by layer 'conv2a', param index 0
I0502 21:16:48.466006 28586 net.cpp:493] Sharing parameters 'conv2a_b' owned by layer 'conv2a', param index 1
I0502 21:16:48.466069 28586 layer_factory.hpp:77] Creating layer relu2a_neg
I0502 21:16:48.466112 28586 net.cpp:100] Creating Layer relu2a_neg
I0502 21:16:48.466125 28586 net.cpp:434] relu2a_neg <- conv2a_neg
I0502 21:16:48.466136 28586 net.cpp:395] relu2a_neg -> conv2a_neg (in-place)
I0502 21:16:48.467702 28586 net.cpp:150] Setting up relu2a_neg
I0502 21:16:48.467723 28586 net.cpp:157] Top shape: 18 128 16 56 56 (115605504)
I0502 21:16:48.467730 28586 net.cpp:165] Memory required for data: 10085326848
I0502 21:16:48.467736 28586 layer_factory.hpp:77] Creating layer pool2_neg
I0502 21:16:48.467748 28586 net.cpp:100] Creating Layer pool2_neg
I0502 21:16:48.467754 28586 net.cpp:434] pool2_neg <- conv2a_neg
I0502 21:16:48.467766 28586 net.cpp:408] pool2_neg -> pool2_neg
I0502 21:16:48.468135 28586 net.cpp:150] Setting up pool2_neg
I0502 21:16:48.468150 28586 net.cpp:157] Top shape: 18 128 8 28 28 (14450688)
I0502 21:16:48.468155 28586 net.cpp:165] Memory required for data: 10143129600
I0502 21:16:48.468159 28586 layer_factory.hpp:77] Creating layer conv3a_neg
I0502 21:16:48.468176 28586 net.cpp:100] Creating Layer conv3a_neg
I0502 21:16:48.468181 28586 net.cpp:434] conv3a_neg <- pool2_neg
I0502 21:16:48.468192 28586 net.cpp:408] conv3a_neg -> conv3a_neg
I0502 21:16:48.512018 28586 net.cpp:150] Setting up conv3a_neg
I0502 21:16:48.512037 28586 net.cpp:157] Top shape: 18 256 8 28 28 (28901376)
I0502 21:16:48.512042 28586 net.cpp:165] Memory required for data: 10258735104
I0502 21:16:48.512056 28586 net.cpp:493] Sharing parameters 'conv3a_w' owned by layer 'conv3a', param index 0
I0502 21:16:48.512063 28586 net.cpp:493] Sharing parameters 'conv3a_b' owned by layer 'conv3a', param index 1
I0502 21:16:48.512117 28586 layer_factory.hpp:77] Creating layer relu3a_neg
I0502 21:16:48.512138 28586 net.cpp:100] Creating Layer relu3a_neg
I0502 21:16:48.512146 28586 net.cpp:434] relu3a_neg <- conv3a_neg
I0502 21:16:48.512154 28586 net.cpp:395] relu3a_neg -> conv3a_neg (in-place)
I0502 21:16:48.512431 28586 net.cpp:150] Setting up relu3a_neg
I0502 21:16:48.512444 28586 net.cpp:157] Top shape: 18 256 8 28 28 (28901376)
I0502 21:16:48.512449 28586 net.cpp:165] Memory required for data: 10374340608
I0502 21:16:48.512454 28586 layer_factory.hpp:77] Creating layer pool3_neg
I0502 21:16:48.512466 28586 net.cpp:100] Creating Layer pool3_neg
I0502 21:16:48.512472 28586 net.cpp:434] pool3_neg <- conv3a_neg
I0502 21:16:48.512481 28586 net.cpp:408] pool3_neg -> pool3_neg
I0502 21:16:48.513715 28586 net.cpp:150] Setting up pool3_neg
I0502 21:16:48.513731 28586 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0502 21:16:48.513736 28586 net.cpp:165] Memory required for data: 10388791296
I0502 21:16:48.513741 28586 layer_factory.hpp:77] Creating layer conv4a_neg
I0502 21:16:48.513758 28586 net.cpp:100] Creating Layer conv4a_neg
I0502 21:16:48.513764 28586 net.cpp:434] conv4a_neg <- pool3_neg
I0502 21:16:48.513774 28586 net.cpp:408] conv4a_neg -> conv4a_neg
I0502 21:16:48.579802 28586 net.cpp:150] Setting up conv4a_neg
I0502 21:16:48.579836 28586 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0502 21:16:48.579841 28586 net.cpp:165] Memory required for data: 10403241984
I0502 21:16:48.579849 28586 net.cpp:493] Sharing parameters 'conv4a_w' owned by layer 'conv4a', param index 0
I0502 21:16:48.579854 28586 net.cpp:493] Sharing parameters 'conv4a_b' owned by layer 'conv4a', param index 1
I0502 21:16:48.579859 28586 layer_factory.hpp:77] Creating layer relu4a_neg
I0502 21:16:48.579871 28586 net.cpp:100] Creating Layer relu4a_neg
I0502 21:16:48.579876 28586 net.cpp:434] relu4a_neg <- conv4a_neg
I0502 21:16:48.579883 28586 net.cpp:395] relu4a_neg -> conv4a_neg (in-place)
I0502 21:16:48.580081 28586 net.cpp:150] Setting up relu4a_neg
I0502 21:16:48.580092 28586 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0502 21:16:48.580098 28586 net.cpp:165] Memory required for data: 10417692672
I0502 21:16:48.580101 28586 layer_factory.hpp:77] Creating layer pool4_neg
I0502 21:16:48.580133 28586 net.cpp:100] Creating Layer pool4_neg
I0502 21:16:48.580137 28586 net.cpp:434] pool4_neg <- conv4a_neg
I0502 21:16:48.580143 28586 net.cpp:408] pool4_neg -> pool4_neg
I0502 21:16:48.581198 28586 net.cpp:150] Setting up pool4_neg
I0502 21:16:48.581212 28586 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0502 21:16:48.581215 28586 net.cpp:165] Memory required for data: 10419499008
I0502 21:16:48.581218 28586 layer_factory.hpp:77] Creating layer conv5a_neg
I0502 21:16:48.581230 28586 net.cpp:100] Creating Layer conv5a_neg
I0502 21:16:48.581234 28586 net.cpp:434] conv5a_neg <- pool4_neg
I0502 21:16:48.581241 28586 net.cpp:408] conv5a_neg -> conv5a_neg
I0502 21:16:48.642720 28586 net.cpp:150] Setting up conv5a_neg
I0502 21:16:48.642750 28586 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0502 21:16:48.642755 28586 net.cpp:165] Memory required for data: 10421305344
I0502 21:16:48.642761 28586 net.cpp:493] Sharing parameters 'conv5a_w' owned by layer 'conv5a', param index 0
I0502 21:16:48.642767 28586 net.cpp:493] Sharing parameters 'conv5a_b' owned by layer 'conv5a', param index 1
I0502 21:16:48.642771 28586 layer_factory.hpp:77] Creating layer relu5a_neg
I0502 21:16:48.642779 28586 net.cpp:100] Creating Layer relu5a_neg
I0502 21:16:48.642786 28586 net.cpp:434] relu5a_neg <- conv5a_neg
I0502 21:16:48.642793 28586 net.cpp:395] relu5a_neg -> conv5a_neg (in-place)
I0502 21:16:48.642997 28586 net.cpp:150] Setting up relu5a_neg
I0502 21:16:48.643009 28586 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0502 21:16:48.643013 28586 net.cpp:165] Memory required for data: 10423111680
I0502 21:16:48.643018 28586 layer_factory.hpp:77] Creating layer pool5_neg
I0502 21:16:48.643028 28586 net.cpp:100] Creating Layer pool5_neg
I0502 21:16:48.643031 28586 net.cpp:434] pool5_neg <- conv5a_neg
I0502 21:16:48.643038 28586 net.cpp:408] pool5_neg -> pool5_neg
I0502 21:16:48.644084 28586 net.cpp:150] Setting up pool5_neg
I0502 21:16:48.644098 28586 net.cpp:157] Top shape: 18 256 1 4 4 (73728)
I0502 21:16:48.644101 28586 net.cpp:165] Memory required for data: 10423406592
I0502 21:16:48.644105 28586 layer_factory.hpp:77] Creating layer fc6_neg
I0502 21:16:48.644119 28586 net.cpp:100] Creating Layer fc6_neg
I0502 21:16:48.644125 28586 net.cpp:434] fc6_neg <- pool5_neg
I0502 21:16:48.644134 28586 net.cpp:408] fc6_neg -> fc6_neg
I0502 21:16:48.927261 28586 net.cpp:150] Setting up fc6_neg
I0502 21:16:48.927290 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:48.927296 28586 net.cpp:165] Memory required for data: 10423554048
I0502 21:16:48.927304 28586 net.cpp:493] Sharing parameters 'fc6_w' owned by layer 'fc6', param index 0
I0502 21:16:48.927311 28586 net.cpp:493] Sharing parameters 'fc6_b' owned by layer 'fc6', param index 1
I0502 21:16:48.927315 28586 layer_factory.hpp:77] Creating layer relu6_neg
I0502 21:16:48.927325 28586 net.cpp:100] Creating Layer relu6_neg
I0502 21:16:48.927330 28586 net.cpp:434] relu6_neg <- fc6_neg
I0502 21:16:48.927338 28586 net.cpp:395] relu6_neg -> fc6_neg (in-place)
I0502 21:16:48.927601 28586 net.cpp:150] Setting up relu6_neg
I0502 21:16:48.927613 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:48.927615 28586 net.cpp:165] Memory required for data: 10423701504
I0502 21:16:48.927619 28586 layer_factory.hpp:77] Creating layer drop6_neg
I0502 21:16:48.927640 28586 net.cpp:100] Creating Layer drop6_neg
I0502 21:16:48.927645 28586 net.cpp:434] drop6_neg <- fc6_neg
I0502 21:16:48.927650 28586 net.cpp:395] drop6_neg -> fc6_neg (in-place)
I0502 21:16:48.927693 28586 net.cpp:150] Setting up drop6_neg
I0502 21:16:48.927700 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:48.927703 28586 net.cpp:165] Memory required for data: 10423848960
I0502 21:16:48.927707 28586 layer_factory.hpp:77] Creating layer fc7_neg
I0502 21:16:48.927721 28586 net.cpp:100] Creating Layer fc7_neg
I0502 21:16:48.927724 28586 net.cpp:434] fc7_neg <- fc6_neg
I0502 21:16:48.927731 28586 net.cpp:408] fc7_neg -> fc7_neg
I0502 21:16:49.067400 28586 net.cpp:150] Setting up fc7_neg
I0502 21:16:49.067446 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:49.067472 28586 net.cpp:165] Memory required for data: 10423996416
I0502 21:16:49.067484 28586 net.cpp:493] Sharing parameters 'fc7_w' owned by layer 'fc7', param index 0
I0502 21:16:49.067490 28586 net.cpp:493] Sharing parameters 'fc7_b' owned by layer 'fc7', param index 1
I0502 21:16:49.067494 28586 layer_factory.hpp:77] Creating layer relu7_neg
I0502 21:16:49.067519 28586 net.cpp:100] Creating Layer relu7_neg
I0502 21:16:49.067529 28586 net.cpp:434] relu7_neg <- fc7_neg
I0502 21:16:49.067538 28586 net.cpp:395] relu7_neg -> fc7_neg (in-place)
I0502 21:16:49.079396 28586 net.cpp:150] Setting up relu7_neg
I0502 21:16:49.079411 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:49.079416 28586 net.cpp:165] Memory required for data: 10424143872
I0502 21:16:49.079421 28586 layer_factory.hpp:77] Creating layer drop7_neg
I0502 21:16:49.079440 28586 net.cpp:100] Creating Layer drop7_neg
I0502 21:16:49.079444 28586 net.cpp:434] drop7_neg <- fc7_neg
I0502 21:16:49.079450 28586 net.cpp:395] drop7_neg -> fc7_neg (in-place)
I0502 21:16:49.079485 28586 net.cpp:150] Setting up drop7_neg
I0502 21:16:49.079493 28586 net.cpp:157] Top shape: 18 2048 (36864)
I0502 21:16:49.079495 28586 net.cpp:165] Memory required for data: 10424291328
I0502 21:16:49.079510 28586 layer_factory.hpp:77] Creating layer loss
I0502 21:16:49.079522 28586 net.cpp:100] Creating Layer loss
I0502 21:16:49.079526 28586 net.cpp:434] loss <- fc7
I0502 21:16:49.079531 28586 net.cpp:434] loss <- fc7_pos
I0502 21:16:49.079535 28586 net.cpp:434] loss <- fc7_neg
I0502 21:16:49.079538 28586 net.cpp:408] loss -> loss
I0502 21:16:49.079609 28586 net.cpp:150] Setting up loss
I0502 21:16:49.079617 28586 net.cpp:157] Top shape: (1)
I0502 21:16:49.079618 28586 net.cpp:160]     with loss weight 1
I0502 21:16:49.079653 28586 net.cpp:165] Memory required for data: 10424291332
I0502 21:16:49.079655 28586 net.cpp:226] loss needs backward computation.
I0502 21:16:49.079659 28586 net.cpp:226] drop7_neg needs backward computation.
I0502 21:16:49.079661 28586 net.cpp:226] relu7_neg needs backward computation.
I0502 21:16:49.079664 28586 net.cpp:226] fc7_neg needs backward computation.
I0502 21:16:49.079668 28586 net.cpp:226] drop6_neg needs backward computation.
I0502 21:16:49.079670 28586 net.cpp:226] relu6_neg needs backward computation.
I0502 21:16:49.079672 28586 net.cpp:226] fc6_neg needs backward computation.
I0502 21:16:49.079676 28586 net.cpp:226] pool5_neg needs backward computation.
I0502 21:16:49.079679 28586 net.cpp:226] relu5a_neg needs backward computation.
I0502 21:16:49.079682 28586 net.cpp:226] conv5a_neg needs backward computation.
I0502 21:16:49.079686 28586 net.cpp:226] pool4_neg needs backward computation.
I0502 21:16:49.079689 28586 net.cpp:226] relu4a_neg needs backward computation.
I0502 21:16:49.079692 28586 net.cpp:226] conv4a_neg needs backward computation.
I0502 21:16:49.079696 28586 net.cpp:226] pool3_neg needs backward computation.
I0502 21:16:49.079701 28586 net.cpp:226] relu3a_neg needs backward computation.
I0502 21:16:49.079704 28586 net.cpp:226] conv3a_neg needs backward computation.
I0502 21:16:49.079707 28586 net.cpp:226] pool2_neg needs backward computation.
I0502 21:16:49.079711 28586 net.cpp:226] relu2a_neg needs backward computation.
I0502 21:16:49.079715 28586 net.cpp:226] conv2a_neg needs backward computation.
I0502 21:16:49.079717 28586 net.cpp:226] pool1_neg needs backward computation.
I0502 21:16:49.079720 28586 net.cpp:226] relu1a_neg needs backward computation.
I0502 21:16:49.079722 28586 net.cpp:226] conv1a_neg needs backward computation.
I0502 21:16:49.079726 28586 net.cpp:226] drop7_pos needs backward computation.
I0502 21:16:49.079730 28586 net.cpp:226] relu7_pos needs backward computation.
I0502 21:16:49.079732 28586 net.cpp:226] fc7_pos needs backward computation.
I0502 21:16:49.079735 28586 net.cpp:226] drop6_pos needs backward computation.
I0502 21:16:49.079738 28586 net.cpp:226] relu6_pos needs backward computation.
I0502 21:16:49.079741 28586 net.cpp:226] fc6_pos needs backward computation.
I0502 21:16:49.079758 28586 net.cpp:226] pool5_pos needs backward computation.
I0502 21:16:49.079762 28586 net.cpp:226] relu5a_pos needs backward computation.
I0502 21:16:49.079766 28586 net.cpp:226] conv5a_pos needs backward computation.
I0502 21:16:49.079769 28586 net.cpp:226] pool4_pos needs backward computation.
I0502 21:16:49.079773 28586 net.cpp:226] relu4a_pos needs backward computation.
I0502 21:16:49.079777 28586 net.cpp:226] conv4a_pos needs backward computation.
I0502 21:16:49.079779 28586 net.cpp:226] pool3_pos needs backward computation.
I0502 21:16:49.079783 28586 net.cpp:226] relu3a_pos needs backward computation.
I0502 21:16:49.079787 28586 net.cpp:226] conv3a_pos needs backward computation.
I0502 21:16:49.079790 28586 net.cpp:226] pool2_pos needs backward computation.
I0502 21:16:49.079793 28586 net.cpp:226] relu2a_pos needs backward computation.
I0502 21:16:49.079797 28586 net.cpp:226] conv2a_pos needs backward computation.
I0502 21:16:49.079799 28586 net.cpp:226] pool1_pos needs backward computation.
I0502 21:16:49.079802 28586 net.cpp:226] relu1a_pos needs backward computation.
I0502 21:16:49.079805 28586 net.cpp:226] conv1a_pos needs backward computation.
I0502 21:16:49.079808 28586 net.cpp:226] drop7 needs backward computation.
I0502 21:16:49.079812 28586 net.cpp:226] relu7 needs backward computation.
I0502 21:16:49.079814 28586 net.cpp:226] fc7 needs backward computation.
I0502 21:16:49.079818 28586 net.cpp:226] drop6 needs backward computation.
I0502 21:16:49.079821 28586 net.cpp:226] relu6 needs backward computation.
I0502 21:16:49.079824 28586 net.cpp:226] fc6 needs backward computation.
I0502 21:16:49.079828 28586 net.cpp:226] pool5 needs backward computation.
I0502 21:16:49.079833 28586 net.cpp:226] relu5a needs backward computation.
I0502 21:16:49.079836 28586 net.cpp:226] conv5a needs backward computation.
I0502 21:16:49.079839 28586 net.cpp:226] pool4 needs backward computation.
I0502 21:16:49.079843 28586 net.cpp:226] relu4a needs backward computation.
I0502 21:16:49.079845 28586 net.cpp:226] conv4a needs backward computation.
I0502 21:16:49.079849 28586 net.cpp:226] pool3 needs backward computation.
I0502 21:16:49.079852 28586 net.cpp:226] relu3a needs backward computation.
I0502 21:16:49.079854 28586 net.cpp:226] conv3a needs backward computation.
I0502 21:16:49.079859 28586 net.cpp:226] pool2 needs backward computation.
I0502 21:16:49.079861 28586 net.cpp:226] relu2a needs backward computation.
I0502 21:16:49.079864 28586 net.cpp:226] conv2a needs backward computation.
I0502 21:16:49.079867 28586 net.cpp:226] pool1 needs backward computation.
I0502 21:16:49.079870 28586 net.cpp:226] relu1a needs backward computation.
I0502 21:16:49.079874 28586 net.cpp:226] conv1a needs backward computation.
I0502 21:16:49.079877 28586 net.cpp:228] reshape_negative does not need backward computation.
I0502 21:16:49.079881 28586 net.cpp:228] reshape_positive does not need backward computation.
I0502 21:16:49.079885 28586 net.cpp:228] reshape_anchor does not need backward computation.
I0502 21:16:49.079890 28586 net.cpp:228] slicer does not need backward computation.
I0502 21:16:49.079893 28586 net.cpp:228] data does not need backward computation.
I0502 21:16:49.079895 28586 net.cpp:270] This network produces output loss
I0502 21:16:49.140410 28586 net.cpp:283] Network initialization done.
I0502 21:16:49.142792 28586 solver.cpp:181] Creating test net (#0) specified by net file: ../../models/three_stream_triplet_loss.prototxt
I0502 21:16:49.142948 28586 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0502 21:16:49.143609 28586 net.cpp:58] Initializing net from parameters: 
name: "C3D-Three-Streams"
state {
  phase: TEST
  stage: "test-on-val"
}
layer {
  name: "data"
  type: "Data"
  top: "triplet"
  include {
    phase: TEST
    stage: "test-on-val"
  }
  transform_param {
    mirror: false
    crop_size: 112
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
  }
  data_param {
    source: "/data/leo-data/Synthetic/LMDB/Triplets/val"
    batch_size: 1
    backend: LMDB
  }
}
layer {
  name: "slicer"
  type: "Slice"
  bottom: "triplet"
  top: "anchor_stacked"
  top: "positive_stacked"
  top: "negative_stacked"
  slice_param {
    slice_dim: 1
    slice_point: 48
    slice_point: 96
  }
}
layer {
  name: "reshape_anchor"
  type: "Reshape"
  bottom: "anchor_stacked"
  top: "anchor"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "reshape_positive"
  type: "Reshape"
  bottom: "positive_stacked"
  top: "positive"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "reshape_negative"
  type: "Reshape"
  bottom: "negative_stacked"
  top: "negative"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "conv1a"
  type: "NdConvolution"
  bottom: "anchor"
  top: "conv1a"
  param {
    name: "conv1a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "pool1"
  type: "NdPooling"
  bottom: "conv1a"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a"
  type: "NdConvolution"
  bottom: "pool1"
  top: "conv2a"
  param {
    name: "conv2a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a"
  type: "ReLU"
  bottom: "conv2a"
  top: "conv2a"
}
layer {
  name: "pool2"
  type: "NdPooling"
  bottom: "conv2a"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a"
  type: "NdConvolution"
  bottom: "pool2"
  top: "conv3a"
  param {
    name: "conv3a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a"
  type: "ReLU"
  bottom: "conv3a"
  top: "conv3a"
}
layer {
  name: "pool3"
  type: "NdPooling"
  bottom: "conv3a"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a"
  type: "NdConvolution"
  bottom: "pool3"
  top: "conv4a"
  param {
    name: "conv4a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a"
  type: "ReLU"
  bottom: "conv4a"
  top: "conv4a"
}
layer {
  name: "pool4"
  type: "NdPooling"
  bottom: "conv4a"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a"
  type: "NdConvolution"
  bottom: "pool4"
  top: "conv5a"
  param {
    name: "conv5a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a"
  type: "ReLU"
  bottom: "conv5a"
  top: "conv5a"
}
layer {
  name: "pool5"
  type: "NdPooling"
  bottom: "conv5a"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv1a_pos"
  type: "NdConvolution"
  bottom: "positive"
  top: "conv1a_pos"
  param {
    name: "conv1a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a_pos"
  type: "ReLU"
  bottom: "conv1a_pos"
  top: "conv1a_pos"
}
layer {
  name: "pool1_pos"
  type: "NdPooling"
  bottom: "conv1a_pos"
  top: "pool1_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a_pos"
  type: "NdConvolution"
  bottom: "pool1_pos"
  top: "conv2a_pos"
  param {
    name: "conv2a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a_pos"
  type: "ReLU"
  bottom: "conv2a_pos"
  top: "conv2a_pos"
}
layer {
  name: "pool2_pos"
  type: "NdPooling"
  bottom: "conv2a_pos"
  top: "pool2_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a_pos"
  type: "NdConvolution"
  bottom: "pool2_pos"
  top: "conv3a_pos"
  param {
    name: "conv3a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a_pos"
  type: "ReLU"
  bottom: "conv3a_pos"
  top: "conv3a_pos"
}
layer {
  name: "pool3_pos"
  type: "NdPooling"
  bottom: "conv3a_pos"
  top: "pool3_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a_pos"
  type: "NdConvolution"
  bottom: "pool3_pos"
  top: "conv4a_pos"
  param {
    name: "conv4a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a_pos"
  type: "ReLU"
  bottom: "conv4a_pos"
  top: "conv4a_pos"
}
layer {
  name: "pool4_pos"
  type: "NdPooling"
  bottom: "conv4a_pos"
  top: "pool4_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a_pos"
  type: "NdConvolution"
  bottom: "pool4_pos"
  top: "conv5a_pos"
  param {
    name: "conv5a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a_pos"
  type: "ReLU"
  bottom: "conv5a_pos"
  top: "conv5a_pos"
}
layer {
  name: "pool5_pos"
  type: "NdPooling"
  bottom: "conv5a_pos"
  top: "pool5_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6_pos"
  type: "InnerProduct"
  bottom: "pool5_pos"
  top: "fc6_pos"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6_pos"
  type: "ReLU"
  bottom: "fc6_pos"
  top: "fc6_pos"
}
layer {
  name: "drop6_pos"
  type: "Dropout"
  bottom: "fc6_pos"
  top: "fc6_pos"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_pos"
  type: "InnerProduct"
  bottom: "fc6_pos"
  top: "fc7_pos"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7_pos"
  type: "ReLU"
  bottom: "fc7_pos"
  top: "fc7_pos"
}
layer {
  name: "drop7_pos"
  type: "Dropout"
  bottom: "fc7_pos"
  top: "fc7_pos"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv1a_neg"
  type: "NdConvolution"
  bottom: "negative"
  top: "conv1a_neg"
  param {
    name: "conv1a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a_neg"
  type: "ReLU"
  bottom: "conv1a_neg"
  top: "conv1a_neg"
}
layer {
  name: "pool1_neg"
  type: "NdPooling"
  bottom: "conv1a_neg"
  top: "pool1_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a_neg"
  type: "NdConvolution"
  bottom: "pool1_neg"
  top: "conv2a_neg"
  param {
    name: "conv2a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a_neg"
  type: "ReLU"
  bottom: "conv2a_neg"
  top: "conv2a_neg"
}
layer {
  name: "pool2_neg"
  type: "NdPooling"
  bottom: "conv2a_neg"
  top: "pool2_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a_neg"
  type: "NdConvolution"
  bottom: "pool2_neg"
  top: "conv3a_neg"
  param {
    name: "conv3a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a_neg"
  type: "ReLU"
  bottom: "conv3a_neg"
  top: "conv3a_neg"
}
layer {
  name: "pool3_neg"
  type: "NdPooling"
  bottom: "conv3a_neg"
  top: "pool3_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a_neg"
  type: "NdConvolution"
  bottom: "pool3_neg"
  top: "conv4a_neg"
  param {
    name: "conv4a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a_neg"
  type: "ReLU"
  bottom: "conv4a_neg"
  top: "conv4a_neg"
}
layer {
  name: "pool4_neg"
  type: "NdPooling"
  bottom: "conv4a_neg"
  top: "pool4_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a_neg"
  type: "NdConvolution"
  bottom: "pool4_neg"
  top: "conv5a_neg"
  param {
    name: "conv5a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a_neg"
  type: "ReLU"
  bottom: "conv5a_neg"
  top: "conv5a_neg"
}
layer {
  name: "pool5_neg"
  type: "NdPooling"
  bottom: "conv5a_neg"
  top: "pool5_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6_neg"
  type: "InnerProduct"
  bottom: "pool5_neg"
  top: "fc6_neg"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6_neg"
  type: "ReLU"
  bottom: "fc6_neg"
  top: "fc6_neg"
}
layer {
  name: "drop6_neg"
  type: "Dropout"
  bottom: "fc6_neg"
  top: "fc6_neg"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_neg"
  type: "InnerProduct"
  bottom: "fc6_neg"
  top: "fc7_neg"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7_neg"
  type: "ReLU"
  bottom: "fc7_neg"
  top: "fc7_neg"
}
layer {
  name: "drop7_neg"
  type: "Dropout"
  bottom: "fc7_neg"
  top: "fc7_neg"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "TripletLoss"
  bottom: "fc7"
  bottom: "fc7_pos"
  bottom: "fc7_neg"
  top: "loss"
  threshold_param {
    threshold: 1
  }
}
I0502 21:16:49.143863 28586 layer_factory.hpp:77] Creating layer data
I0502 21:16:49.143929 28586 net.cpp:100] Creating Layer data
I0502 21:16:49.143937 28586 net.cpp:408] data -> triplet
I0502 21:16:49.162483 28612 db_lmdb.cpp:35] Opened lmdb /data/leo-data/Synthetic/LMDB/Triplets/val
I0502 21:16:49.166800 28586 data_layer.cpp:41] output data size: 1,144,112,112
I0502 21:16:49.185000 28586 net.cpp:150] Setting up data
I0502 21:16:49.185045 28586 net.cpp:157] Top shape: 1 144 112 112 (1806336)
I0502 21:16:49.185048 28586 net.cpp:165] Memory required for data: 7225344
I0502 21:16:49.185055 28586 layer_factory.hpp:77] Creating layer slicer
I0502 21:16:49.185071 28586 net.cpp:100] Creating Layer slicer
I0502 21:16:49.185075 28586 net.cpp:434] slicer <- triplet
I0502 21:16:49.185082 28586 net.cpp:408] slicer -> anchor_stacked
I0502 21:16:49.185092 28586 net.cpp:408] slicer -> positive_stacked
I0502 21:16:49.185101 28586 net.cpp:408] slicer -> negative_stacked
I0502 21:16:49.185241 28586 net.cpp:150] Setting up slicer
I0502 21:16:49.185250 28586 net.cpp:157] Top shape: 1 48 112 112 (602112)
I0502 21:16:49.185256 28586 net.cpp:157] Top shape: 1 48 112 112 (602112)
I0502 21:16:49.185258 28586 net.cpp:157] Top shape: 1 48 112 112 (602112)
I0502 21:16:49.185261 28586 net.cpp:165] Memory required for data: 14450688
I0502 21:16:49.185264 28586 layer_factory.hpp:77] Creating layer reshape_anchor
I0502 21:16:49.185273 28586 net.cpp:100] Creating Layer reshape_anchor
I0502 21:16:49.185278 28586 net.cpp:434] reshape_anchor <- anchor_stacked
I0502 21:16:49.185286 28586 net.cpp:408] reshape_anchor -> anchor
I0502 21:16:49.185379 28586 net.cpp:150] Setting up reshape_anchor
I0502 21:16:49.185389 28586 net.cpp:157] Top shape: 1 3 16 112 112 (602112)
I0502 21:16:49.185391 28586 net.cpp:165] Memory required for data: 16859136
I0502 21:16:49.185395 28586 layer_factory.hpp:77] Creating layer reshape_positive
I0502 21:16:49.185400 28586 net.cpp:100] Creating Layer reshape_positive
I0502 21:16:49.185403 28586 net.cpp:434] reshape_positive <- positive_stacked
I0502 21:16:49.185413 28586 net.cpp:408] reshape_positive -> positive
I0502 21:16:49.185446 28586 net.cpp:150] Setting up reshape_positive
I0502 21:16:49.185452 28586 net.cpp:157] Top shape: 1 3 16 112 112 (602112)
I0502 21:16:49.185456 28586 net.cpp:165] Memory required for data: 19267584
I0502 21:16:49.185458 28586 layer_factory.hpp:77] Creating layer reshape_negative
I0502 21:16:49.185464 28586 net.cpp:100] Creating Layer reshape_negative
I0502 21:16:49.185468 28586 net.cpp:434] reshape_negative <- negative_stacked
I0502 21:16:49.185473 28586 net.cpp:408] reshape_negative -> negative
I0502 21:16:49.185504 28586 net.cpp:150] Setting up reshape_negative
I0502 21:16:49.185510 28586 net.cpp:157] Top shape: 1 3 16 112 112 (602112)
I0502 21:16:49.185513 28586 net.cpp:165] Memory required for data: 21676032
I0502 21:16:49.185516 28586 layer_factory.hpp:77] Creating layer conv1a
I0502 21:16:49.185528 28586 net.cpp:100] Creating Layer conv1a
I0502 21:16:49.185533 28586 net.cpp:434] conv1a <- anchor
I0502 21:16:49.185541 28586 net.cpp:408] conv1a -> conv1a
I0502 21:16:49.192438 28586 net.cpp:150] Setting up conv1a
I0502 21:16:49.192453 28586 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0502 21:16:49.192457 28586 net.cpp:165] Memory required for data: 73056256
I0502 21:16:49.192507 28586 layer_factory.hpp:77] Creating layer relu1a
I0502 21:16:49.192517 28586 net.cpp:100] Creating Layer relu1a
I0502 21:16:49.192523 28586 net.cpp:434] relu1a <- conv1a
I0502 21:16:49.192526 28586 net.cpp:395] relu1a -> conv1a (in-place)
I0502 21:16:49.193393 28586 net.cpp:150] Setting up relu1a
I0502 21:16:49.193410 28586 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0502 21:16:49.193426 28586 net.cpp:165] Memory required for data: 124436480
I0502 21:16:49.193430 28586 layer_factory.hpp:77] Creating layer pool1
I0502 21:16:49.193439 28586 net.cpp:100] Creating Layer pool1
I0502 21:16:49.193444 28586 net.cpp:434] pool1 <- conv1a
I0502 21:16:49.193451 28586 net.cpp:408] pool1 -> pool1
I0502 21:16:49.194489 28586 net.cpp:150] Setting up pool1
I0502 21:16:49.194501 28586 net.cpp:157] Top shape: 1 64 16 56 56 (3211264)
I0502 21:16:49.194505 28586 net.cpp:165] Memory required for data: 137281536
I0502 21:16:49.194521 28586 layer_factory.hpp:77] Creating layer conv2a
I0502 21:16:49.194533 28586 net.cpp:100] Creating Layer conv2a
I0502 21:16:49.194537 28586 net.cpp:434] conv2a <- pool1
I0502 21:16:49.194556 28586 net.cpp:408] conv2a -> conv2a
I0502 21:16:49.202847 28586 net.cpp:150] Setting up conv2a
I0502 21:16:49.202860 28586 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0502 21:16:49.202864 28586 net.cpp:165] Memory required for data: 162971648
I0502 21:16:49.202885 28586 layer_factory.hpp:77] Creating layer relu2a
I0502 21:16:49.202893 28586 net.cpp:100] Creating Layer relu2a
I0502 21:16:49.202896 28586 net.cpp:434] relu2a <- conv2a
I0502 21:16:49.202921 28586 net.cpp:395] relu2a -> conv2a (in-place)
I0502 21:16:49.203848 28586 net.cpp:150] Setting up relu2a
I0502 21:16:49.203861 28586 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0502 21:16:49.203865 28586 net.cpp:165] Memory required for data: 188661760
I0502 21:16:49.203867 28586 layer_factory.hpp:77] Creating layer pool2
I0502 21:16:49.203886 28586 net.cpp:100] Creating Layer pool2
I0502 21:16:49.203889 28586 net.cpp:434] pool2 <- conv2a
I0502 21:16:49.203907 28586 net.cpp:408] pool2 -> pool2
I0502 21:16:49.204836 28586 net.cpp:150] Setting up pool2
I0502 21:16:49.204849 28586 net.cpp:157] Top shape: 1 128 8 28 28 (802816)
I0502 21:16:49.204852 28586 net.cpp:165] Memory required for data: 191873024
I0502 21:16:49.204855 28586 layer_factory.hpp:77] Creating layer conv3a
I0502 21:16:49.204875 28586 net.cpp:100] Creating Layer conv3a
I0502 21:16:49.204892 28586 net.cpp:434] conv3a <- pool2
I0502 21:16:49.204901 28586 net.cpp:408] conv3a -> conv3a
I0502 21:16:49.236460 28586 net.cpp:150] Setting up conv3a
I0502 21:16:49.236501 28586 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0502 21:16:49.236505 28586 net.cpp:165] Memory required for data: 198295552
I0502 21:16:49.236521 28586 layer_factory.hpp:77] Creating layer relu3a
I0502 21:16:49.236531 28586 net.cpp:100] Creating Layer relu3a
I0502 21:16:49.236536 28586 net.cpp:434] relu3a <- conv3a
I0502 21:16:49.236544 28586 net.cpp:395] relu3a -> conv3a (in-place)
I0502 21:16:49.237519 28586 net.cpp:150] Setting up relu3a
I0502 21:16:49.237530 28586 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0502 21:16:49.237545 28586 net.cpp:165] Memory required for data: 204718080
I0502 21:16:49.237548 28586 layer_factory.hpp:77] Creating layer pool3
I0502 21:16:49.237555 28586 net.cpp:100] Creating Layer pool3
I0502 21:16:49.237558 28586 net.cpp:434] pool3 <- conv3a
I0502 21:16:49.237566 28586 net.cpp:408] pool3 -> pool3
I0502 21:16:49.237820 28586 net.cpp:150] Setting up pool3
I0502 21:16:49.237830 28586 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0502 21:16:49.237833 28586 net.cpp:165] Memory required for data: 205520896
I0502 21:16:49.237836 28586 layer_factory.hpp:77] Creating layer conv4a
I0502 21:16:49.237846 28586 net.cpp:100] Creating Layer conv4a
I0502 21:16:49.237850 28586 net.cpp:434] conv4a <- pool3
I0502 21:16:49.237856 28586 net.cpp:408] conv4a -> conv4a
I0502 21:16:49.293737 28586 net.cpp:150] Setting up conv4a
I0502 21:16:49.293777 28586 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0502 21:16:49.293799 28586 net.cpp:165] Memory required for data: 206323712
I0502 21:16:49.293810 28586 layer_factory.hpp:77] Creating layer relu4a
I0502 21:16:49.293830 28586 net.cpp:100] Creating Layer relu4a
I0502 21:16:49.293834 28586 net.cpp:434] relu4a <- conv4a
I0502 21:16:49.293843 28586 net.cpp:395] relu4a -> conv4a (in-place)
I0502 21:16:49.294881 28586 net.cpp:150] Setting up relu4a
I0502 21:16:49.294894 28586 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0502 21:16:49.294924 28586 net.cpp:165] Memory required for data: 207126528
I0502 21:16:49.294929 28586 layer_factory.hpp:77] Creating layer pool4
I0502 21:16:49.294955 28586 net.cpp:100] Creating Layer pool4
I0502 21:16:49.294957 28586 net.cpp:434] pool4 <- conv4a
I0502 21:16:49.294965 28586 net.cpp:408] pool4 -> pool4
I0502 21:16:49.295224 28586 net.cpp:150] Setting up pool4
I0502 21:16:49.295234 28586 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0502 21:16:49.295238 28586 net.cpp:165] Memory required for data: 207226880
I0502 21:16:49.295240 28586 layer_factory.hpp:77] Creating layer conv5a
I0502 21:16:49.295253 28586 net.cpp:100] Creating Layer conv5a
I0502 21:16:49.295258 28586 net.cpp:434] conv5a <- pool4
I0502 21:16:49.295264 28586 net.cpp:408] conv5a -> conv5a
I0502 21:16:49.350149 28586 net.cpp:150] Setting up conv5a
I0502 21:16:49.350191 28586 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0502 21:16:49.350196 28586 net.cpp:165] Memory required for data: 207327232
I0502 21:16:49.350210 28586 layer_factory.hpp:77] Creating layer relu5a
I0502 21:16:49.350221 28586 net.cpp:100] Creating Layer relu5a
I0502 21:16:49.350226 28586 net.cpp:434] relu5a <- conv5a
I0502 21:16:49.350232 28586 net.cpp:395] relu5a -> conv5a (in-place)
I0502 21:16:49.351196 28586 net.cpp:150] Setting up relu5a
I0502 21:16:49.351210 28586 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0502 21:16:49.351213 28586 net.cpp:165] Memory required for data: 207427584
I0502 21:16:49.351217 28586 layer_factory.hpp:77] Creating layer pool5
I0502 21:16:49.351227 28586 net.cpp:100] Creating Layer pool5
I0502 21:16:49.351241 28586 net.cpp:434] pool5 <- conv5a
I0502 21:16:49.351251 28586 net.cpp:408] pool5 -> pool5
I0502 21:16:49.351506 28586 net.cpp:150] Setting up pool5
I0502 21:16:49.351518 28586 net.cpp:157] Top shape: 1 256 1 4 4 (4096)
I0502 21:16:49.351521 28586 net.cpp:165] Memory required for data: 207443968
I0502 21:16:49.351524 28586 layer_factory.hpp:77] Creating layer fc6
I0502 21:16:49.351534 28586 net.cpp:100] Creating Layer fc6
I0502 21:16:49.351537 28586 net.cpp:434] fc6 <- pool5
I0502 21:16:49.351544 28586 net.cpp:408] fc6 -> fc6
I0502 21:16:49.597417 28586 net.cpp:150] Setting up fc6
I0502 21:16:49.597460 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:49.597465 28586 net.cpp:165] Memory required for data: 207452160
I0502 21:16:49.597478 28586 layer_factory.hpp:77] Creating layer relu6
I0502 21:16:49.597491 28586 net.cpp:100] Creating Layer relu6
I0502 21:16:49.597506 28586 net.cpp:434] relu6 <- fc6
I0502 21:16:49.597514 28586 net.cpp:395] relu6 -> fc6 (in-place)
I0502 21:16:49.598615 28586 net.cpp:150] Setting up relu6
I0502 21:16:49.598628 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:49.598642 28586 net.cpp:165] Memory required for data: 207460352
I0502 21:16:49.598646 28586 layer_factory.hpp:77] Creating layer drop6
I0502 21:16:49.598652 28586 net.cpp:100] Creating Layer drop6
I0502 21:16:49.598655 28586 net.cpp:434] drop6 <- fc6
I0502 21:16:49.598662 28586 net.cpp:395] drop6 -> fc6 (in-place)
I0502 21:16:49.598707 28586 net.cpp:150] Setting up drop6
I0502 21:16:49.598714 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:49.598717 28586 net.cpp:165] Memory required for data: 207468544
I0502 21:16:49.598721 28586 layer_factory.hpp:77] Creating layer fc7
I0502 21:16:49.598731 28586 net.cpp:100] Creating Layer fc7
I0502 21:16:49.598733 28586 net.cpp:434] fc7 <- fc6
I0502 21:16:49.598739 28586 net.cpp:408] fc7 -> fc7
I0502 21:16:49.763957 28586 net.cpp:150] Setting up fc7
I0502 21:16:49.763996 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:49.764001 28586 net.cpp:165] Memory required for data: 207476736
I0502 21:16:49.764047 28586 layer_factory.hpp:77] Creating layer relu7
I0502 21:16:49.764070 28586 net.cpp:100] Creating Layer relu7
I0502 21:16:49.764077 28586 net.cpp:434] relu7 <- fc7
I0502 21:16:49.764091 28586 net.cpp:395] relu7 -> fc7 (in-place)
I0502 21:16:49.764417 28586 net.cpp:150] Setting up relu7
I0502 21:16:49.764431 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:49.764436 28586 net.cpp:165] Memory required for data: 207484928
I0502 21:16:49.764439 28586 layer_factory.hpp:77] Creating layer drop7
I0502 21:16:49.764449 28586 net.cpp:100] Creating Layer drop7
I0502 21:16:49.764453 28586 net.cpp:434] drop7 <- fc7
I0502 21:16:49.764462 28586 net.cpp:395] drop7 -> fc7 (in-place)
I0502 21:16:49.764513 28586 net.cpp:150] Setting up drop7
I0502 21:16:49.764523 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:49.764530 28586 net.cpp:165] Memory required for data: 207493120
I0502 21:16:49.764539 28586 layer_factory.hpp:77] Creating layer conv1a_pos
I0502 21:16:49.764559 28586 net.cpp:100] Creating Layer conv1a_pos
I0502 21:16:49.764565 28586 net.cpp:434] conv1a_pos <- positive
I0502 21:16:49.764581 28586 net.cpp:408] conv1a_pos -> conv1a_pos
I0502 21:16:49.773484 28586 net.cpp:150] Setting up conv1a_pos
I0502 21:16:49.773511 28586 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0502 21:16:49.773516 28586 net.cpp:165] Memory required for data: 258873344
I0502 21:16:49.773524 28586 net.cpp:493] Sharing parameters 'conv1a_w' owned by layer 'conv1a', param index 0
I0502 21:16:49.773531 28586 net.cpp:493] Sharing parameters 'conv1a_b' owned by layer 'conv1a', param index 1
I0502 21:16:49.773535 28586 layer_factory.hpp:77] Creating layer relu1a_pos
I0502 21:16:49.773546 28586 net.cpp:100] Creating Layer relu1a_pos
I0502 21:16:49.773552 28586 net.cpp:434] relu1a_pos <- conv1a_pos
I0502 21:16:49.773564 28586 net.cpp:395] relu1a_pos -> conv1a_pos (in-place)
I0502 21:16:49.774668 28586 net.cpp:150] Setting up relu1a_pos
I0502 21:16:49.774682 28586 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0502 21:16:49.774684 28586 net.cpp:165] Memory required for data: 310253568
I0502 21:16:49.774688 28586 layer_factory.hpp:77] Creating layer pool1_pos
I0502 21:16:49.774699 28586 net.cpp:100] Creating Layer pool1_pos
I0502 21:16:49.774703 28586 net.cpp:434] pool1_pos <- conv1a_pos
I0502 21:16:49.774708 28586 net.cpp:408] pool1_pos -> pool1_pos
I0502 21:16:49.775001 28586 net.cpp:150] Setting up pool1_pos
I0502 21:16:49.775015 28586 net.cpp:157] Top shape: 1 64 16 56 56 (3211264)
I0502 21:16:49.775019 28586 net.cpp:165] Memory required for data: 323098624
I0502 21:16:49.775022 28586 layer_factory.hpp:77] Creating layer conv2a_pos
I0502 21:16:49.775032 28586 net.cpp:100] Creating Layer conv2a_pos
I0502 21:16:49.775035 28586 net.cpp:434] conv2a_pos <- pool1_pos
I0502 21:16:49.775043 28586 net.cpp:408] conv2a_pos -> conv2a_pos
I0502 21:16:49.786312 28586 net.cpp:150] Setting up conv2a_pos
I0502 21:16:49.786341 28586 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0502 21:16:49.786345 28586 net.cpp:165] Memory required for data: 348788736
I0502 21:16:49.786355 28586 net.cpp:493] Sharing parameters 'conv2a_w' owned by layer 'conv2a', param index 0
I0502 21:16:49.786361 28586 net.cpp:493] Sharing parameters 'conv2a_b' owned by layer 'conv2a', param index 1
I0502 21:16:49.786365 28586 layer_factory.hpp:77] Creating layer relu2a_pos
I0502 21:16:49.786372 28586 net.cpp:100] Creating Layer relu2a_pos
I0502 21:16:49.786377 28586 net.cpp:434] relu2a_pos <- conv2a_pos
I0502 21:16:49.786386 28586 net.cpp:395] relu2a_pos -> conv2a_pos (in-place)
I0502 21:16:49.786590 28586 net.cpp:150] Setting up relu2a_pos
I0502 21:16:49.786602 28586 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0502 21:16:49.786605 28586 net.cpp:165] Memory required for data: 374478848
I0502 21:16:49.786610 28586 layer_factory.hpp:77] Creating layer pool2_pos
I0502 21:16:49.786618 28586 net.cpp:100] Creating Layer pool2_pos
I0502 21:16:49.786623 28586 net.cpp:434] pool2_pos <- conv2a_pos
I0502 21:16:49.786643 28586 net.cpp:408] pool2_pos -> pool2_pos
I0502 21:16:49.787694 28586 net.cpp:150] Setting up pool2_pos
I0502 21:16:49.787708 28586 net.cpp:157] Top shape: 1 128 8 28 28 (802816)
I0502 21:16:49.787710 28586 net.cpp:165] Memory required for data: 377690112
I0502 21:16:49.787714 28586 layer_factory.hpp:77] Creating layer conv3a_pos
I0502 21:16:49.787740 28586 net.cpp:100] Creating Layer conv3a_pos
I0502 21:16:49.787745 28586 net.cpp:434] conv3a_pos <- pool2_pos
I0502 21:16:49.787753 28586 net.cpp:408] conv3a_pos -> conv3a_pos
I0502 21:16:49.817867 28586 net.cpp:150] Setting up conv3a_pos
I0502 21:16:49.817901 28586 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0502 21:16:49.817905 28586 net.cpp:165] Memory required for data: 384112640
I0502 21:16:49.817910 28586 net.cpp:493] Sharing parameters 'conv3a_w' owned by layer 'conv3a', param index 0
I0502 21:16:49.817916 28586 net.cpp:493] Sharing parameters 'conv3a_b' owned by layer 'conv3a', param index 1
I0502 21:16:49.817920 28586 layer_factory.hpp:77] Creating layer relu3a_pos
I0502 21:16:49.817929 28586 net.cpp:100] Creating Layer relu3a_pos
I0502 21:16:49.817945 28586 net.cpp:434] relu3a_pos <- conv3a_pos
I0502 21:16:49.817951 28586 net.cpp:395] relu3a_pos -> conv3a_pos (in-place)
I0502 21:16:49.818148 28586 net.cpp:150] Setting up relu3a_pos
I0502 21:16:49.818161 28586 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0502 21:16:49.818166 28586 net.cpp:165] Memory required for data: 390535168
I0502 21:16:49.818171 28586 layer_factory.hpp:77] Creating layer pool3_pos
I0502 21:16:49.818184 28586 net.cpp:100] Creating Layer pool3_pos
I0502 21:16:49.818192 28586 net.cpp:434] pool3_pos <- conv3a_pos
I0502 21:16:49.818202 28586 net.cpp:408] pool3_pos -> pool3_pos
I0502 21:16:49.819277 28586 net.cpp:150] Setting up pool3_pos
I0502 21:16:49.819290 28586 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0502 21:16:49.819295 28586 net.cpp:165] Memory required for data: 391337984
I0502 21:16:49.819300 28586 layer_factory.hpp:77] Creating layer conv4a_pos
I0502 21:16:49.819327 28586 net.cpp:100] Creating Layer conv4a_pos
I0502 21:16:49.819334 28586 net.cpp:434] conv4a_pos <- pool3_pos
I0502 21:16:49.819346 28586 net.cpp:408] conv4a_pos -> conv4a_pos
I0502 21:16:49.916544 28586 net.cpp:150] Setting up conv4a_pos
I0502 21:16:49.916575 28586 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0502 21:16:49.916579 28586 net.cpp:165] Memory required for data: 392140800
I0502 21:16:49.916586 28586 net.cpp:493] Sharing parameters 'conv4a_w' owned by layer 'conv4a', param index 0
I0502 21:16:49.916592 28586 net.cpp:493] Sharing parameters 'conv4a_b' owned by layer 'conv4a', param index 1
I0502 21:16:49.916599 28586 layer_factory.hpp:77] Creating layer relu4a_pos
I0502 21:16:49.916606 28586 net.cpp:100] Creating Layer relu4a_pos
I0502 21:16:49.916611 28586 net.cpp:434] relu4a_pos <- conv4a_pos
I0502 21:16:49.916616 28586 net.cpp:395] relu4a_pos -> conv4a_pos (in-place)
I0502 21:16:49.916828 28586 net.cpp:150] Setting up relu4a_pos
I0502 21:16:49.916838 28586 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0502 21:16:49.916841 28586 net.cpp:165] Memory required for data: 392943616
I0502 21:16:49.916873 28586 layer_factory.hpp:77] Creating layer pool4_pos
I0502 21:16:49.916882 28586 net.cpp:100] Creating Layer pool4_pos
I0502 21:16:49.916884 28586 net.cpp:434] pool4_pos <- conv4a_pos
I0502 21:16:49.916891 28586 net.cpp:408] pool4_pos -> pool4_pos
I0502 21:16:49.917875 28586 net.cpp:150] Setting up pool4_pos
I0502 21:16:49.917886 28586 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0502 21:16:49.917888 28586 net.cpp:165] Memory required for data: 393043968
I0502 21:16:49.917892 28586 layer_factory.hpp:77] Creating layer conv5a_pos
I0502 21:16:49.917902 28586 net.cpp:100] Creating Layer conv5a_pos
I0502 21:16:49.917906 28586 net.cpp:434] conv5a_pos <- pool4_pos
I0502 21:16:49.917914 28586 net.cpp:408] conv5a_pos -> conv5a_pos
I0502 21:16:50.004631 28586 net.cpp:150] Setting up conv5a_pos
I0502 21:16:50.004663 28586 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0502 21:16:50.004668 28586 net.cpp:165] Memory required for data: 393144320
I0502 21:16:50.004698 28586 net.cpp:493] Sharing parameters 'conv5a_w' owned by layer 'conv5a', param index 0
I0502 21:16:50.004709 28586 net.cpp:493] Sharing parameters 'conv5a_b' owned by layer 'conv5a', param index 1
I0502 21:16:50.004714 28586 layer_factory.hpp:77] Creating layer relu5a_pos
I0502 21:16:50.004727 28586 net.cpp:100] Creating Layer relu5a_pos
I0502 21:16:50.004734 28586 net.cpp:434] relu5a_pos <- conv5a_pos
I0502 21:16:50.004741 28586 net.cpp:395] relu5a_pos -> conv5a_pos (in-place)
I0502 21:16:50.007629 28586 net.cpp:150] Setting up relu5a_pos
I0502 21:16:50.007655 28586 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0502 21:16:50.007673 28586 net.cpp:165] Memory required for data: 393244672
I0502 21:16:50.007680 28586 layer_factory.hpp:77] Creating layer pool5_pos
I0502 21:16:50.007699 28586 net.cpp:100] Creating Layer pool5_pos
I0502 21:16:50.007707 28586 net.cpp:434] pool5_pos <- conv5a_pos
I0502 21:16:50.007719 28586 net.cpp:408] pool5_pos -> pool5_pos
I0502 21:16:50.008826 28586 net.cpp:150] Setting up pool5_pos
I0502 21:16:50.008841 28586 net.cpp:157] Top shape: 1 256 1 4 4 (4096)
I0502 21:16:50.008846 28586 net.cpp:165] Memory required for data: 393261056
I0502 21:16:50.008852 28586 layer_factory.hpp:77] Creating layer fc6_pos
I0502 21:16:50.008865 28586 net.cpp:100] Creating Layer fc6_pos
I0502 21:16:50.008872 28586 net.cpp:434] fc6_pos <- pool5_pos
I0502 21:16:50.008883 28586 net.cpp:408] fc6_pos -> fc6_pos
I0502 21:16:50.263104 28586 net.cpp:150] Setting up fc6_pos
I0502 21:16:50.263140 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:50.263145 28586 net.cpp:165] Memory required for data: 393269248
I0502 21:16:50.263156 28586 net.cpp:493] Sharing parameters 'fc6_w' owned by layer 'fc6', param index 0
I0502 21:16:50.263164 28586 net.cpp:493] Sharing parameters 'fc6_b' owned by layer 'fc6', param index 1
I0502 21:16:50.263180 28586 layer_factory.hpp:77] Creating layer relu6_pos
I0502 21:16:50.263196 28586 net.cpp:100] Creating Layer relu6_pos
I0502 21:16:50.263206 28586 net.cpp:434] relu6_pos <- fc6_pos
I0502 21:16:50.263218 28586 net.cpp:395] relu6_pos -> fc6_pos (in-place)
I0502 21:16:50.263525 28586 net.cpp:150] Setting up relu6_pos
I0502 21:16:50.263537 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:50.263550 28586 net.cpp:165] Memory required for data: 393277440
I0502 21:16:50.263556 28586 layer_factory.hpp:77] Creating layer drop6_pos
I0502 21:16:50.263568 28586 net.cpp:100] Creating Layer drop6_pos
I0502 21:16:50.263576 28586 net.cpp:434] drop6_pos <- fc6_pos
I0502 21:16:50.263582 28586 net.cpp:395] drop6_pos -> fc6_pos (in-place)
I0502 21:16:50.263630 28586 net.cpp:150] Setting up drop6_pos
I0502 21:16:50.263639 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:50.263644 28586 net.cpp:165] Memory required for data: 393285632
I0502 21:16:50.263649 28586 layer_factory.hpp:77] Creating layer fc7_pos
I0502 21:16:50.263671 28586 net.cpp:100] Creating Layer fc7_pos
I0502 21:16:50.263677 28586 net.cpp:434] fc7_pos <- fc6_pos
I0502 21:16:50.263689 28586 net.cpp:408] fc7_pos -> fc7_pos
I0502 21:16:50.395685 28586 net.cpp:150] Setting up fc7_pos
I0502 21:16:50.395714 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:50.395720 28586 net.cpp:165] Memory required for data: 393293824
I0502 21:16:50.395731 28586 net.cpp:493] Sharing parameters 'fc7_w' owned by layer 'fc7', param index 0
I0502 21:16:50.395750 28586 net.cpp:493] Sharing parameters 'fc7_b' owned by layer 'fc7', param index 1
I0502 21:16:50.395756 28586 layer_factory.hpp:77] Creating layer relu7_pos
I0502 21:16:50.395768 28586 net.cpp:100] Creating Layer relu7_pos
I0502 21:16:50.395778 28586 net.cpp:434] relu7_pos <- fc7_pos
I0502 21:16:50.395792 28586 net.cpp:395] relu7_pos -> fc7_pos (in-place)
I0502 21:16:50.405148 28586 net.cpp:150] Setting up relu7_pos
I0502 21:16:50.405164 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:50.405170 28586 net.cpp:165] Memory required for data: 393302016
I0502 21:16:50.405187 28586 layer_factory.hpp:77] Creating layer drop7_pos
I0502 21:16:50.405230 28586 net.cpp:100] Creating Layer drop7_pos
I0502 21:16:50.405239 28586 net.cpp:434] drop7_pos <- fc7_pos
I0502 21:16:50.405247 28586 net.cpp:395] drop7_pos -> fc7_pos (in-place)
I0502 21:16:50.405295 28586 net.cpp:150] Setting up drop7_pos
I0502 21:16:50.405305 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:50.405310 28586 net.cpp:165] Memory required for data: 393310208
I0502 21:16:50.405316 28586 layer_factory.hpp:77] Creating layer conv1a_neg
I0502 21:16:50.405336 28586 net.cpp:100] Creating Layer conv1a_neg
I0502 21:16:50.405345 28586 net.cpp:434] conv1a_neg <- negative
I0502 21:16:50.405359 28586 net.cpp:408] conv1a_neg -> conv1a_neg
I0502 21:16:50.425781 28586 net.cpp:150] Setting up conv1a_neg
I0502 21:16:50.425797 28586 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0502 21:16:50.425802 28586 net.cpp:165] Memory required for data: 444690432
I0502 21:16:50.425809 28586 net.cpp:493] Sharing parameters 'conv1a_w' owned by layer 'conv1a', param index 0
I0502 21:16:50.425832 28586 net.cpp:493] Sharing parameters 'conv1a_b' owned by layer 'conv1a', param index 1
I0502 21:16:50.425839 28586 layer_factory.hpp:77] Creating layer relu1a_neg
I0502 21:16:50.425849 28586 net.cpp:100] Creating Layer relu1a_neg
I0502 21:16:50.425856 28586 net.cpp:434] relu1a_neg <- conv1a_neg
I0502 21:16:50.425864 28586 net.cpp:395] relu1a_neg -> conv1a_neg (in-place)
I0502 21:16:50.434849 28586 net.cpp:150] Setting up relu1a_neg
I0502 21:16:50.434882 28586 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0502 21:16:50.434890 28586 net.cpp:165] Memory required for data: 496070656
I0502 21:16:50.434896 28586 layer_factory.hpp:77] Creating layer pool1_neg
I0502 21:16:50.434938 28586 net.cpp:100] Creating Layer pool1_neg
I0502 21:16:50.434953 28586 net.cpp:434] pool1_neg <- conv1a_neg
I0502 21:16:50.434967 28586 net.cpp:408] pool1_neg -> pool1_neg
I0502 21:16:50.435382 28586 net.cpp:150] Setting up pool1_neg
I0502 21:16:50.435400 28586 net.cpp:157] Top shape: 1 64 16 56 56 (3211264)
I0502 21:16:50.435405 28586 net.cpp:165] Memory required for data: 508915712
I0502 21:16:50.435410 28586 layer_factory.hpp:77] Creating layer conv2a_neg
I0502 21:16:50.435420 28586 net.cpp:100] Creating Layer conv2a_neg
I0502 21:16:50.435425 28586 net.cpp:434] conv2a_neg <- pool1_neg
I0502 21:16:50.435436 28586 net.cpp:408] conv2a_neg -> conv2a_neg
I0502 21:16:50.455386 28586 net.cpp:150] Setting up conv2a_neg
I0502 21:16:50.455421 28586 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0502 21:16:50.455426 28586 net.cpp:165] Memory required for data: 534605824
I0502 21:16:50.455433 28586 net.cpp:493] Sharing parameters 'conv2a_w' owned by layer 'conv2a', param index 0
I0502 21:16:50.455441 28586 net.cpp:493] Sharing parameters 'conv2a_b' owned by layer 'conv2a', param index 1
I0502 21:16:50.455446 28586 layer_factory.hpp:77] Creating layer relu2a_neg
I0502 21:16:50.455456 28586 net.cpp:100] Creating Layer relu2a_neg
I0502 21:16:50.455467 28586 net.cpp:434] relu2a_neg <- conv2a_neg
I0502 21:16:50.455484 28586 net.cpp:395] relu2a_neg -> conv2a_neg (in-place)
I0502 21:16:50.456698 28586 net.cpp:150] Setting up relu2a_neg
I0502 21:16:50.456717 28586 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0502 21:16:50.456727 28586 net.cpp:165] Memory required for data: 560295936
I0502 21:16:50.456732 28586 layer_factory.hpp:77] Creating layer pool2_neg
I0502 21:16:50.456743 28586 net.cpp:100] Creating Layer pool2_neg
I0502 21:16:50.456748 28586 net.cpp:434] pool2_neg <- conv2a_neg
I0502 21:16:50.456760 28586 net.cpp:408] pool2_neg -> pool2_neg
I0502 21:16:50.457111 28586 net.cpp:150] Setting up pool2_neg
I0502 21:16:50.457125 28586 net.cpp:157] Top shape: 1 128 8 28 28 (802816)
I0502 21:16:50.457129 28586 net.cpp:165] Memory required for data: 563507200
I0502 21:16:50.457134 28586 layer_factory.hpp:77] Creating layer conv3a_neg
I0502 21:16:50.457145 28586 net.cpp:100] Creating Layer conv3a_neg
I0502 21:16:50.457150 28586 net.cpp:434] conv3a_neg <- pool2_neg
I0502 21:16:50.457162 28586 net.cpp:408] conv3a_neg -> conv3a_neg
I0502 21:16:50.505348 28586 net.cpp:150] Setting up conv3a_neg
I0502 21:16:50.505403 28586 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0502 21:16:50.505410 28586 net.cpp:165] Memory required for data: 569929728
I0502 21:16:50.505440 28586 net.cpp:493] Sharing parameters 'conv3a_w' owned by layer 'conv3a', param index 0
I0502 21:16:50.505450 28586 net.cpp:493] Sharing parameters 'conv3a_b' owned by layer 'conv3a', param index 1
I0502 21:16:50.505457 28586 layer_factory.hpp:77] Creating layer relu3a_neg
I0502 21:16:50.505477 28586 net.cpp:100] Creating Layer relu3a_neg
I0502 21:16:50.505488 28586 net.cpp:434] relu3a_neg <- conv3a_neg
I0502 21:16:50.505501 28586 net.cpp:395] relu3a_neg -> conv3a_neg (in-place)
I0502 21:16:50.507241 28586 net.cpp:150] Setting up relu3a_neg
I0502 21:16:50.507269 28586 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0502 21:16:50.507279 28586 net.cpp:165] Memory required for data: 576352256
I0502 21:16:50.507287 28586 layer_factory.hpp:77] Creating layer pool3_neg
I0502 21:16:50.507302 28586 net.cpp:100] Creating Layer pool3_neg
I0502 21:16:50.507309 28586 net.cpp:434] pool3_neg <- conv3a_neg
I0502 21:16:50.507324 28586 net.cpp:408] pool3_neg -> pool3_neg
I0502 21:16:50.507779 28586 net.cpp:150] Setting up pool3_neg
I0502 21:16:50.507798 28586 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0502 21:16:50.507805 28586 net.cpp:165] Memory required for data: 577155072
I0502 21:16:50.507813 28586 layer_factory.hpp:77] Creating layer conv4a_neg
I0502 21:16:50.507832 28586 net.cpp:100] Creating Layer conv4a_neg
I0502 21:16:50.507838 28586 net.cpp:434] conv4a_neg <- pool3_neg
I0502 21:16:50.507854 28586 net.cpp:408] conv4a_neg -> conv4a_neg
I0502 21:16:50.596959 28586 net.cpp:150] Setting up conv4a_neg
I0502 21:16:50.596994 28586 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0502 21:16:50.596999 28586 net.cpp:165] Memory required for data: 577957888
I0502 21:16:50.597017 28586 net.cpp:493] Sharing parameters 'conv4a_w' owned by layer 'conv4a', param index 0
I0502 21:16:50.597023 28586 net.cpp:493] Sharing parameters 'conv4a_b' owned by layer 'conv4a', param index 1
I0502 21:16:50.597030 28586 layer_factory.hpp:77] Creating layer relu4a_neg
I0502 21:16:50.597040 28586 net.cpp:100] Creating Layer relu4a_neg
I0502 21:16:50.597046 28586 net.cpp:434] relu4a_neg <- conv4a_neg
I0502 21:16:50.597056 28586 net.cpp:395] relu4a_neg -> conv4a_neg (in-place)
I0502 21:16:50.598227 28586 net.cpp:150] Setting up relu4a_neg
I0502 21:16:50.598242 28586 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0502 21:16:50.598254 28586 net.cpp:165] Memory required for data: 578760704
I0502 21:16:50.598258 28586 layer_factory.hpp:77] Creating layer pool4_neg
I0502 21:16:50.598270 28586 net.cpp:100] Creating Layer pool4_neg
I0502 21:16:50.598275 28586 net.cpp:434] pool4_neg <- conv4a_neg
I0502 21:16:50.598289 28586 net.cpp:408] pool4_neg -> pool4_neg
I0502 21:16:50.598598 28586 net.cpp:150] Setting up pool4_neg
I0502 21:16:50.598610 28586 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0502 21:16:50.598614 28586 net.cpp:165] Memory required for data: 578861056
I0502 21:16:50.598618 28586 layer_factory.hpp:77] Creating layer conv5a_neg
I0502 21:16:50.598633 28586 net.cpp:100] Creating Layer conv5a_neg
I0502 21:16:50.598639 28586 net.cpp:434] conv5a_neg <- pool4_neg
I0502 21:16:50.598649 28586 net.cpp:408] conv5a_neg -> conv5a_neg
I0502 21:16:50.676817 28586 net.cpp:150] Setting up conv5a_neg
I0502 21:16:50.676856 28586 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0502 21:16:50.676862 28586 net.cpp:165] Memory required for data: 578961408
I0502 21:16:50.676872 28586 net.cpp:493] Sharing parameters 'conv5a_w' owned by layer 'conv5a', param index 0
I0502 21:16:50.676882 28586 net.cpp:493] Sharing parameters 'conv5a_b' owned by layer 'conv5a', param index 1
I0502 21:16:50.676888 28586 layer_factory.hpp:77] Creating layer relu5a_neg
I0502 21:16:50.676901 28586 net.cpp:100] Creating Layer relu5a_neg
I0502 21:16:50.676908 28586 net.cpp:434] relu5a_neg <- conv5a_neg
I0502 21:16:50.676919 28586 net.cpp:395] relu5a_neg -> conv5a_neg (in-place)
I0502 21:16:50.677258 28586 net.cpp:150] Setting up relu5a_neg
I0502 21:16:50.677273 28586 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0502 21:16:50.677278 28586 net.cpp:165] Memory required for data: 579061760
I0502 21:16:50.677284 28586 layer_factory.hpp:77] Creating layer pool5_neg
I0502 21:16:50.677299 28586 net.cpp:100] Creating Layer pool5_neg
I0502 21:16:50.677305 28586 net.cpp:434] pool5_neg <- conv5a_neg
I0502 21:16:50.677315 28586 net.cpp:408] pool5_neg -> pool5_neg
I0502 21:16:50.678557 28586 net.cpp:150] Setting up pool5_neg
I0502 21:16:50.678575 28586 net.cpp:157] Top shape: 1 256 1 4 4 (4096)
I0502 21:16:50.678581 28586 net.cpp:165] Memory required for data: 579078144
I0502 21:16:50.678586 28586 layer_factory.hpp:77] Creating layer fc6_neg
I0502 21:16:50.678601 28586 net.cpp:100] Creating Layer fc6_neg
I0502 21:16:50.678613 28586 net.cpp:434] fc6_neg <- pool5_neg
I0502 21:16:50.678624 28586 net.cpp:408] fc6_neg -> fc6_neg
I0502 21:16:50.995472 28586 net.cpp:150] Setting up fc6_neg
I0502 21:16:50.995584 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:50.995609 28586 net.cpp:165] Memory required for data: 579086336
I0502 21:16:50.995632 28586 net.cpp:493] Sharing parameters 'fc6_w' owned by layer 'fc6', param index 0
I0502 21:16:50.995647 28586 net.cpp:493] Sharing parameters 'fc6_b' owned by layer 'fc6', param index 1
I0502 21:16:50.995656 28586 layer_factory.hpp:77] Creating layer relu6_neg
I0502 21:16:50.995672 28586 net.cpp:100] Creating Layer relu6_neg
I0502 21:16:50.995694 28586 net.cpp:434] relu6_neg <- fc6_neg
I0502 21:16:50.995707 28586 net.cpp:395] relu6_neg -> fc6_neg (in-place)
I0502 21:16:50.996466 28586 net.cpp:150] Setting up relu6_neg
I0502 21:16:50.996479 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:50.996482 28586 net.cpp:165] Memory required for data: 579094528
I0502 21:16:50.996486 28586 layer_factory.hpp:77] Creating layer drop6_neg
I0502 21:16:50.996512 28586 net.cpp:100] Creating Layer drop6_neg
I0502 21:16:50.996518 28586 net.cpp:434] drop6_neg <- fc6_neg
I0502 21:16:50.996526 28586 net.cpp:395] drop6_neg -> fc6_neg (in-place)
I0502 21:16:50.996567 28586 net.cpp:150] Setting up drop6_neg
I0502 21:16:50.996574 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:50.996577 28586 net.cpp:165] Memory required for data: 579102720
I0502 21:16:50.996582 28586 layer_factory.hpp:77] Creating layer fc7_neg
I0502 21:16:50.996593 28586 net.cpp:100] Creating Layer fc7_neg
I0502 21:16:50.996603 28586 net.cpp:434] fc7_neg <- fc6_neg
I0502 21:16:50.996613 28586 net.cpp:408] fc7_neg -> fc7_neg
I0502 21:16:51.144624 28586 net.cpp:150] Setting up fc7_neg
I0502 21:16:51.144664 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:51.144667 28586 net.cpp:165] Memory required for data: 579110912
I0502 21:16:51.144676 28586 net.cpp:493] Sharing parameters 'fc7_w' owned by layer 'fc7', param index 0
I0502 21:16:51.144685 28586 net.cpp:493] Sharing parameters 'fc7_b' owned by layer 'fc7', param index 1
I0502 21:16:51.144690 28586 layer_factory.hpp:77] Creating layer relu7_neg
I0502 21:16:51.144701 28586 net.cpp:100] Creating Layer relu7_neg
I0502 21:16:51.144706 28586 net.cpp:434] relu7_neg <- fc7_neg
I0502 21:16:51.144719 28586 net.cpp:395] relu7_neg -> fc7_neg (in-place)
I0502 21:16:51.151190 28586 net.cpp:150] Setting up relu7_neg
I0502 21:16:51.151207 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:51.151211 28586 net.cpp:165] Memory required for data: 579119104
I0502 21:16:51.151216 28586 layer_factory.hpp:77] Creating layer drop7_neg
I0502 21:16:51.151224 28586 net.cpp:100] Creating Layer drop7_neg
I0502 21:16:51.151228 28586 net.cpp:434] drop7_neg <- fc7_neg
I0502 21:16:51.151235 28586 net.cpp:395] drop7_neg -> fc7_neg (in-place)
I0502 21:16:51.151278 28586 net.cpp:150] Setting up drop7_neg
I0502 21:16:51.151285 28586 net.cpp:157] Top shape: 1 2048 (2048)
I0502 21:16:51.151288 28586 net.cpp:165] Memory required for data: 579127296
I0502 21:16:51.151293 28586 layer_factory.hpp:77] Creating layer loss
I0502 21:16:51.151301 28586 net.cpp:100] Creating Layer loss
I0502 21:16:51.151305 28586 net.cpp:434] loss <- fc7
I0502 21:16:51.151332 28586 net.cpp:434] loss <- fc7_pos
I0502 21:16:51.151337 28586 net.cpp:434] loss <- fc7_neg
I0502 21:16:51.151342 28586 net.cpp:408] loss -> loss
I0502 21:16:51.151422 28586 net.cpp:150] Setting up loss
I0502 21:16:51.151429 28586 net.cpp:157] Top shape: (1)
I0502 21:16:51.151432 28586 net.cpp:160]     with loss weight 1
I0502 21:16:51.151518 28586 net.cpp:165] Memory required for data: 579127300
I0502 21:16:51.151535 28586 net.cpp:226] loss needs backward computation.
I0502 21:16:51.151547 28586 net.cpp:226] drop7_neg needs backward computation.
I0502 21:16:51.151556 28586 net.cpp:226] relu7_neg needs backward computation.
I0502 21:16:51.151566 28586 net.cpp:226] fc7_neg needs backward computation.
I0502 21:16:51.151574 28586 net.cpp:226] drop6_neg needs backward computation.
I0502 21:16:51.151583 28586 net.cpp:226] relu6_neg needs backward computation.
I0502 21:16:51.151592 28586 net.cpp:226] fc6_neg needs backward computation.
I0502 21:16:51.151607 28586 net.cpp:226] pool5_neg needs backward computation.
I0502 21:16:51.151615 28586 net.cpp:226] relu5a_neg needs backward computation.
I0502 21:16:51.151626 28586 net.cpp:226] conv5a_neg needs backward computation.
I0502 21:16:51.151635 28586 net.cpp:226] pool4_neg needs backward computation.
I0502 21:16:51.151646 28586 net.cpp:226] relu4a_neg needs backward computation.
I0502 21:16:51.151654 28586 net.cpp:226] conv4a_neg needs backward computation.
I0502 21:16:51.151665 28586 net.cpp:226] pool3_neg needs backward computation.
I0502 21:16:51.151674 28586 net.cpp:226] relu3a_neg needs backward computation.
I0502 21:16:51.151684 28586 net.cpp:226] conv3a_neg needs backward computation.
I0502 21:16:51.151692 28586 net.cpp:226] pool2_neg needs backward computation.
I0502 21:16:51.151703 28586 net.cpp:226] relu2a_neg needs backward computation.
I0502 21:16:51.151715 28586 net.cpp:226] conv2a_neg needs backward computation.
I0502 21:16:51.151723 28586 net.cpp:226] pool1_neg needs backward computation.
I0502 21:16:51.151734 28586 net.cpp:226] relu1a_neg needs backward computation.
I0502 21:16:51.151742 28586 net.cpp:226] conv1a_neg needs backward computation.
I0502 21:16:51.151754 28586 net.cpp:226] drop7_pos needs backward computation.
I0502 21:16:51.151765 28586 net.cpp:226] relu7_pos needs backward computation.
I0502 21:16:51.151777 28586 net.cpp:226] fc7_pos needs backward computation.
I0502 21:16:51.151785 28586 net.cpp:226] drop6_pos needs backward computation.
I0502 21:16:51.151795 28586 net.cpp:226] relu6_pos needs backward computation.
I0502 21:16:51.151803 28586 net.cpp:226] fc6_pos needs backward computation.
I0502 21:16:51.151813 28586 net.cpp:226] pool5_pos needs backward computation.
I0502 21:16:51.151823 28586 net.cpp:226] relu5a_pos needs backward computation.
I0502 21:16:51.151834 28586 net.cpp:226] conv5a_pos needs backward computation.
I0502 21:16:51.151841 28586 net.cpp:226] pool4_pos needs backward computation.
I0502 21:16:51.151852 28586 net.cpp:226] relu4a_pos needs backward computation.
I0502 21:16:51.151861 28586 net.cpp:226] conv4a_pos needs backward computation.
I0502 21:16:51.151872 28586 net.cpp:226] pool3_pos needs backward computation.
I0502 21:16:51.151885 28586 net.cpp:226] relu3a_pos needs backward computation.
I0502 21:16:51.151895 28586 net.cpp:226] conv3a_pos needs backward computation.
I0502 21:16:51.151904 28586 net.cpp:226] pool2_pos needs backward computation.
I0502 21:16:51.151916 28586 net.cpp:226] relu2a_pos needs backward computation.
I0502 21:16:51.151923 28586 net.cpp:226] conv2a_pos needs backward computation.
I0502 21:16:51.151934 28586 net.cpp:226] pool1_pos needs backward computation.
I0502 21:16:51.151947 28586 net.cpp:226] relu1a_pos needs backward computation.
I0502 21:16:51.151957 28586 net.cpp:226] conv1a_pos needs backward computation.
I0502 21:16:51.151969 28586 net.cpp:226] drop7 needs backward computation.
I0502 21:16:51.151980 28586 net.cpp:226] relu7 needs backward computation.
I0502 21:16:51.151988 28586 net.cpp:226] fc7 needs backward computation.
I0502 21:16:51.152021 28586 net.cpp:226] drop6 needs backward computation.
I0502 21:16:51.152031 28586 net.cpp:226] relu6 needs backward computation.
I0502 21:16:51.152036 28586 net.cpp:226] fc6 needs backward computation.
I0502 21:16:51.152045 28586 net.cpp:226] pool5 needs backward computation.
I0502 21:16:51.152053 28586 net.cpp:226] relu5a needs backward computation.
I0502 21:16:51.152065 28586 net.cpp:226] conv5a needs backward computation.
I0502 21:16:51.152073 28586 net.cpp:226] pool4 needs backward computation.
I0502 21:16:51.152084 28586 net.cpp:226] relu4a needs backward computation.
I0502 21:16:51.152096 28586 net.cpp:226] conv4a needs backward computation.
I0502 21:16:51.152104 28586 net.cpp:226] pool3 needs backward computation.
I0502 21:16:51.152115 28586 net.cpp:226] relu3a needs backward computation.
I0502 21:16:51.152123 28586 net.cpp:226] conv3a needs backward computation.
I0502 21:16:51.152139 28586 net.cpp:226] pool2 needs backward computation.
I0502 21:16:51.152149 28586 net.cpp:226] relu2a needs backward computation.
I0502 21:16:51.152158 28586 net.cpp:226] conv2a needs backward computation.
I0502 21:16:51.152168 28586 net.cpp:226] pool1 needs backward computation.
I0502 21:16:51.152179 28586 net.cpp:226] relu1a needs backward computation.
I0502 21:16:51.152190 28586 net.cpp:226] conv1a needs backward computation.
I0502 21:16:51.152204 28586 net.cpp:228] reshape_negative does not need backward computation.
I0502 21:16:51.152215 28586 net.cpp:228] reshape_positive does not need backward computation.
I0502 21:16:51.152226 28586 net.cpp:228] reshape_anchor does not need backward computation.
I0502 21:16:51.152240 28586 net.cpp:228] slicer does not need backward computation.
I0502 21:16:51.152252 28586 net.cpp:228] data does not need backward computation.
I0502 21:16:51.152262 28586 net.cpp:270] This network produces output loss
I0502 21:16:51.199735 28586 net.cpp:283] Network initialization done.
I0502 21:16:51.200229 28586 solver.cpp:60] Solver scaffolding done.
I0502 21:16:51.201561 28586 caffe.cpp:155] Finetuning from ../../../c3d_ucf101_iter_38000.caffemodel
I0502 21:16:51.555415 28586 net.cpp:761] Ignoring source layer fc8
I0502 21:16:51.754709 28586 net.cpp:761] Ignoring source layer fc8
I0502 21:16:51.758074 28586 caffe.cpp:251] Starting Optimization
I0502 21:16:51.758087 28586 solver.cpp:279] Solving C3D-Three-Streams
I0502 21:16:51.758091 28586 solver.cpp:280] Learning Rate Policy: step
I0502 21:16:51.769326 28586 solver.cpp:337] Iteration 0, Testing net (#0)
I0502 21:17:20.461865 28586 solver.cpp:404]     Test net output #0: loss = 35.3498 (* 1 = 35.3498 loss)
I0502 21:17:21.430562 28586 solver.cpp:228] Iteration 0, loss = 313.542
I0502 21:17:21.430630 28586 solver.cpp:244]     Train net output #0: loss = 313.542 (* 1 = 313.542 loss)
I0502 21:17:21.430663 28586 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0502 21:19:35.548831 28586 solver.cpp:228] Iteration 50, loss = 0.54059
I0502 21:19:35.548992 28586 solver.cpp:244]     Train net output #0: loss = 0.540586 (* 1 = 0.540586 loss)
I0502 21:19:35.549006 28586 sgd_solver.cpp:106] Iteration 50, lr = 1e-05
I0502 21:21:51.477093 28586 solver.cpp:228] Iteration 100, loss = 0.469955
I0502 21:21:51.477241 28586 solver.cpp:244]     Train net output #0: loss = 0.469951 (* 1 = 0.469951 loss)
I0502 21:21:51.477252 28586 sgd_solver.cpp:106] Iteration 100, lr = 1e-05
I0502 21:24:07.101203 28586 solver.cpp:228] Iteration 150, loss = 0.440991
I0502 21:24:07.101425 28586 solver.cpp:244]     Train net output #0: loss = 0.440987 (* 1 = 0.440987 loss)
I0502 21:24:07.101469 28586 sgd_solver.cpp:106] Iteration 150, lr = 1e-05
I0502 21:26:20.216380 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_200.caffemodel
I0502 21:26:23.319718 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_200.solverstate
I0502 21:26:23.373859 28586 solver.cpp:337] Iteration 200, Testing net (#0)
I0502 21:26:52.422507 28586 solver.cpp:404]     Test net output #0: loss = 0.499798 (* 1 = 0.499798 loss)
I0502 21:26:53.339900 28586 solver.cpp:228] Iteration 200, loss = 0.464372
I0502 21:26:53.339959 28586 solver.cpp:244]     Train net output #0: loss = 0.464368 (* 1 = 0.464368 loss)
I0502 21:26:53.339969 28586 sgd_solver.cpp:106] Iteration 200, lr = 1e-05
I0502 21:29:09.080879 28586 solver.cpp:228] Iteration 250, loss = 0.533862
I0502 21:29:09.081039 28586 solver.cpp:244]     Train net output #0: loss = 0.533859 (* 1 = 0.533859 loss)
I0502 21:29:09.081051 28586 sgd_solver.cpp:106] Iteration 250, lr = 1e-05
I0502 21:31:24.952728 28586 solver.cpp:228] Iteration 300, loss = 0.474272
I0502 21:31:24.954531 28586 solver.cpp:244]     Train net output #0: loss = 0.474269 (* 1 = 0.474269 loss)
I0502 21:31:24.954558 28586 sgd_solver.cpp:106] Iteration 300, lr = 1e-05
I0502 21:33:41.144647 28586 solver.cpp:228] Iteration 350, loss = 0.48985
I0502 21:33:41.144796 28586 solver.cpp:244]     Train net output #0: loss = 0.489846 (* 1 = 0.489846 loss)
I0502 21:33:41.144807 28586 sgd_solver.cpp:106] Iteration 350, lr = 1e-05
I0502 21:35:54.304329 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_400.caffemodel
I0502 21:36:14.016831 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_400.solverstate
I0502 21:36:14.097340 28586 solver.cpp:337] Iteration 400, Testing net (#0)
I0502 21:36:19.845863 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0502 21:36:43.335022 28586 solver.cpp:404]     Test net output #0: loss = 0.498483 (* 1 = 0.498483 loss)
I0502 21:36:44.231420 28586 solver.cpp:228] Iteration 400, loss = 0.541556
I0502 21:36:44.231477 28586 solver.cpp:244]     Train net output #0: loss = 0.541553 (* 1 = 0.541553 loss)
I0502 21:36:44.231487 28586 sgd_solver.cpp:106] Iteration 400, lr = 1e-05
I0502 21:38:59.720815 28586 solver.cpp:228] Iteration 450, loss = 0.506088
I0502 21:38:59.721616 28586 solver.cpp:244]     Train net output #0: loss = 0.506084 (* 1 = 0.506084 loss)
I0502 21:38:59.721628 28586 sgd_solver.cpp:106] Iteration 450, lr = 1e-05
I0502 21:41:15.934667 28586 solver.cpp:228] Iteration 500, loss = 0.451563
I0502 21:41:15.934829 28586 solver.cpp:244]     Train net output #0: loss = 0.451559 (* 1 = 0.451559 loss)
I0502 21:41:15.934844 28586 sgd_solver.cpp:106] Iteration 500, lr = 1e-05
I0502 21:43:31.953407 28586 solver.cpp:228] Iteration 550, loss = 0.504332
I0502 21:43:31.953541 28586 solver.cpp:244]     Train net output #0: loss = 0.504328 (* 1 = 0.504328 loss)
I0502 21:43:31.953565 28586 sgd_solver.cpp:106] Iteration 550, lr = 1e-05
I0502 21:45:45.208493 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_600.caffemodel
I0502 21:45:48.157606 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_600.solverstate
I0502 21:45:48.212339 28586 solver.cpp:337] Iteration 600, Testing net (#0)
I0502 21:46:17.543916 28586 solver.cpp:404]     Test net output #0: loss = 0.495109 (* 1 = 0.495109 loss)
I0502 21:46:18.447883 28586 solver.cpp:228] Iteration 600, loss = 0.505218
I0502 21:46:18.447947 28586 solver.cpp:244]     Train net output #0: loss = 0.505214 (* 1 = 0.505214 loss)
I0502 21:46:18.447958 28586 sgd_solver.cpp:106] Iteration 600, lr = 1e-05
I0502 21:48:34.929383 28586 solver.cpp:228] Iteration 650, loss = 0.493817
I0502 21:48:34.929605 28586 solver.cpp:244]     Train net output #0: loss = 0.493813 (* 1 = 0.493813 loss)
I0502 21:48:34.929630 28586 sgd_solver.cpp:106] Iteration 650, lr = 1e-05
I0502 21:50:51.005661 28586 solver.cpp:228] Iteration 700, loss = 0.367948
I0502 21:50:51.005883 28586 solver.cpp:244]     Train net output #0: loss = 0.367945 (* 1 = 0.367945 loss)
I0502 21:50:51.005899 28586 sgd_solver.cpp:106] Iteration 700, lr = 1e-05
I0502 21:53:07.301532 28586 solver.cpp:228] Iteration 750, loss = 0.46286
I0502 21:53:07.301702 28586 solver.cpp:244]     Train net output #0: loss = 0.462856 (* 1 = 0.462856 loss)
I0502 21:53:07.301715 28586 sgd_solver.cpp:106] Iteration 750, lr = 1e-05
I0502 21:55:20.931783 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_800.caffemodel
I0502 21:55:32.843902 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_800.solverstate
I0502 21:55:32.890043 28586 solver.cpp:337] Iteration 800, Testing net (#0)
I0502 21:55:50.382081 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0502 21:56:02.220959 28586 solver.cpp:404]     Test net output #0: loss = 0.49413 (* 1 = 0.49413 loss)
I0502 21:56:03.114348 28586 solver.cpp:228] Iteration 800, loss = 0.460344
I0502 21:56:03.114424 28586 solver.cpp:244]     Train net output #0: loss = 0.46034 (* 1 = 0.46034 loss)
I0502 21:56:03.114435 28586 sgd_solver.cpp:106] Iteration 800, lr = 1e-05
I0502 21:58:19.484658 28586 solver.cpp:228] Iteration 850, loss = 0.419693
I0502 21:58:19.485798 28586 solver.cpp:244]     Train net output #0: loss = 0.41969 (* 1 = 0.41969 loss)
I0502 21:58:19.485828 28586 sgd_solver.cpp:106] Iteration 850, lr = 1e-05
I0502 22:00:35.920081 28586 solver.cpp:228] Iteration 900, loss = 0.55051
I0502 22:00:35.920267 28586 solver.cpp:244]     Train net output #0: loss = 0.550506 (* 1 = 0.550506 loss)
I0502 22:00:35.920281 28586 sgd_solver.cpp:106] Iteration 900, lr = 1e-05
I0502 22:02:51.578572 28586 solver.cpp:228] Iteration 950, loss = 0.491412
I0502 22:02:51.578727 28586 solver.cpp:244]     Train net output #0: loss = 0.491409 (* 1 = 0.491409 loss)
I0502 22:02:51.578745 28586 sgd_solver.cpp:106] Iteration 950, lr = 1e-05
I0502 22:05:05.263682 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_1000.caffemodel
I0502 22:05:14.491942 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_1000.solverstate
I0502 22:05:14.539603 28586 solver.cpp:337] Iteration 1000, Testing net (#0)
I0502 22:05:44.121426 28586 solver.cpp:404]     Test net output #0: loss = 0.439328 (* 1 = 0.439328 loss)
I0502 22:05:45.023481 28586 solver.cpp:228] Iteration 1000, loss = 0.386783
I0502 22:05:45.023555 28586 solver.cpp:244]     Train net output #0: loss = 0.38678 (* 1 = 0.38678 loss)
I0502 22:05:45.023566 28586 sgd_solver.cpp:106] Iteration 1000, lr = 1e-05
I0502 22:08:01.257987 28586 solver.cpp:228] Iteration 1050, loss = 0.279977
I0502 22:08:01.258159 28586 solver.cpp:244]     Train net output #0: loss = 0.279973 (* 1 = 0.279973 loss)
I0502 22:08:01.258172 28586 sgd_solver.cpp:106] Iteration 1050, lr = 1e-05
I0502 22:10:17.399235 28586 solver.cpp:228] Iteration 1100, loss = 0.357178
I0502 22:10:17.399384 28586 solver.cpp:244]     Train net output #0: loss = 0.357174 (* 1 = 0.357174 loss)
I0502 22:10:17.399399 28586 sgd_solver.cpp:106] Iteration 1100, lr = 1e-05
I0502 22:12:33.714359 28586 solver.cpp:228] Iteration 1150, loss = 0.271535
I0502 22:12:33.714586 28586 solver.cpp:244]     Train net output #0: loss = 0.271531 (* 1 = 0.271531 loss)
I0502 22:12:33.714623 28586 sgd_solver.cpp:106] Iteration 1150, lr = 1e-05
I0502 22:14:47.309094 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_1200.caffemodel
I0502 22:14:51.303730 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_1200.solverstate
I0502 22:14:51.363607 28586 solver.cpp:337] Iteration 1200, Testing net (#0)
I0502 22:15:20.466855 28586 solver.cpp:404]     Test net output #0: loss = 0.433839 (* 1 = 0.433839 loss)
I0502 22:15:21.372045 28586 solver.cpp:228] Iteration 1200, loss = 0.493098
I0502 22:15:21.372117 28586 solver.cpp:244]     Train net output #0: loss = 0.493094 (* 1 = 0.493094 loss)
I0502 22:15:21.372128 28586 sgd_solver.cpp:106] Iteration 1200, lr = 1e-05
I0502 22:17:37.907474 28586 solver.cpp:228] Iteration 1250, loss = 0.366826
I0502 22:17:37.907722 28586 solver.cpp:244]     Train net output #0: loss = 0.366822 (* 1 = 0.366822 loss)
I0502 22:17:37.907754 28586 sgd_solver.cpp:106] Iteration 1250, lr = 1e-05
I0502 22:19:54.130496 28586 solver.cpp:228] Iteration 1300, loss = 0.257089
I0502 22:19:54.143101 28586 solver.cpp:244]     Train net output #0: loss = 0.257086 (* 1 = 0.257086 loss)
I0502 22:19:54.143126 28586 sgd_solver.cpp:106] Iteration 1300, lr = 1e-05
I0502 22:22:10.177309 28586 solver.cpp:228] Iteration 1350, loss = 0.13418
I0502 22:22:10.178395 28586 solver.cpp:244]     Train net output #0: loss = 0.134176 (* 1 = 0.134176 loss)
I0502 22:22:10.178407 28586 sgd_solver.cpp:106] Iteration 1350, lr = 1e-05
I0502 22:24:23.890291 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_1400.caffemodel
I0502 22:24:28.721678 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_1400.solverstate
I0502 22:24:28.781343 28586 solver.cpp:337] Iteration 1400, Testing net (#0)
I0502 22:24:57.944690 28586 solver.cpp:404]     Test net output #0: loss = 0.411089 (* 1 = 0.411089 loss)
I0502 22:24:58.847496 28586 solver.cpp:228] Iteration 1400, loss = 0.139203
I0502 22:24:58.847553 28586 solver.cpp:244]     Train net output #0: loss = 0.139199 (* 1 = 0.139199 loss)
I0502 22:24:58.847565 28586 sgd_solver.cpp:106] Iteration 1400, lr = 1e-05
I0502 22:27:15.010617 28586 solver.cpp:228] Iteration 1450, loss = 0.231263
I0502 22:27:15.010854 28586 solver.cpp:244]     Train net output #0: loss = 0.231259 (* 1 = 0.231259 loss)
I0502 22:27:15.010887 28586 sgd_solver.cpp:106] Iteration 1450, lr = 1e-05
I0502 22:29:31.553218 28586 solver.cpp:228] Iteration 1500, loss = 0.267841
I0502 22:29:31.553396 28586 solver.cpp:244]     Train net output #0: loss = 0.267838 (* 1 = 0.267838 loss)
I0502 22:29:31.553412 28586 sgd_solver.cpp:106] Iteration 1500, lr = 1e-05
I0502 22:31:47.707161 28586 solver.cpp:228] Iteration 1550, loss = 0.18333
I0502 22:31:47.707346 28586 solver.cpp:244]     Train net output #0: loss = 0.183326 (* 1 = 0.183326 loss)
I0502 22:31:47.707360 28586 sgd_solver.cpp:106] Iteration 1550, lr = 1e-05
I0502 22:34:01.199636 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_1600.caffemodel
I0502 22:34:04.381177 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_1600.solverstate
I0502 22:34:04.438874 28586 solver.cpp:337] Iteration 1600, Testing net (#0)
I0502 22:34:33.698977 28586 solver.cpp:404]     Test net output #0: loss = 0.391583 (* 1 = 0.391583 loss)
I0502 22:34:34.599545 28586 solver.cpp:228] Iteration 1600, loss = 0.167363
I0502 22:34:34.599606 28586 solver.cpp:244]     Train net output #0: loss = 0.167359 (* 1 = 0.167359 loss)
I0502 22:34:34.599617 28586 sgd_solver.cpp:106] Iteration 1600, lr = 1e-05
I0502 22:36:50.802410 28586 solver.cpp:228] Iteration 1650, loss = 0.3306
I0502 22:36:50.802642 28586 solver.cpp:244]     Train net output #0: loss = 0.330596 (* 1 = 0.330596 loss)
I0502 22:36:50.802665 28586 sgd_solver.cpp:106] Iteration 1650, lr = 1e-05
I0502 22:39:06.799450 28586 solver.cpp:228] Iteration 1700, loss = 0.302992
I0502 22:39:06.799727 28586 solver.cpp:244]     Train net output #0: loss = 0.302988 (* 1 = 0.302988 loss)
I0502 22:39:06.799772 28586 sgd_solver.cpp:106] Iteration 1700, lr = 1e-05
I0502 22:41:23.220115 28586 solver.cpp:228] Iteration 1750, loss = 0.432991
I0502 22:41:23.220270 28586 solver.cpp:244]     Train net output #0: loss = 0.432987 (* 1 = 0.432987 loss)
I0502 22:41:23.220285 28586 sgd_solver.cpp:106] Iteration 1750, lr = 1e-05
I0502 22:43:36.753123 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_1800.caffemodel
I0502 22:43:41.359791 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_1800.solverstate
I0502 22:43:41.410672 28586 solver.cpp:337] Iteration 1800, Testing net (#0)
I0502 22:44:09.470544 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0502 22:44:10.774380 28586 solver.cpp:404]     Test net output #0: loss = 0.43238 (* 1 = 0.43238 loss)
I0502 22:44:11.676349 28586 solver.cpp:228] Iteration 1800, loss = 0.296684
I0502 22:44:11.676419 28586 solver.cpp:244]     Train net output #0: loss = 0.29668 (* 1 = 0.29668 loss)
I0502 22:44:11.676429 28586 sgd_solver.cpp:106] Iteration 1800, lr = 1e-05
I0502 22:46:27.985908 28586 solver.cpp:228] Iteration 1850, loss = 0.112002
I0502 22:46:27.998440 28586 solver.cpp:244]     Train net output #0: loss = 0.111998 (* 1 = 0.111998 loss)
I0502 22:46:27.998466 28586 sgd_solver.cpp:106] Iteration 1850, lr = 1e-05
I0502 22:48:44.164429 28586 solver.cpp:228] Iteration 1900, loss = 0.501483
I0502 22:48:44.164592 28586 solver.cpp:244]     Train net output #0: loss = 0.501479 (* 1 = 0.501479 loss)
I0502 22:48:44.164607 28586 sgd_solver.cpp:106] Iteration 1900, lr = 1e-05
I0502 22:51:00.008966 28586 solver.cpp:228] Iteration 1950, loss = 0.182527
I0502 22:51:00.009141 28586 solver.cpp:244]     Train net output #0: loss = 0.182523 (* 1 = 0.182523 loss)
I0502 22:51:00.009155 28586 sgd_solver.cpp:106] Iteration 1950, lr = 1e-05
I0502 22:53:13.475450 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_2000.caffemodel
I0502 22:53:16.315649 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_2000.solverstate
I0502 22:53:16.377308 28586 solver.cpp:337] Iteration 2000, Testing net (#0)
I0502 22:53:46.304664 28586 solver.cpp:404]     Test net output #0: loss = 0.316381 (* 1 = 0.316381 loss)
I0502 22:53:47.200556 28586 solver.cpp:228] Iteration 2000, loss = 0.355603
I0502 22:53:47.200637 28586 solver.cpp:244]     Train net output #0: loss = 0.355599 (* 1 = 0.355599 loss)
I0502 22:53:47.200652 28586 sgd_solver.cpp:106] Iteration 2000, lr = 1e-06
I0502 22:56:02.968725 28586 solver.cpp:228] Iteration 2050, loss = 0.238947
I0502 22:56:02.968869 28586 solver.cpp:244]     Train net output #0: loss = 0.238943 (* 1 = 0.238943 loss)
I0502 22:56:02.968881 28586 sgd_solver.cpp:106] Iteration 2050, lr = 1e-06
I0502 22:58:19.144471 28586 solver.cpp:228] Iteration 2100, loss = 0.255539
I0502 22:58:19.144665 28586 solver.cpp:244]     Train net output #0: loss = 0.255535 (* 1 = 0.255535 loss)
I0502 22:58:19.144683 28586 sgd_solver.cpp:106] Iteration 2100, lr = 1e-06
I0502 23:00:35.559543 28586 solver.cpp:228] Iteration 2150, loss = 0.189254
I0502 23:00:35.560199 28586 solver.cpp:244]     Train net output #0: loss = 0.18925 (* 1 = 0.18925 loss)
I0502 23:00:35.560214 28586 sgd_solver.cpp:106] Iteration 2150, lr = 1e-06
I0502 23:02:48.998656 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_2200.caffemodel
I0502 23:02:51.996912 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_2200.solverstate
I0502 23:02:52.048013 28586 solver.cpp:337] Iteration 2200, Testing net (#0)
I0502 23:03:22.209686 28586 solver.cpp:404]     Test net output #0: loss = 0.337364 (* 1 = 0.337364 loss)
I0502 23:03:23.105743 28586 solver.cpp:228] Iteration 2200, loss = 0.483098
I0502 23:03:23.105815 28586 solver.cpp:244]     Train net output #0: loss = 0.483094 (* 1 = 0.483094 loss)
I0502 23:03:23.105828 28586 sgd_solver.cpp:106] Iteration 2200, lr = 1e-06
I0502 23:05:39.285732 28586 solver.cpp:228] Iteration 2250, loss = 0.260251
I0502 23:05:39.285956 28586 solver.cpp:244]     Train net output #0: loss = 0.260247 (* 1 = 0.260247 loss)
I0502 23:05:39.285985 28586 sgd_solver.cpp:106] Iteration 2250, lr = 1e-06
I0502 23:07:55.328866 28586 solver.cpp:228] Iteration 2300, loss = 0.334791
I0502 23:07:55.329020 28586 solver.cpp:244]     Train net output #0: loss = 0.334787 (* 1 = 0.334787 loss)
I0502 23:07:55.329035 28586 sgd_solver.cpp:106] Iteration 2300, lr = 1e-06
I0502 23:10:11.657985 28586 solver.cpp:228] Iteration 2350, loss = 0.130251
I0502 23:10:11.659850 28586 solver.cpp:244]     Train net output #0: loss = 0.130247 (* 1 = 0.130247 loss)
I0502 23:10:11.659876 28586 sgd_solver.cpp:106] Iteration 2350, lr = 1e-06
I0502 23:12:25.193017 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_2400.caffemodel
I0502 23:12:31.266881 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_2400.solverstate
I0502 23:12:31.314327 28586 solver.cpp:337] Iteration 2400, Testing net (#0)
I0502 23:12:40.529135 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0502 23:13:01.277576 28586 solver.cpp:404]     Test net output #0: loss = 0.361156 (* 1 = 0.361156 loss)
I0502 23:13:02.194054 28586 solver.cpp:228] Iteration 2400, loss = 0.181171
I0502 23:13:02.194129 28586 solver.cpp:244]     Train net output #0: loss = 0.181167 (* 1 = 0.181167 loss)
I0502 23:13:02.194140 28586 sgd_solver.cpp:106] Iteration 2400, lr = 1e-06
I0502 23:15:18.568974 28586 solver.cpp:228] Iteration 2450, loss = 0.102371
I0502 23:15:18.569146 28586 solver.cpp:244]     Train net output #0: loss = 0.102367 (* 1 = 0.102367 loss)
I0502 23:15:18.569161 28586 sgd_solver.cpp:106] Iteration 2450, lr = 1e-06
I0502 23:17:34.853730 28586 solver.cpp:228] Iteration 2500, loss = 0.285931
I0502 23:17:34.856514 28586 solver.cpp:244]     Train net output #0: loss = 0.285927 (* 1 = 0.285927 loss)
I0502 23:17:34.856542 28586 sgd_solver.cpp:106] Iteration 2500, lr = 1e-06
I0502 23:19:51.069758 28586 solver.cpp:228] Iteration 2550, loss = 0.102157
I0502 23:19:51.069939 28586 solver.cpp:244]     Train net output #0: loss = 0.102153 (* 1 = 0.102153 loss)
I0502 23:19:51.069954 28586 sgd_solver.cpp:106] Iteration 2550, lr = 1e-06
I0502 23:22:04.829936 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_2600.caffemodel
I0502 23:22:07.896167 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_2600.solverstate
I0502 23:22:07.949241 28586 solver.cpp:337] Iteration 2600, Testing net (#0)
I0502 23:22:37.727231 28586 solver.cpp:404]     Test net output #0: loss = 0.315999 (* 1 = 0.315999 loss)
I0502 23:22:38.636633 28586 solver.cpp:228] Iteration 2600, loss = 0.1771
I0502 23:22:38.636711 28586 solver.cpp:244]     Train net output #0: loss = 0.177096 (* 1 = 0.177096 loss)
I0502 23:22:38.636725 28586 sgd_solver.cpp:106] Iteration 2600, lr = 1e-06
I0502 23:24:54.770596 28586 solver.cpp:228] Iteration 2650, loss = 0.257106
I0502 23:24:54.770803 28586 solver.cpp:244]     Train net output #0: loss = 0.257103 (* 1 = 0.257103 loss)
I0502 23:24:54.770825 28586 sgd_solver.cpp:106] Iteration 2650, lr = 1e-06
I0502 23:27:10.994206 28586 solver.cpp:228] Iteration 2700, loss = 0.0659934
I0502 23:27:10.994925 28586 solver.cpp:244]     Train net output #0: loss = 0.0659896 (* 1 = 0.0659896 loss)
I0502 23:27:10.994941 28586 sgd_solver.cpp:106] Iteration 2700, lr = 1e-06
I0502 23:29:26.949738 28586 solver.cpp:228] Iteration 2750, loss = 0.0903665
I0502 23:29:26.949905 28586 solver.cpp:244]     Train net output #0: loss = 0.0903627 (* 1 = 0.0903627 loss)
I0502 23:29:26.949921 28586 sgd_solver.cpp:106] Iteration 2750, lr = 1e-06
I0502 23:31:40.174228 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_2800.caffemodel
I0502 23:31:43.303189 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_2800.solverstate
I0502 23:31:43.352545 28586 solver.cpp:337] Iteration 2800, Testing net (#0)
I0502 23:32:04.284585 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0502 23:32:13.057888 28586 solver.cpp:404]     Test net output #0: loss = 0.322109 (* 1 = 0.322109 loss)
I0502 23:32:13.955014 28586 solver.cpp:228] Iteration 2800, loss = 0.152731
I0502 23:32:13.955072 28586 solver.cpp:244]     Train net output #0: loss = 0.152727 (* 1 = 0.152727 loss)
I0502 23:32:13.955083 28586 sgd_solver.cpp:106] Iteration 2800, lr = 1e-06
I0502 23:34:30.003043 28586 solver.cpp:228] Iteration 2850, loss = 0.259578
I0502 23:34:30.003397 28586 solver.cpp:244]     Train net output #0: loss = 0.259574 (* 1 = 0.259574 loss)
I0502 23:34:30.003434 28586 sgd_solver.cpp:106] Iteration 2850, lr = 1e-06
I0502 23:36:46.115947 28586 solver.cpp:228] Iteration 2900, loss = 0.285858
I0502 23:36:46.116914 28586 solver.cpp:244]     Train net output #0: loss = 0.285854 (* 1 = 0.285854 loss)
I0502 23:36:46.116931 28586 sgd_solver.cpp:106] Iteration 2900, lr = 1e-06
I0502 23:39:02.580348 28586 solver.cpp:228] Iteration 2950, loss = 0.15465
I0502 23:39:02.580541 28586 solver.cpp:244]     Train net output #0: loss = 0.154646 (* 1 = 0.154646 loss)
I0502 23:39:02.580557 28586 sgd_solver.cpp:106] Iteration 2950, lr = 1e-06
I0502 23:41:16.219575 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_3000.caffemodel
I0502 23:41:19.268633 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_3000.solverstate
I0502 23:41:19.324295 28586 solver.cpp:337] Iteration 3000, Testing net (#0)
I0502 23:41:49.278512 28586 solver.cpp:404]     Test net output #0: loss = 0.320538 (* 1 = 0.320538 loss)
I0502 23:41:50.176014 28586 solver.cpp:228] Iteration 3000, loss = 0.223473
I0502 23:41:50.176080 28586 solver.cpp:244]     Train net output #0: loss = 0.223469 (* 1 = 0.223469 loss)
I0502 23:41:50.176091 28586 sgd_solver.cpp:106] Iteration 3000, lr = 1e-06
I0502 23:44:06.462857 28586 solver.cpp:228] Iteration 3050, loss = 0.275295
I0502 23:44:06.463069 28586 solver.cpp:244]     Train net output #0: loss = 0.275292 (* 1 = 0.275292 loss)
I0502 23:44:06.463093 28586 sgd_solver.cpp:106] Iteration 3050, lr = 1e-06
I0502 23:46:22.851502 28586 solver.cpp:228] Iteration 3100, loss = 0.167276
I0502 23:46:22.851660 28586 solver.cpp:244]     Train net output #0: loss = 0.167272 (* 1 = 0.167272 loss)
I0502 23:46:22.851673 28586 sgd_solver.cpp:106] Iteration 3100, lr = 1e-06
I0502 23:48:39.078280 28586 solver.cpp:228] Iteration 3150, loss = 0.399296
I0502 23:48:39.078430 28586 solver.cpp:244]     Train net output #0: loss = 0.399293 (* 1 = 0.399293 loss)
I0502 23:48:39.078454 28586 sgd_solver.cpp:106] Iteration 3150, lr = 1e-06
I0502 23:50:52.728389 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_3200.caffemodel
I0502 23:51:02.816649 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_3200.solverstate
I0502 23:51:02.866963 28586 solver.cpp:337] Iteration 3200, Testing net (#0)
I0502 23:51:28.963173 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0502 23:51:33.221302 28586 solver.cpp:404]     Test net output #0: loss = 0.333145 (* 1 = 0.333145 loss)
I0502 23:51:34.119683 28586 solver.cpp:228] Iteration 3200, loss = 0.258345
I0502 23:51:34.119740 28586 solver.cpp:244]     Train net output #0: loss = 0.258341 (* 1 = 0.258341 loss)
I0502 23:51:34.119756 28586 sgd_solver.cpp:106] Iteration 3200, lr = 1e-06
I0502 23:53:50.228102 28586 solver.cpp:228] Iteration 3250, loss = 0.318182
I0502 23:53:50.228260 28586 solver.cpp:244]     Train net output #0: loss = 0.318178 (* 1 = 0.318178 loss)
I0502 23:53:50.228276 28586 sgd_solver.cpp:106] Iteration 3250, lr = 1e-06
I0502 23:56:06.520946 28586 solver.cpp:228] Iteration 3300, loss = 0.251985
I0502 23:56:06.521189 28586 solver.cpp:244]     Train net output #0: loss = 0.251982 (* 1 = 0.251982 loss)
I0502 23:56:06.521236 28586 sgd_solver.cpp:106] Iteration 3300, lr = 1e-06
I0502 23:58:22.518857 28586 solver.cpp:228] Iteration 3350, loss = 0.112176
I0502 23:58:22.520216 28586 solver.cpp:244]     Train net output #0: loss = 0.112172 (* 1 = 0.112172 loss)
I0502 23:58:22.520262 28586 sgd_solver.cpp:106] Iteration 3350, lr = 1e-06
I0503 00:00:36.150108 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_3400.caffemodel
I0503 00:00:49.593021 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_3400.solverstate
I0503 00:00:49.648046 28586 solver.cpp:337] Iteration 3400, Testing net (#0)
I0503 00:01:19.850009 28586 solver.cpp:404]     Test net output #0: loss = 0.352197 (* 1 = 0.352197 loss)
I0503 00:01:20.763994 28586 solver.cpp:228] Iteration 3400, loss = 0.175886
I0503 00:01:20.764053 28586 solver.cpp:244]     Train net output #0: loss = 0.175882 (* 1 = 0.175882 loss)
I0503 00:01:20.764063 28586 sgd_solver.cpp:106] Iteration 3400, lr = 1e-06
I0503 00:03:36.771770 28586 solver.cpp:228] Iteration 3450, loss = 0.0708646
I0503 00:03:36.773282 28586 solver.cpp:244]     Train net output #0: loss = 0.0708606 (* 1 = 0.0708606 loss)
I0503 00:03:36.773298 28586 sgd_solver.cpp:106] Iteration 3450, lr = 1e-06
I0503 00:05:53.265498 28586 solver.cpp:228] Iteration 3500, loss = 0.433002
I0503 00:05:53.265709 28586 solver.cpp:244]     Train net output #0: loss = 0.432998 (* 1 = 0.432998 loss)
I0503 00:05:53.265725 28586 sgd_solver.cpp:106] Iteration 3500, lr = 1e-06
I0503 00:08:09.222681 28586 solver.cpp:228] Iteration 3550, loss = 0.0912898
I0503 00:08:09.222889 28586 solver.cpp:244]     Train net output #0: loss = 0.0912858 (* 1 = 0.0912858 loss)
I0503 00:08:09.222903 28586 sgd_solver.cpp:106] Iteration 3550, lr = 1e-06
I0503 00:10:22.662564 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_3600.caffemodel
I0503 00:10:35.162461 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_3600.solverstate
I0503 00:10:35.216850 28586 solver.cpp:337] Iteration 3600, Testing net (#0)
I0503 00:11:04.635529 28586 solver.cpp:404]     Test net output #0: loss = 0.320148 (* 1 = 0.320148 loss)
I0503 00:11:05.536110 28586 solver.cpp:228] Iteration 3600, loss = 0.355362
I0503 00:11:05.536186 28586 solver.cpp:244]     Train net output #0: loss = 0.355358 (* 1 = 0.355358 loss)
I0503 00:11:05.536196 28586 sgd_solver.cpp:106] Iteration 3600, lr = 1e-06
I0503 00:13:21.339582 28586 solver.cpp:228] Iteration 3650, loss = 0.141488
I0503 00:13:21.339746 28586 solver.cpp:244]     Train net output #0: loss = 0.141484 (* 1 = 0.141484 loss)
I0503 00:13:21.339763 28586 sgd_solver.cpp:106] Iteration 3650, lr = 1e-06
I0503 00:15:37.335136 28586 solver.cpp:228] Iteration 3700, loss = 0.178213
I0503 00:15:37.335350 28586 solver.cpp:244]     Train net output #0: loss = 0.178209 (* 1 = 0.178209 loss)
I0503 00:15:37.335376 28586 sgd_solver.cpp:106] Iteration 3700, lr = 1e-06
I0503 00:17:53.594868 28586 solver.cpp:228] Iteration 3750, loss = 0.190493
I0503 00:17:53.595064 28586 solver.cpp:244]     Train net output #0: loss = 0.190489 (* 1 = 0.190489 loss)
I0503 00:17:53.595082 28586 sgd_solver.cpp:106] Iteration 3750, lr = 1e-06
I0503 00:20:07.072217 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_3800.caffemodel
I0503 00:20:16.554581 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_3800.solverstate
I0503 00:20:16.603498 28586 solver.cpp:337] Iteration 3800, Testing net (#0)
I0503 00:20:20.194505 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 00:20:46.241938 28586 solver.cpp:404]     Test net output #0: loss = 0.308675 (* 1 = 0.308675 loss)
I0503 00:20:47.146231 28586 solver.cpp:228] Iteration 3800, loss = 0.174201
I0503 00:20:47.146302 28586 solver.cpp:244]     Train net output #0: loss = 0.174197 (* 1 = 0.174197 loss)
I0503 00:20:47.146318 28586 sgd_solver.cpp:106] Iteration 3800, lr = 1e-06
I0503 00:23:03.359473 28586 solver.cpp:228] Iteration 3850, loss = 0.277178
I0503 00:23:03.359676 28586 solver.cpp:244]     Train net output #0: loss = 0.277174 (* 1 = 0.277174 loss)
I0503 00:23:03.359690 28586 sgd_solver.cpp:106] Iteration 3850, lr = 1e-06
I0503 00:25:19.373724 28586 solver.cpp:228] Iteration 3900, loss = 0.18519
I0503 00:25:19.373989 28586 solver.cpp:244]     Train net output #0: loss = 0.185186 (* 1 = 0.185186 loss)
I0503 00:25:19.374016 28586 sgd_solver.cpp:106] Iteration 3900, lr = 1e-06
I0503 00:27:35.657305 28586 solver.cpp:228] Iteration 3950, loss = 0.254349
I0503 00:27:35.657449 28586 solver.cpp:244]     Train net output #0: loss = 0.254345 (* 1 = 0.254345 loss)
I0503 00:27:35.657466 28586 sgd_solver.cpp:106] Iteration 3950, lr = 1e-06
I0503 00:29:49.221810 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_4000.caffemodel
I0503 00:29:57.163640 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_4000.solverstate
I0503 00:29:57.217972 28586 solver.cpp:337] Iteration 4000, Testing net (#0)
I0503 00:30:26.810973 28586 solver.cpp:404]     Test net output #0: loss = 0.324993 (* 1 = 0.324993 loss)
I0503 00:30:27.729176 28586 solver.cpp:228] Iteration 4000, loss = 0.188215
I0503 00:30:27.729244 28586 solver.cpp:244]     Train net output #0: loss = 0.188211 (* 1 = 0.188211 loss)
I0503 00:30:27.729254 28586 sgd_solver.cpp:106] Iteration 4000, lr = 1e-07
I0503 00:32:43.970732 28586 solver.cpp:228] Iteration 4050, loss = 0.0859398
I0503 00:32:43.970887 28586 solver.cpp:244]     Train net output #0: loss = 0.085936 (* 1 = 0.085936 loss)
I0503 00:32:43.970924 28586 sgd_solver.cpp:106] Iteration 4050, lr = 1e-07
I0503 00:35:00.254174 28586 solver.cpp:228] Iteration 4100, loss = 0.047833
I0503 00:35:00.254331 28586 solver.cpp:244]     Train net output #0: loss = 0.0478291 (* 1 = 0.0478291 loss)
I0503 00:35:00.254350 28586 sgd_solver.cpp:106] Iteration 4100, lr = 1e-07
I0503 00:37:16.613962 28586 solver.cpp:228] Iteration 4150, loss = 0.28259
I0503 00:37:16.614797 28586 solver.cpp:244]     Train net output #0: loss = 0.282586 (* 1 = 0.282586 loss)
I0503 00:37:16.614812 28586 sgd_solver.cpp:106] Iteration 4150, lr = 1e-07
I0503 00:39:30.195226 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_4200.caffemodel
I0503 00:39:33.595845 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_4200.solverstate
I0503 00:39:33.645692 28586 solver.cpp:337] Iteration 4200, Testing net (#0)
I0503 00:39:42.419188 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 00:40:03.095690 28586 solver.cpp:404]     Test net output #0: loss = 0.336882 (* 1 = 0.336882 loss)
I0503 00:40:03.999771 28586 solver.cpp:228] Iteration 4200, loss = 0.316777
I0503 00:40:03.999833 28586 solver.cpp:244]     Train net output #0: loss = 0.316773 (* 1 = 0.316773 loss)
I0503 00:40:03.999845 28586 sgd_solver.cpp:106] Iteration 4200, lr = 1e-07
I0503 00:42:20.244482 28586 solver.cpp:228] Iteration 4250, loss = 0.0845951
I0503 00:42:20.244663 28586 solver.cpp:244]     Train net output #0: loss = 0.0845913 (* 1 = 0.0845913 loss)
I0503 00:42:20.244678 28586 sgd_solver.cpp:106] Iteration 4250, lr = 1e-07
I0503 00:44:36.591920 28586 solver.cpp:228] Iteration 4300, loss = 0.0736417
I0503 00:44:36.592066 28586 solver.cpp:244]     Train net output #0: loss = 0.0736379 (* 1 = 0.0736379 loss)
I0503 00:44:36.592082 28586 sgd_solver.cpp:106] Iteration 4300, lr = 1e-07
I0503 00:46:52.818047 28586 solver.cpp:228] Iteration 4350, loss = 0.18234
I0503 00:46:52.818284 28586 solver.cpp:244]     Train net output #0: loss = 0.182336 (* 1 = 0.182336 loss)
I0503 00:46:52.818311 28586 sgd_solver.cpp:106] Iteration 4350, lr = 1e-07
I0503 00:49:06.517065 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_4400.caffemodel
I0503 00:49:18.101764 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_4400.solverstate
I0503 00:49:18.163964 28586 solver.cpp:337] Iteration 4400, Testing net (#0)
I0503 00:49:47.718590 28586 solver.cpp:404]     Test net output #0: loss = 0.338032 (* 1 = 0.338032 loss)
I0503 00:49:48.620329 28586 solver.cpp:228] Iteration 4400, loss = 0.315655
I0503 00:49:48.620409 28586 solver.cpp:244]     Train net output #0: loss = 0.315651 (* 1 = 0.315651 loss)
I0503 00:49:48.620424 28586 sgd_solver.cpp:106] Iteration 4400, lr = 1e-07
I0503 00:52:04.366750 28586 solver.cpp:228] Iteration 4450, loss = 0.0344938
I0503 00:52:04.366961 28586 solver.cpp:244]     Train net output #0: loss = 0.0344899 (* 1 = 0.0344899 loss)
I0503 00:52:04.366974 28586 sgd_solver.cpp:106] Iteration 4450, lr = 1e-07
I0503 00:54:20.591174 28586 solver.cpp:228] Iteration 4500, loss = 0.333055
I0503 00:54:20.592344 28586 solver.cpp:244]     Train net output #0: loss = 0.333051 (* 1 = 0.333051 loss)
I0503 00:54:20.592358 28586 sgd_solver.cpp:106] Iteration 4500, lr = 1e-07
I0503 00:56:37.119663 28586 solver.cpp:228] Iteration 4550, loss = 0.205299
I0503 00:56:37.132115 28586 solver.cpp:244]     Train net output #0: loss = 0.205295 (* 1 = 0.205295 loss)
I0503 00:56:37.132139 28586 sgd_solver.cpp:106] Iteration 4550, lr = 1e-07
I0503 00:58:50.798089 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_4600.caffemodel
I0503 00:59:04.233320 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_4600.solverstate
I0503 00:59:04.290838 28586 solver.cpp:337] Iteration 4600, Testing net (#0)
I0503 00:59:22.185282 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 00:59:34.098250 28586 solver.cpp:404]     Test net output #0: loss = 0.335339 (* 1 = 0.335339 loss)
I0503 00:59:34.996117 28586 solver.cpp:228] Iteration 4600, loss = 0.376622
I0503 00:59:34.996186 28586 solver.cpp:244]     Train net output #0: loss = 0.376618 (* 1 = 0.376618 loss)
I0503 00:59:34.996196 28586 sgd_solver.cpp:106] Iteration 4600, lr = 1e-07
I0503 01:01:50.754796 28586 solver.cpp:228] Iteration 4650, loss = 0.262025
I0503 01:01:50.754968 28586 solver.cpp:244]     Train net output #0: loss = 0.262021 (* 1 = 0.262021 loss)
I0503 01:01:50.754981 28586 sgd_solver.cpp:106] Iteration 4650, lr = 1e-07
I0503 01:04:07.149672 28586 solver.cpp:228] Iteration 4700, loss = 0.286905
I0503 01:04:07.149866 28586 solver.cpp:244]     Train net output #0: loss = 0.286901 (* 1 = 0.286901 loss)
I0503 01:04:07.149878 28586 sgd_solver.cpp:106] Iteration 4700, lr = 1e-07
I0503 01:06:23.340225 28586 solver.cpp:228] Iteration 4750, loss = 0.168181
I0503 01:06:23.344435 28586 solver.cpp:244]     Train net output #0: loss = 0.168177 (* 1 = 0.168177 loss)
I0503 01:06:23.344451 28586 sgd_solver.cpp:106] Iteration 4750, lr = 1e-07
I0503 01:08:36.811041 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_4800.caffemodel
I0503 01:08:39.999723 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_4800.solverstate
I0503 01:08:40.052356 28586 solver.cpp:337] Iteration 4800, Testing net (#0)
I0503 01:09:09.577468 28586 solver.cpp:404]     Test net output #0: loss = 0.339127 (* 1 = 0.339127 loss)
I0503 01:09:10.480556 28586 solver.cpp:228] Iteration 4800, loss = 0.269376
I0503 01:09:10.480615 28586 solver.cpp:244]     Train net output #0: loss = 0.269372 (* 1 = 0.269372 loss)
I0503 01:09:10.480639 28586 sgd_solver.cpp:106] Iteration 4800, lr = 1e-07
I0503 01:11:26.792515 28586 solver.cpp:228] Iteration 4850, loss = 0.0932926
I0503 01:11:26.792702 28586 solver.cpp:244]     Train net output #0: loss = 0.0932888 (* 1 = 0.0932888 loss)
I0503 01:11:26.792719 28586 sgd_solver.cpp:106] Iteration 4850, lr = 1e-07
I0503 01:13:43.122100 28586 solver.cpp:228] Iteration 4900, loss = 0.101636
I0503 01:13:43.122256 28586 solver.cpp:244]     Train net output #0: loss = 0.101632 (* 1 = 0.101632 loss)
I0503 01:13:43.122272 28586 sgd_solver.cpp:106] Iteration 4900, lr = 1e-07
I0503 01:15:59.308809 28586 solver.cpp:228] Iteration 4950, loss = 0.340473
I0503 01:15:59.309151 28586 solver.cpp:244]     Train net output #0: loss = 0.34047 (* 1 = 0.34047 loss)
I0503 01:15:59.309170 28586 sgd_solver.cpp:106] Iteration 4950, lr = 1e-07
I0503 01:18:12.878849 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_5000.caffemodel
I0503 01:18:15.897560 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_5000.solverstate
I0503 01:18:15.951575 28586 solver.cpp:337] Iteration 5000, Testing net (#0)
I0503 01:18:41.356348 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 01:18:45.371698 28586 solver.cpp:404]     Test net output #0: loss = 0.323328 (* 1 = 0.323328 loss)
I0503 01:18:46.281134 28586 solver.cpp:228] Iteration 5000, loss = 0.119221
I0503 01:18:46.281225 28586 solver.cpp:244]     Train net output #0: loss = 0.119217 (* 1 = 0.119217 loss)
I0503 01:18:46.281242 28586 sgd_solver.cpp:106] Iteration 5000, lr = 1e-07
I0503 01:21:02.566553 28586 solver.cpp:228] Iteration 5050, loss = 0.490632
I0503 01:21:02.579104 28586 solver.cpp:244]     Train net output #0: loss = 0.490628 (* 1 = 0.490628 loss)
I0503 01:21:02.579133 28586 sgd_solver.cpp:106] Iteration 5050, lr = 1e-07
I0503 01:23:18.845497 28586 solver.cpp:228] Iteration 5100, loss = 0.186676
I0503 01:23:18.847590 28586 solver.cpp:244]     Train net output #0: loss = 0.186672 (* 1 = 0.186672 loss)
I0503 01:23:18.847616 28586 sgd_solver.cpp:106] Iteration 5100, lr = 1e-07
I0503 01:25:34.997551 28586 solver.cpp:228] Iteration 5150, loss = 0.173836
I0503 01:25:34.997704 28586 solver.cpp:244]     Train net output #0: loss = 0.173832 (* 1 = 0.173832 loss)
I0503 01:25:34.997721 28586 sgd_solver.cpp:106] Iteration 5150, lr = 1e-07
I0503 01:27:48.478896 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_5200.caffemodel
I0503 01:27:51.573853 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_5200.solverstate
I0503 01:27:51.650879 28586 solver.cpp:337] Iteration 5200, Testing net (#0)
I0503 01:28:21.125820 28586 solver.cpp:404]     Test net output #0: loss = 0.333239 (* 1 = 0.333239 loss)
I0503 01:28:22.028745 28586 solver.cpp:228] Iteration 5200, loss = 0.215715
I0503 01:28:22.028825 28586 solver.cpp:244]     Train net output #0: loss = 0.215711 (* 1 = 0.215711 loss)
I0503 01:28:22.028838 28586 sgd_solver.cpp:106] Iteration 5200, lr = 1e-07
I0503 01:30:38.143476 28586 solver.cpp:228] Iteration 5250, loss = 0.13589
I0503 01:30:38.146088 28586 solver.cpp:244]     Train net output #0: loss = 0.135886 (* 1 = 0.135886 loss)
I0503 01:30:38.146111 28586 sgd_solver.cpp:106] Iteration 5250, lr = 1e-07
I0503 01:32:54.370513 28586 solver.cpp:228] Iteration 5300, loss = 0.769055
I0503 01:32:54.374922 28586 solver.cpp:244]     Train net output #0: loss = 0.769052 (* 1 = 0.769052 loss)
I0503 01:32:54.374943 28586 sgd_solver.cpp:106] Iteration 5300, lr = 1e-07
I0503 01:35:10.758148 28586 solver.cpp:228] Iteration 5350, loss = 0.230767
I0503 01:35:10.758322 28586 solver.cpp:244]     Train net output #0: loss = 0.230764 (* 1 = 0.230764 loss)
I0503 01:35:10.758339 28586 sgd_solver.cpp:106] Iteration 5350, lr = 1e-07
I0503 01:37:24.430452 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_5400.caffemodel
I0503 01:37:27.597489 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_5400.solverstate
I0503 01:37:27.647485 28586 solver.cpp:337] Iteration 5400, Testing net (#0)
I0503 01:37:56.617579 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 01:37:57.152086 28586 solver.cpp:404]     Test net output #0: loss = 0.339596 (* 1 = 0.339596 loss)
I0503 01:37:58.049649 28586 solver.cpp:228] Iteration 5400, loss = 0.148113
I0503 01:37:58.049768 28586 solver.cpp:244]     Train net output #0: loss = 0.148109 (* 1 = 0.148109 loss)
I0503 01:37:58.049800 28586 sgd_solver.cpp:106] Iteration 5400, lr = 1e-07
I0503 01:40:14.182431 28586 solver.cpp:228] Iteration 5450, loss = 0.276384
I0503 01:40:14.182611 28586 solver.cpp:244]     Train net output #0: loss = 0.276381 (* 1 = 0.276381 loss)
I0503 01:40:14.182624 28586 sgd_solver.cpp:106] Iteration 5450, lr = 1e-07
I0503 01:42:30.395601 28586 solver.cpp:228] Iteration 5500, loss = 0.397418
I0503 01:42:30.396317 28586 solver.cpp:244]     Train net output #0: loss = 0.397414 (* 1 = 0.397414 loss)
I0503 01:42:30.396332 28586 sgd_solver.cpp:106] Iteration 5500, lr = 1e-07
I0503 01:44:46.548460 28586 solver.cpp:228] Iteration 5550, loss = 0.228075
I0503 01:44:46.548662 28586 solver.cpp:244]     Train net output #0: loss = 0.228071 (* 1 = 0.228071 loss)
I0503 01:44:46.548691 28586 sgd_solver.cpp:106] Iteration 5550, lr = 1e-07
I0503 01:46:59.901105 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_5600.caffemodel
I0503 01:47:02.927772 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_5600.solverstate
I0503 01:47:02.987975 28586 solver.cpp:337] Iteration 5600, Testing net (#0)
I0503 01:47:32.379295 28586 solver.cpp:404]     Test net output #0: loss = 0.323117 (* 1 = 0.323117 loss)
I0503 01:47:33.280838 28586 solver.cpp:228] Iteration 5600, loss = 0.207557
I0503 01:47:33.280903 28586 solver.cpp:244]     Train net output #0: loss = 0.207553 (* 1 = 0.207553 loss)
I0503 01:47:33.280915 28586 sgd_solver.cpp:106] Iteration 5600, lr = 1e-07
I0503 01:49:49.347270 28586 solver.cpp:228] Iteration 5650, loss = 0.0521805
I0503 01:49:49.347450 28586 solver.cpp:244]     Train net output #0: loss = 0.0521768 (* 1 = 0.0521768 loss)
I0503 01:49:49.347463 28586 sgd_solver.cpp:106] Iteration 5650, lr = 1e-07
I0503 01:52:05.404906 28586 solver.cpp:228] Iteration 5700, loss = 0.116523
I0503 01:52:05.405601 28586 solver.cpp:244]     Train net output #0: loss = 0.116519 (* 1 = 0.116519 loss)
I0503 01:52:05.405628 28586 sgd_solver.cpp:106] Iteration 5700, lr = 1e-07
I0503 01:54:21.501591 28586 solver.cpp:228] Iteration 5750, loss = 0.114359
I0503 01:54:21.503428 28586 solver.cpp:244]     Train net output #0: loss = 0.114355 (* 1 = 0.114355 loss)
I0503 01:54:21.503454 28586 sgd_solver.cpp:106] Iteration 5750, lr = 1e-07
I0503 01:56:34.701508 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_5800.caffemodel
I0503 01:56:37.662628 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_5800.solverstate
I0503 01:56:37.713837 28586 solver.cpp:337] Iteration 5800, Testing net (#0)
I0503 01:57:07.113976 28586 solver.cpp:404]     Test net output #0: loss = 0.31634 (* 1 = 0.31634 loss)
I0503 01:57:08.010484 28586 solver.cpp:228] Iteration 5800, loss = 0.138305
I0503 01:57:08.010560 28586 solver.cpp:244]     Train net output #0: loss = 0.138302 (* 1 = 0.138302 loss)
I0503 01:57:08.010572 28586 sgd_solver.cpp:106] Iteration 5800, lr = 1e-07
I0503 01:59:24.398187 28586 solver.cpp:228] Iteration 5850, loss = 0.112829
I0503 01:59:24.398432 28586 solver.cpp:244]     Train net output #0: loss = 0.112825 (* 1 = 0.112825 loss)
I0503 01:59:24.398460 28586 sgd_solver.cpp:106] Iteration 5850, lr = 1e-07
I0503 02:01:40.783097 28586 solver.cpp:228] Iteration 5900, loss = 0.179366
I0503 02:01:40.786136 28586 solver.cpp:244]     Train net output #0: loss = 0.179363 (* 1 = 0.179363 loss)
I0503 02:01:40.786154 28586 sgd_solver.cpp:106] Iteration 5900, lr = 1e-07
I0503 02:03:56.697461 28586 solver.cpp:228] Iteration 5950, loss = 0.186424
I0503 02:03:56.697649 28586 solver.cpp:244]     Train net output #0: loss = 0.18642 (* 1 = 0.18642 loss)
I0503 02:03:56.697664 28586 sgd_solver.cpp:106] Iteration 5950, lr = 1e-07
I0503 02:06:10.252725 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_6000.caffemodel
I0503 02:06:13.243630 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_6000.solverstate
I0503 02:06:13.294075 28586 solver.cpp:337] Iteration 6000, Testing net (#0)
I0503 02:06:18.228904 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 02:06:42.843992 28586 solver.cpp:404]     Test net output #0: loss = 0.335471 (* 1 = 0.335471 loss)
I0503 02:06:43.744407 28586 solver.cpp:228] Iteration 6000, loss = 0.261007
I0503 02:06:43.744483 28586 solver.cpp:244]     Train net output #0: loss = 0.261003 (* 1 = 0.261003 loss)
I0503 02:06:43.744498 28586 sgd_solver.cpp:106] Iteration 6000, lr = 1e-08
I0503 02:08:59.793292 28586 solver.cpp:228] Iteration 6050, loss = 0.15216
I0503 02:08:59.794214 28586 solver.cpp:244]     Train net output #0: loss = 0.152156 (* 1 = 0.152156 loss)
I0503 02:08:59.794247 28586 sgd_solver.cpp:106] Iteration 6050, lr = 1e-08
I0503 02:11:15.916406 28586 solver.cpp:228] Iteration 6100, loss = 0.0864652
I0503 02:11:15.916609 28586 solver.cpp:244]     Train net output #0: loss = 0.0864615 (* 1 = 0.0864615 loss)
I0503 02:11:15.916625 28586 sgd_solver.cpp:106] Iteration 6100, lr = 1e-08
I0503 02:13:32.065639 28586 solver.cpp:228] Iteration 6150, loss = 0.157382
I0503 02:13:32.065863 28586 solver.cpp:244]     Train net output #0: loss = 0.157378 (* 1 = 0.157378 loss)
I0503 02:13:32.065894 28586 sgd_solver.cpp:106] Iteration 6150, lr = 1e-08
I0503 02:15:45.063182 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_6200.caffemodel
I0503 02:15:48.160992 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_6200.solverstate
I0503 02:15:48.217432 28586 solver.cpp:337] Iteration 6200, Testing net (#0)
I0503 02:16:17.816808 28586 solver.cpp:404]     Test net output #0: loss = 0.331099 (* 1 = 0.331099 loss)
I0503 02:16:18.716181 28586 solver.cpp:228] Iteration 6200, loss = 0.392063
I0503 02:16:18.716250 28586 solver.cpp:244]     Train net output #0: loss = 0.39206 (* 1 = 0.39206 loss)
I0503 02:16:18.716262 28586 sgd_solver.cpp:106] Iteration 6200, lr = 1e-08
I0503 02:18:34.717066 28586 solver.cpp:228] Iteration 6250, loss = 0.3179
I0503 02:18:34.718236 28586 solver.cpp:244]     Train net output #0: loss = 0.317896 (* 1 = 0.317896 loss)
I0503 02:18:34.718258 28586 sgd_solver.cpp:106] Iteration 6250, lr = 1e-08
I0503 02:20:50.761868 28586 solver.cpp:228] Iteration 6300, loss = 0.0212183
I0503 02:20:50.762045 28586 solver.cpp:244]     Train net output #0: loss = 0.0212145 (* 1 = 0.0212145 loss)
I0503 02:20:50.762059 28586 sgd_solver.cpp:106] Iteration 6300, lr = 1e-08
I0503 02:23:06.856744 28586 solver.cpp:228] Iteration 6350, loss = 0.0831629
I0503 02:23:06.859501 28586 solver.cpp:244]     Train net output #0: loss = 0.0831592 (* 1 = 0.0831592 loss)
I0503 02:23:06.859524 28586 sgd_solver.cpp:106] Iteration 6350, lr = 1e-08
I0503 02:25:20.407348 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_6400.caffemodel
I0503 02:25:23.508545 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_6400.solverstate
I0503 02:25:23.561329 28586 solver.cpp:337] Iteration 6400, Testing net (#0)
I0503 02:25:36.593967 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 02:25:52.998838 28586 solver.cpp:404]     Test net output #0: loss = 0.331867 (* 1 = 0.331867 loss)
I0503 02:25:53.901278 28586 solver.cpp:228] Iteration 6400, loss = 0.133775
I0503 02:25:53.901331 28586 solver.cpp:244]     Train net output #0: loss = 0.133772 (* 1 = 0.133772 loss)
I0503 02:25:53.901355 28586 sgd_solver.cpp:106] Iteration 6400, lr = 1e-08
I0503 02:28:09.905279 28586 solver.cpp:228] Iteration 6450, loss = 0.316076
I0503 02:28:09.905443 28586 solver.cpp:244]     Train net output #0: loss = 0.316072 (* 1 = 0.316072 loss)
I0503 02:28:09.905459 28586 sgd_solver.cpp:106] Iteration 6450, lr = 1e-08
I0503 02:30:26.097825 28586 solver.cpp:228] Iteration 6500, loss = 0.354106
I0503 02:30:26.099086 28586 solver.cpp:244]     Train net output #0: loss = 0.354102 (* 1 = 0.354102 loss)
I0503 02:30:26.099112 28586 sgd_solver.cpp:106] Iteration 6500, lr = 1e-08
I0503 02:32:42.341981 28586 solver.cpp:228] Iteration 6550, loss = 0.27751
I0503 02:32:42.343139 28586 solver.cpp:244]     Train net output #0: loss = 0.277506 (* 1 = 0.277506 loss)
I0503 02:32:42.343159 28586 sgd_solver.cpp:106] Iteration 6550, lr = 1e-08
I0503 02:34:55.700644 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_6600.caffemodel
I0503 02:34:58.744854 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_6600.solverstate
I0503 02:34:58.796413 28586 solver.cpp:337] Iteration 6600, Testing net (#0)
I0503 02:35:28.485050 28586 solver.cpp:404]     Test net output #0: loss = 0.337557 (* 1 = 0.337557 loss)
I0503 02:35:29.387852 28586 solver.cpp:228] Iteration 6600, loss = 0.326484
I0503 02:35:29.387913 28586 solver.cpp:244]     Train net output #0: loss = 0.32648 (* 1 = 0.32648 loss)
I0503 02:35:29.387938 28586 sgd_solver.cpp:106] Iteration 6600, lr = 1e-08
I0503 02:37:45.706446 28586 solver.cpp:228] Iteration 6650, loss = 0.22448
I0503 02:37:45.706651 28586 solver.cpp:244]     Train net output #0: loss = 0.224477 (* 1 = 0.224477 loss)
I0503 02:37:45.706670 28586 sgd_solver.cpp:106] Iteration 6650, lr = 1e-08
I0503 02:40:01.776371 28586 solver.cpp:228] Iteration 6700, loss = 0.176049
I0503 02:40:01.788933 28586 solver.cpp:244]     Train net output #0: loss = 0.176045 (* 1 = 0.176045 loss)
I0503 02:40:01.788961 28586 sgd_solver.cpp:106] Iteration 6700, lr = 1e-08
I0503 02:42:18.063007 28586 solver.cpp:228] Iteration 6750, loss = 0.268188
I0503 02:42:18.063232 28586 solver.cpp:244]     Train net output #0: loss = 0.268184 (* 1 = 0.268184 loss)
I0503 02:42:18.063263 28586 sgd_solver.cpp:106] Iteration 6750, lr = 1e-08
I0503 02:44:31.707578 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_6800.caffemodel
I0503 02:44:34.937121 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_6800.solverstate
I0503 02:44:34.990865 28586 solver.cpp:337] Iteration 6800, Testing net (#0)
I0503 02:44:52.358265 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 02:45:04.585021 28586 solver.cpp:404]     Test net output #0: loss = 0.339751 (* 1 = 0.339751 loss)
I0503 02:45:05.498706 28586 solver.cpp:228] Iteration 6800, loss = 0.0819454
I0503 02:45:05.498771 28586 solver.cpp:244]     Train net output #0: loss = 0.0819418 (* 1 = 0.0819418 loss)
I0503 02:45:05.498783 28586 sgd_solver.cpp:106] Iteration 6800, lr = 1e-08
I0503 02:47:21.588896 28586 solver.cpp:228] Iteration 6850, loss = 0.221262
I0503 02:47:21.591718 28586 solver.cpp:244]     Train net output #0: loss = 0.221258 (* 1 = 0.221258 loss)
I0503 02:47:21.591756 28586 sgd_solver.cpp:106] Iteration 6850, lr = 1e-08
I0503 02:49:37.976570 28586 solver.cpp:228] Iteration 6900, loss = 0.302874
I0503 02:49:37.976766 28586 solver.cpp:244]     Train net output #0: loss = 0.30287 (* 1 = 0.30287 loss)
I0503 02:49:37.976788 28586 sgd_solver.cpp:106] Iteration 6900, lr = 1e-08
I0503 02:51:54.206962 28586 solver.cpp:228] Iteration 6950, loss = 0.133575
I0503 02:51:54.207866 28586 solver.cpp:244]     Train net output #0: loss = 0.133571 (* 1 = 0.133571 loss)
I0503 02:51:54.207907 28586 sgd_solver.cpp:106] Iteration 6950, lr = 1e-08
I0503 02:54:07.812000 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_7000.caffemodel
I0503 02:54:11.308830 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_7000.solverstate
I0503 02:54:11.374390 28586 solver.cpp:337] Iteration 7000, Testing net (#0)
I0503 02:54:40.959736 28586 solver.cpp:404]     Test net output #0: loss = 0.326754 (* 1 = 0.326754 loss)
I0503 02:54:41.863168 28586 solver.cpp:228] Iteration 7000, loss = 0.190551
I0503 02:54:41.863224 28586 solver.cpp:244]     Train net output #0: loss = 0.190547 (* 1 = 0.190547 loss)
I0503 02:54:41.863240 28586 sgd_solver.cpp:106] Iteration 7000, lr = 1e-08
I0503 02:56:57.989248 28586 solver.cpp:228] Iteration 7050, loss = 0.322628
I0503 02:56:57.989416 28586 solver.cpp:244]     Train net output #0: loss = 0.322625 (* 1 = 0.322625 loss)
I0503 02:56:57.989429 28586 sgd_solver.cpp:106] Iteration 7050, lr = 1e-08
I0503 02:59:14.056411 28586 solver.cpp:228] Iteration 7100, loss = 0.293364
I0503 02:59:14.059062 28586 solver.cpp:244]     Train net output #0: loss = 0.29336 (* 1 = 0.29336 loss)
I0503 02:59:14.059101 28586 sgd_solver.cpp:106] Iteration 7100, lr = 1e-08
I0503 03:01:30.162693 28586 solver.cpp:228] Iteration 7150, loss = 0.25278
I0503 03:01:30.162859 28586 solver.cpp:244]     Train net output #0: loss = 0.252776 (* 1 = 0.252776 loss)
I0503 03:01:30.162875 28586 sgd_solver.cpp:106] Iteration 7150, lr = 1e-08
I0503 03:03:43.277148 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_7200.caffemodel
I0503 03:03:47.982287 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_7200.solverstate
I0503 03:03:48.074074 28586 solver.cpp:337] Iteration 7200, Testing net (#0)
I0503 03:04:12.079737 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 03:04:17.533462 28586 solver.cpp:404]     Test net output #0: loss = 0.328555 (* 1 = 0.328555 loss)
I0503 03:04:18.453727 28586 solver.cpp:228] Iteration 7200, loss = 0.193313
I0503 03:04:18.453795 28586 solver.cpp:244]     Train net output #0: loss = 0.19331 (* 1 = 0.19331 loss)
I0503 03:04:18.453812 28586 sgd_solver.cpp:106] Iteration 7200, lr = 1e-08
I0503 03:06:34.708428 28586 solver.cpp:228] Iteration 7250, loss = 0.409037
I0503 03:06:34.708611 28586 solver.cpp:244]     Train net output #0: loss = 0.409034 (* 1 = 0.409034 loss)
I0503 03:06:34.708632 28586 sgd_solver.cpp:106] Iteration 7250, lr = 1e-08
I0503 03:08:50.856257 28586 solver.cpp:228] Iteration 7300, loss = 0.324008
I0503 03:08:50.859010 28586 solver.cpp:244]     Train net output #0: loss = 0.324005 (* 1 = 0.324005 loss)
I0503 03:08:50.859037 28586 sgd_solver.cpp:106] Iteration 7300, lr = 1e-08
I0503 03:11:06.980051 28586 solver.cpp:228] Iteration 7350, loss = 0.168035
I0503 03:11:06.980216 28586 solver.cpp:244]     Train net output #0: loss = 0.168032 (* 1 = 0.168032 loss)
I0503 03:11:06.980231 28586 sgd_solver.cpp:106] Iteration 7350, lr = 1e-08
I0503 03:13:20.628192 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_7400.caffemodel
I0503 03:13:24.146898 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_7400.solverstate
I0503 03:13:24.196271 28586 solver.cpp:337] Iteration 7400, Testing net (#0)
I0503 03:13:53.687644 28586 solver.cpp:404]     Test net output #0: loss = 0.327056 (* 1 = 0.327056 loss)
I0503 03:13:54.589764 28586 solver.cpp:228] Iteration 7400, loss = 0.180202
I0503 03:13:54.589826 28586 solver.cpp:244]     Train net output #0: loss = 0.180199 (* 1 = 0.180199 loss)
I0503 03:13:54.589843 28586 sgd_solver.cpp:106] Iteration 7400, lr = 1e-08
I0503 03:16:10.969861 28586 solver.cpp:228] Iteration 7450, loss = 0.259451
I0503 03:16:10.970111 28586 solver.cpp:244]     Train net output #0: loss = 0.259447 (* 1 = 0.259447 loss)
I0503 03:16:10.970146 28586 sgd_solver.cpp:106] Iteration 7450, lr = 1e-08
I0503 03:18:27.299051 28586 solver.cpp:228] Iteration 7500, loss = 0.229683
I0503 03:18:27.299283 28586 solver.cpp:244]     Train net output #0: loss = 0.229679 (* 1 = 0.229679 loss)
I0503 03:18:27.299320 28586 sgd_solver.cpp:106] Iteration 7500, lr = 1e-08
I0503 03:20:43.387044 28586 solver.cpp:228] Iteration 7550, loss = 0.46475
I0503 03:20:43.388993 28586 solver.cpp:244]     Train net output #0: loss = 0.464746 (* 1 = 0.464746 loss)
I0503 03:20:43.389020 28586 sgd_solver.cpp:106] Iteration 7550, lr = 1e-08
I0503 03:22:56.880812 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_7600.caffemodel
I0503 03:23:00.831377 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_7600.solverstate
I0503 03:23:00.887199 28586 solver.cpp:337] Iteration 7600, Testing net (#0)
I0503 03:23:30.161363 28586 solver.cpp:404]     Test net output #0: loss = 0.323605 (* 1 = 0.323605 loss)
I0503 03:23:31.066828 28586 solver.cpp:228] Iteration 7600, loss = 0.613165
I0503 03:23:31.066941 28586 solver.cpp:244]     Train net output #0: loss = 0.613161 (* 1 = 0.613161 loss)
I0503 03:23:31.066970 28586 sgd_solver.cpp:106] Iteration 7600, lr = 1e-08
I0503 03:25:47.304714 28586 solver.cpp:228] Iteration 7650, loss = 0.248996
I0503 03:25:47.305066 28586 solver.cpp:244]     Train net output #0: loss = 0.248992 (* 1 = 0.248992 loss)
I0503 03:25:47.305088 28586 sgd_solver.cpp:106] Iteration 7650, lr = 1e-08
I0503 03:28:03.673993 28586 solver.cpp:228] Iteration 7700, loss = 0.111336
I0503 03:28:03.674266 28586 solver.cpp:244]     Train net output #0: loss = 0.111332 (* 1 = 0.111332 loss)
I0503 03:28:03.674305 28586 sgd_solver.cpp:106] Iteration 7700, lr = 1e-08
I0503 03:30:20.071588 28586 solver.cpp:228] Iteration 7750, loss = 0.209464
I0503 03:30:20.071801 28586 solver.cpp:244]     Train net output #0: loss = 0.20946 (* 1 = 0.20946 loss)
I0503 03:30:20.071841 28586 sgd_solver.cpp:106] Iteration 7750, lr = 1e-08
I0503 03:32:33.611253 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_7800.caffemodel
I0503 03:32:37.066195 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_7800.solverstate
I0503 03:32:37.113867 28586 solver.cpp:337] Iteration 7800, Testing net (#0)
I0503 03:32:45.085438 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 03:33:06.459708 28586 solver.cpp:404]     Test net output #0: loss = 0.319181 (* 1 = 0.319181 loss)
I0503 03:33:07.361148 28586 solver.cpp:228] Iteration 7800, loss = 0.106552
I0503 03:33:07.361220 28586 solver.cpp:244]     Train net output #0: loss = 0.106549 (* 1 = 0.106549 loss)
I0503 03:33:07.361246 28586 sgd_solver.cpp:106] Iteration 7800, lr = 1e-08
I0503 03:35:23.531894 28586 solver.cpp:228] Iteration 7850, loss = 0.134847
I0503 03:35:23.532093 28586 solver.cpp:244]     Train net output #0: loss = 0.134843 (* 1 = 0.134843 loss)
I0503 03:35:23.532104 28586 sgd_solver.cpp:106] Iteration 7850, lr = 1e-08
I0503 03:37:39.601266 28586 solver.cpp:228] Iteration 7900, loss = 0.264767
I0503 03:37:39.601506 28586 solver.cpp:244]     Train net output #0: loss = 0.264763 (* 1 = 0.264763 loss)
I0503 03:37:39.601541 28586 sgd_solver.cpp:106] Iteration 7900, lr = 1e-08
I0503 03:39:55.623270 28586 solver.cpp:228] Iteration 7950, loss = 0.0487818
I0503 03:39:55.623425 28586 solver.cpp:244]     Train net output #0: loss = 0.0487783 (* 1 = 0.0487783 loss)
I0503 03:39:55.623438 28586 sgd_solver.cpp:106] Iteration 7950, lr = 1e-08
I0503 03:42:08.935178 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_8000.caffemodel
I0503 03:42:12.230031 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_8000.solverstate
I0503 03:42:12.303606 28586 solver.cpp:337] Iteration 8000, Testing net (#0)
I0503 03:42:41.347203 28586 solver.cpp:404]     Test net output #0: loss = 0.340526 (* 1 = 0.340526 loss)
I0503 03:42:42.250136 28586 solver.cpp:228] Iteration 8000, loss = 0.361127
I0503 03:42:42.250196 28586 solver.cpp:244]     Train net output #0: loss = 0.361123 (* 1 = 0.361123 loss)
I0503 03:42:42.250219 28586 sgd_solver.cpp:106] Iteration 8000, lr = 1e-09
I0503 03:44:58.372743 28586 solver.cpp:228] Iteration 8050, loss = 0.1203
I0503 03:44:58.372927 28586 solver.cpp:244]     Train net output #0: loss = 0.120296 (* 1 = 0.120296 loss)
I0503 03:44:58.372946 28586 sgd_solver.cpp:106] Iteration 8050, lr = 1e-09
I0503 03:47:14.789919 28586 solver.cpp:228] Iteration 8100, loss = 0.281493
I0503 03:47:14.790060 28586 solver.cpp:244]     Train net output #0: loss = 0.281489 (* 1 = 0.281489 loss)
I0503 03:47:14.790076 28586 sgd_solver.cpp:106] Iteration 8100, lr = 1e-09
I0503 03:49:31.010536 28586 solver.cpp:228] Iteration 8150, loss = 0.185828
I0503 03:49:31.010725 28586 solver.cpp:244]     Train net output #0: loss = 0.185825 (* 1 = 0.185825 loss)
I0503 03:49:31.010740 28586 sgd_solver.cpp:106] Iteration 8150, lr = 1e-09
I0503 03:51:44.587748 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_8200.caffemodel
I0503 03:51:47.794744 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_8200.solverstate
I0503 03:51:47.844099 28586 solver.cpp:337] Iteration 8200, Testing net (#0)
I0503 03:52:17.354481 28586 solver.cpp:404]     Test net output #0: loss = 0.336033 (* 1 = 0.336033 loss)
I0503 03:52:18.262035 28586 solver.cpp:228] Iteration 8200, loss = 0.182801
I0503 03:52:18.262095 28586 solver.cpp:244]     Train net output #0: loss = 0.182797 (* 1 = 0.182797 loss)
I0503 03:52:18.262106 28586 sgd_solver.cpp:106] Iteration 8200, lr = 1e-09
I0503 03:54:34.472440 28586 solver.cpp:228] Iteration 8250, loss = 0.204566
I0503 03:54:34.472611 28586 solver.cpp:244]     Train net output #0: loss = 0.204563 (* 1 = 0.204563 loss)
I0503 03:54:34.472625 28586 sgd_solver.cpp:106] Iteration 8250, lr = 1e-09
I0503 03:56:50.458706 28586 solver.cpp:228] Iteration 8300, loss = 0.0788796
I0503 03:56:50.458885 28586 solver.cpp:244]     Train net output #0: loss = 0.0788759 (* 1 = 0.0788759 loss)
I0503 03:56:50.458900 28586 sgd_solver.cpp:106] Iteration 8300, lr = 1e-09
I0503 03:59:07.002637 28586 solver.cpp:228] Iteration 8350, loss = 0.00882244
I0503 03:59:07.016091 28586 solver.cpp:244]     Train net output #0: loss = 0.00881869 (* 1 = 0.00881869 loss)
I0503 03:59:07.016114 28586 sgd_solver.cpp:106] Iteration 8350, lr = 1e-09
I0503 04:01:20.541373 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_8400.caffemodel
I0503 04:01:23.591356 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_8400.solverstate
I0503 04:01:23.681911 28586 solver.cpp:337] Iteration 8400, Testing net (#0)
I0503 04:01:35.418813 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 04:01:53.646821 28586 solver.cpp:404]     Test net output #0: loss = 0.335974 (* 1 = 0.335974 loss)
I0503 04:01:54.549468 28586 solver.cpp:228] Iteration 8400, loss = 0.122366
I0503 04:01:54.549536 28586 solver.cpp:244]     Train net output #0: loss = 0.122362 (* 1 = 0.122362 loss)
I0503 04:01:54.549548 28586 sgd_solver.cpp:106] Iteration 8400, lr = 1e-09
I0503 04:04:10.783143 28586 solver.cpp:228] Iteration 8450, loss = 0.223764
I0503 04:04:10.783390 28586 solver.cpp:244]     Train net output #0: loss = 0.22376 (* 1 = 0.22376 loss)
I0503 04:04:10.783421 28586 sgd_solver.cpp:106] Iteration 8450, lr = 1e-09
I0503 04:06:26.918881 28586 solver.cpp:228] Iteration 8500, loss = 0.124951
I0503 04:06:26.921126 28586 solver.cpp:244]     Train net output #0: loss = 0.124947 (* 1 = 0.124947 loss)
I0503 04:06:26.921140 28586 sgd_solver.cpp:106] Iteration 8500, lr = 1e-09
I0503 04:08:42.973928 28586 solver.cpp:228] Iteration 8550, loss = 0.288865
I0503 04:08:42.974181 28586 solver.cpp:244]     Train net output #0: loss = 0.288862 (* 1 = 0.288862 loss)
I0503 04:08:42.974197 28586 sgd_solver.cpp:106] Iteration 8550, lr = 1e-09
I0503 04:10:56.462021 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_8600.caffemodel
I0503 04:10:59.882840 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_8600.solverstate
I0503 04:10:59.934702 28586 solver.cpp:337] Iteration 8600, Testing net (#0)
I0503 04:11:29.629925 28586 solver.cpp:404]     Test net output #0: loss = 0.340691 (* 1 = 0.340691 loss)
I0503 04:11:30.529069 28586 solver.cpp:228] Iteration 8600, loss = 0.367704
I0503 04:11:30.529135 28586 solver.cpp:244]     Train net output #0: loss = 0.3677 (* 1 = 0.3677 loss)
I0503 04:11:30.529148 28586 sgd_solver.cpp:106] Iteration 8600, lr = 1e-09
I0503 04:13:46.661029 28586 solver.cpp:228] Iteration 8650, loss = 0.194833
I0503 04:13:46.666592 28586 solver.cpp:244]     Train net output #0: loss = 0.194829 (* 1 = 0.194829 loss)
I0503 04:13:46.666620 28586 sgd_solver.cpp:106] Iteration 8650, lr = 1e-09
I0503 04:16:03.044858 28586 solver.cpp:228] Iteration 8700, loss = 0.207966
I0503 04:16:03.045101 28586 solver.cpp:244]     Train net output #0: loss = 0.207962 (* 1 = 0.207962 loss)
I0503 04:16:03.045135 28586 sgd_solver.cpp:106] Iteration 8700, lr = 1e-09
I0503 04:18:19.176450 28586 solver.cpp:228] Iteration 8750, loss = 0.287707
I0503 04:18:19.176635 28586 solver.cpp:244]     Train net output #0: loss = 0.287703 (* 1 = 0.287703 loss)
I0503 04:18:19.176650 28586 sgd_solver.cpp:106] Iteration 8750, lr = 1e-09
I0503 04:20:32.631939 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_8800.caffemodel
I0503 04:20:39.341825 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_8800.solverstate
I0503 04:20:39.399971 28586 solver.cpp:337] Iteration 8800, Testing net (#0)
I0503 04:21:03.360388 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 04:21:09.160859 28586 solver.cpp:404]     Test net output #0: loss = 0.341599 (* 1 = 0.341599 loss)
I0503 04:21:10.065951 28586 solver.cpp:228] Iteration 8800, loss = 0.224422
I0503 04:21:10.066010 28586 solver.cpp:244]     Train net output #0: loss = 0.224418 (* 1 = 0.224418 loss)
I0503 04:21:10.066032 28586 sgd_solver.cpp:106] Iteration 8800, lr = 1e-09
I0503 04:23:26.322307 28586 solver.cpp:228] Iteration 8850, loss = 0.192982
I0503 04:23:26.334846 28586 solver.cpp:244]     Train net output #0: loss = 0.192978 (* 1 = 0.192978 loss)
I0503 04:23:26.334873 28586 sgd_solver.cpp:106] Iteration 8850, lr = 1e-09
I0503 04:25:42.483453 28586 solver.cpp:228] Iteration 8900, loss = 0.305123
I0503 04:25:42.483635 28586 solver.cpp:244]     Train net output #0: loss = 0.305119 (* 1 = 0.305119 loss)
I0503 04:25:42.483649 28586 sgd_solver.cpp:106] Iteration 8900, lr = 1e-09
I0503 04:27:58.782588 28586 solver.cpp:228] Iteration 8950, loss = 0.0826028
I0503 04:27:58.782843 28586 solver.cpp:244]     Train net output #0: loss = 0.0825989 (* 1 = 0.0825989 loss)
I0503 04:27:58.782860 28586 sgd_solver.cpp:106] Iteration 8950, lr = 1e-09
I0503 04:30:12.318040 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_9000.caffemodel
I0503 04:30:15.407356 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_9000.solverstate
I0503 04:30:15.467743 28586 solver.cpp:337] Iteration 9000, Testing net (#0)
I0503 04:30:44.817381 28586 solver.cpp:404]     Test net output #0: loss = 0.328999 (* 1 = 0.328999 loss)
I0503 04:30:45.716768 28586 solver.cpp:228] Iteration 9000, loss = 0.139224
I0503 04:30:45.716841 28586 solver.cpp:244]     Train net output #0: loss = 0.13922 (* 1 = 0.13922 loss)
I0503 04:30:45.716855 28586 sgd_solver.cpp:106] Iteration 9000, lr = 1e-09
I0503 04:33:02.123610 28586 solver.cpp:228] Iteration 9050, loss = 0.404667
I0503 04:33:02.126327 28586 solver.cpp:244]     Train net output #0: loss = 0.404663 (* 1 = 0.404663 loss)
I0503 04:33:02.126363 28586 sgd_solver.cpp:106] Iteration 9050, lr = 1e-09
I0503 04:35:18.296458 28586 solver.cpp:228] Iteration 9100, loss = 0.129714
I0503 04:35:18.296618 28586 solver.cpp:244]     Train net output #0: loss = 0.12971 (* 1 = 0.12971 loss)
I0503 04:35:18.296633 28586 sgd_solver.cpp:106] Iteration 9100, lr = 1e-09
I0503 04:37:34.638301 28586 solver.cpp:228] Iteration 9150, loss = 0.387507
I0503 04:37:34.639587 28586 solver.cpp:244]     Train net output #0: loss = 0.387503 (* 1 = 0.387503 loss)
I0503 04:37:34.639613 28586 sgd_solver.cpp:106] Iteration 9150, lr = 1e-09
I0503 04:39:48.349640 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_9200.caffemodel
I0503 04:39:51.579766 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_9200.solverstate
I0503 04:39:51.627663 28586 solver.cpp:337] Iteration 9200, Testing net (#0)
I0503 04:40:20.905760 28586 solver.cpp:404]     Test net output #0: loss = 0.330996 (* 1 = 0.330996 loss)
I0503 04:40:21.809067 28586 solver.cpp:228] Iteration 9200, loss = 0.246849
I0503 04:40:21.809132 28586 solver.cpp:244]     Train net output #0: loss = 0.246845 (* 1 = 0.246845 loss)
I0503 04:40:21.809144 28586 sgd_solver.cpp:106] Iteration 9200, lr = 1e-09
I0503 04:42:38.058388 28586 solver.cpp:228] Iteration 9250, loss = 0.241166
I0503 04:42:38.058562 28586 solver.cpp:244]     Train net output #0: loss = 0.241162 (* 1 = 0.241162 loss)
I0503 04:42:38.058576 28586 sgd_solver.cpp:106] Iteration 9250, lr = 1e-09
I0503 04:44:54.246135 28586 solver.cpp:228] Iteration 9300, loss = 0.205706
I0503 04:44:54.246351 28586 solver.cpp:244]     Train net output #0: loss = 0.205702 (* 1 = 0.205702 loss)
I0503 04:44:54.246374 28586 sgd_solver.cpp:106] Iteration 9300, lr = 1e-09
I0503 04:47:10.264801 28586 solver.cpp:228] Iteration 9350, loss = 0.28218
I0503 04:47:10.264993 28586 solver.cpp:244]     Train net output #0: loss = 0.282176 (* 1 = 0.282176 loss)
I0503 04:47:10.265012 28586 sgd_solver.cpp:106] Iteration 9350, lr = 1e-09
I0503 04:49:23.840131 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_9400.caffemodel
I0503 04:49:27.291862 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_9400.solverstate
I0503 04:49:27.338758 28586 solver.cpp:337] Iteration 9400, Testing net (#0)
I0503 04:49:57.287405 28586 solver.cpp:404]     Test net output #0: loss = 0.329419 (* 1 = 0.329419 loss)
I0503 04:49:58.201932 28586 solver.cpp:228] Iteration 9400, loss = 0.0722824
I0503 04:49:58.202036 28586 solver.cpp:244]     Train net output #0: loss = 0.0722784 (* 1 = 0.0722784 loss)
I0503 04:49:58.202049 28586 sgd_solver.cpp:106] Iteration 9400, lr = 1e-09
I0503 04:52:14.363628 28586 solver.cpp:228] Iteration 9450, loss = 0.114137
I0503 04:52:14.364178 28586 solver.cpp:244]     Train net output #0: loss = 0.114133 (* 1 = 0.114133 loss)
I0503 04:52:14.364193 28586 sgd_solver.cpp:106] Iteration 9450, lr = 1e-09
I0503 04:54:30.546242 28586 solver.cpp:228] Iteration 9500, loss = 0.136396
I0503 04:54:30.546492 28586 solver.cpp:244]     Train net output #0: loss = 0.136392 (* 1 = 0.136392 loss)
I0503 04:54:30.546509 28586 sgd_solver.cpp:106] Iteration 9500, lr = 1e-09
I0503 04:56:46.694357 28586 solver.cpp:228] Iteration 9550, loss = 0.215945
I0503 04:56:46.694515 28586 solver.cpp:244]     Train net output #0: loss = 0.215941 (* 1 = 0.215941 loss)
I0503 04:56:46.694540 28586 sgd_solver.cpp:106] Iteration 9550, lr = 1e-09
I0503 04:58:59.949400 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_9600.caffemodel
I0503 04:59:03.358276 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_9600.solverstate
I0503 04:59:03.408740 28586 solver.cpp:337] Iteration 9600, Testing net (#0)
I0503 04:59:31.789073 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 04:59:32.748895 28586 solver.cpp:404]     Test net output #0: loss = 0.324444 (* 1 = 0.324444 loss)
I0503 04:59:33.653606 28586 solver.cpp:228] Iteration 9600, loss = 0.219152
I0503 04:59:33.653689 28586 solver.cpp:244]     Train net output #0: loss = 0.219148 (* 1 = 0.219148 loss)
I0503 04:59:33.653712 28586 sgd_solver.cpp:106] Iteration 9600, lr = 1e-09
I0503 05:01:49.866330 28586 solver.cpp:228] Iteration 9650, loss = 0.363462
I0503 05:01:49.866484 28586 solver.cpp:244]     Train net output #0: loss = 0.363458 (* 1 = 0.363458 loss)
I0503 05:01:49.866498 28586 sgd_solver.cpp:106] Iteration 9650, lr = 1e-09
I0503 05:04:06.288806 28586 solver.cpp:228] Iteration 9700, loss = 0.189054
I0503 05:04:06.288988 28586 solver.cpp:244]     Train net output #0: loss = 0.189051 (* 1 = 0.189051 loss)
I0503 05:04:06.289008 28586 sgd_solver.cpp:106] Iteration 9700, lr = 1e-09
I0503 05:06:22.459005 28586 solver.cpp:228] Iteration 9750, loss = 0.123004
I0503 05:06:22.459127 28586 solver.cpp:244]     Train net output #0: loss = 0.123 (* 1 = 0.123 loss)
I0503 05:06:22.459151 28586 sgd_solver.cpp:106] Iteration 9750, lr = 1e-09
I0503 05:08:35.971487 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_9800.caffemodel
I0503 05:08:46.094472 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_9800.solverstate
I0503 05:08:46.145272 28586 solver.cpp:337] Iteration 9800, Testing net (#0)
I0503 05:09:16.395908 28586 solver.cpp:404]     Test net output #0: loss = 0.318619 (* 1 = 0.318619 loss)
I0503 05:09:17.296008 28586 solver.cpp:228] Iteration 9800, loss = 0.216831
I0503 05:09:17.296069 28586 solver.cpp:244]     Train net output #0: loss = 0.216827 (* 1 = 0.216827 loss)
I0503 05:09:17.296080 28586 sgd_solver.cpp:106] Iteration 9800, lr = 1e-09
I0503 05:11:33.250404 28586 solver.cpp:228] Iteration 9850, loss = 0.0154187
I0503 05:11:33.259037 28586 solver.cpp:244]     Train net output #0: loss = 0.0154149 (* 1 = 0.0154149 loss)
I0503 05:11:33.259079 28586 sgd_solver.cpp:106] Iteration 9850, lr = 1e-09
I0503 05:13:49.741654 28586 solver.cpp:228] Iteration 9900, loss = 0.132591
I0503 05:13:49.741888 28586 solver.cpp:244]     Train net output #0: loss = 0.132588 (* 1 = 0.132588 loss)
I0503 05:13:49.741920 28586 sgd_solver.cpp:106] Iteration 9900, lr = 1e-09
I0503 05:16:06.152132 28586 solver.cpp:228] Iteration 9950, loss = 0.0699341
I0503 05:16:06.164726 28586 solver.cpp:244]     Train net output #0: loss = 0.0699302 (* 1 = 0.0699302 loss)
I0503 05:16:06.164773 28586 sgd_solver.cpp:106] Iteration 9950, lr = 1e-09
I0503 05:18:19.629894 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_10000.caffemodel
I0503 05:18:24.963502 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_10000.solverstate
I0503 05:18:25.013906 28586 solver.cpp:337] Iteration 10000, Testing net (#0)
I0503 05:18:54.996250 28586 solver.cpp:404]     Test net output #0: loss = 0.340425 (* 1 = 0.340425 loss)
I0503 05:18:55.900956 28586 solver.cpp:228] Iteration 10000, loss = 0.201036
I0503 05:18:55.901023 28586 solver.cpp:244]     Train net output #0: loss = 0.201032 (* 1 = 0.201032 loss)
I0503 05:18:55.901034 28586 sgd_solver.cpp:106] Iteration 10000, lr = 1e-10
I0503 05:21:12.391261 28586 solver.cpp:228] Iteration 10050, loss = 0.164054
I0503 05:21:12.391402 28586 solver.cpp:244]     Train net output #0: loss = 0.16405 (* 1 = 0.16405 loss)
I0503 05:21:12.391415 28586 sgd_solver.cpp:106] Iteration 10050, lr = 1e-10
I0503 05:23:28.586977 28586 solver.cpp:228] Iteration 10100, loss = 0.19035
I0503 05:23:28.587149 28586 solver.cpp:244]     Train net output #0: loss = 0.190346 (* 1 = 0.190346 loss)
I0503 05:23:28.587164 28586 sgd_solver.cpp:106] Iteration 10100, lr = 1e-10
I0503 05:25:44.924415 28586 solver.cpp:228] Iteration 10150, loss = 0.272449
I0503 05:25:44.924572 28586 solver.cpp:244]     Train net output #0: loss = 0.272445 (* 1 = 0.272445 loss)
I0503 05:25:44.924587 28586 sgd_solver.cpp:106] Iteration 10150, lr = 1e-10
I0503 05:27:58.368126 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_10200.caffemodel
I0503 05:28:11.640017 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_10200.solverstate
I0503 05:28:11.706941 28586 solver.cpp:337] Iteration 10200, Testing net (#0)
I0503 05:28:41.030892 28586 solver.cpp:404]     Test net output #0: loss = 0.33594 (* 1 = 0.33594 loss)
I0503 05:28:41.923046 28586 solver.cpp:228] Iteration 10200, loss = 0.0660272
I0503 05:28:41.923115 28586 solver.cpp:244]     Train net output #0: loss = 0.0660233 (* 1 = 0.0660233 loss)
I0503 05:28:41.923125 28586 sgd_solver.cpp:106] Iteration 10200, lr = 1e-10
I0503 05:30:57.887406 28586 solver.cpp:228] Iteration 10250, loss = 0.114815
I0503 05:30:57.887563 28586 solver.cpp:244]     Train net output #0: loss = 0.114811 (* 1 = 0.114811 loss)
I0503 05:30:57.887578 28586 sgd_solver.cpp:106] Iteration 10250, lr = 1e-10
I0503 05:33:14.021193 28586 solver.cpp:228] Iteration 10300, loss = 0.286733
I0503 05:33:14.021363 28586 solver.cpp:244]     Train net output #0: loss = 0.286729 (* 1 = 0.286729 loss)
I0503 05:33:14.021381 28586 sgd_solver.cpp:106] Iteration 10300, lr = 1e-10
I0503 05:35:30.129793 28586 solver.cpp:228] Iteration 10350, loss = 0.131355
I0503 05:35:30.129971 28586 solver.cpp:244]     Train net output #0: loss = 0.131351 (* 1 = 0.131351 loss)
I0503 05:35:30.129987 28586 sgd_solver.cpp:106] Iteration 10350, lr = 1e-10
I0503 05:37:43.587720 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_10400.caffemodel
I0503 05:37:46.629166 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_10400.solverstate
I0503 05:37:46.679785 28586 solver.cpp:337] Iteration 10400, Testing net (#0)
I0503 05:38:00.140028 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 05:38:16.140383 28586 solver.cpp:404]     Test net output #0: loss = 0.335844 (* 1 = 0.335844 loss)
I0503 05:38:17.040654 28586 solver.cpp:228] Iteration 10400, loss = 0.134088
I0503 05:38:17.040768 28586 solver.cpp:244]     Train net output #0: loss = 0.134084 (* 1 = 0.134084 loss)
I0503 05:38:17.040797 28586 sgd_solver.cpp:106] Iteration 10400, lr = 1e-10
I0503 05:40:33.225193 28586 solver.cpp:228] Iteration 10450, loss = 0.295444
I0503 05:40:33.225347 28586 solver.cpp:244]     Train net output #0: loss = 0.29544 (* 1 = 0.29544 loss)
I0503 05:40:33.225360 28586 sgd_solver.cpp:106] Iteration 10450, lr = 1e-10
I0503 05:42:49.325597 28586 solver.cpp:228] Iteration 10500, loss = 0.108841
I0503 05:42:49.338177 28586 solver.cpp:244]     Train net output #0: loss = 0.108837 (* 1 = 0.108837 loss)
I0503 05:42:49.338222 28586 sgd_solver.cpp:106] Iteration 10500, lr = 1e-10
I0503 05:45:05.562368 28586 solver.cpp:228] Iteration 10550, loss = 0.201044
I0503 05:45:05.562625 28586 solver.cpp:244]     Train net output #0: loss = 0.201041 (* 1 = 0.201041 loss)
I0503 05:45:05.562661 28586 sgd_solver.cpp:106] Iteration 10550, lr = 1e-10
I0503 05:47:18.882042 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_10600.caffemodel
I0503 05:47:27.118566 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_10600.solverstate
I0503 05:47:27.171882 28586 solver.cpp:337] Iteration 10600, Testing net (#0)
I0503 05:47:56.796353 28586 solver.cpp:404]     Test net output #0: loss = 0.340602 (* 1 = 0.340602 loss)
I0503 05:47:57.700604 28586 solver.cpp:228] Iteration 10600, loss = 0.310935
I0503 05:47:57.700708 28586 solver.cpp:244]     Train net output #0: loss = 0.310931 (* 1 = 0.310931 loss)
I0503 05:47:57.700722 28586 sgd_solver.cpp:106] Iteration 10600, lr = 1e-10
I0503 05:50:13.747923 28586 solver.cpp:228] Iteration 10650, loss = 0.0284736
I0503 05:50:13.748087 28586 solver.cpp:244]     Train net output #0: loss = 0.0284698 (* 1 = 0.0284698 loss)
I0503 05:50:13.748101 28586 sgd_solver.cpp:106] Iteration 10650, lr = 1e-10
I0503 05:52:29.706984 28586 solver.cpp:228] Iteration 10700, loss = 0.312391
I0503 05:52:29.707195 28586 solver.cpp:244]     Train net output #0: loss = 0.312387 (* 1 = 0.312387 loss)
I0503 05:52:29.707222 28586 sgd_solver.cpp:106] Iteration 10700, lr = 1e-10
I0503 05:54:45.805037 28586 solver.cpp:228] Iteration 10750, loss = 0.132604
I0503 05:54:45.805236 28586 solver.cpp:244]     Train net output #0: loss = 0.132601 (* 1 = 0.132601 loss)
I0503 05:54:45.805259 28586 sgd_solver.cpp:106] Iteration 10750, lr = 1e-10
I0503 05:56:59.453752 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_10800.caffemodel
I0503 05:57:02.472455 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_10800.solverstate
I0503 05:57:02.526844 28586 solver.cpp:337] Iteration 10800, Testing net (#0)
I0503 05:57:31.769629 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 05:57:31.946508 28586 solver.cpp:404]     Test net output #0: loss = 0.341496 (* 1 = 0.341496 loss)
I0503 05:57:32.844081 28586 solver.cpp:228] Iteration 10800, loss = 0.123965
I0503 05:57:32.844295 28586 solver.cpp:244]     Train net output #0: loss = 0.123961 (* 1 = 0.123961 loss)
I0503 05:57:32.844326 28586 sgd_solver.cpp:106] Iteration 10800, lr = 1e-10
I0503 05:59:49.128321 28586 solver.cpp:228] Iteration 10850, loss = 0.224676
I0503 05:59:49.129102 28586 solver.cpp:244]     Train net output #0: loss = 0.224672 (* 1 = 0.224672 loss)
I0503 05:59:49.129128 28586 sgd_solver.cpp:106] Iteration 10850, lr = 1e-10
I0503 06:02:05.600569 28586 solver.cpp:228] Iteration 10900, loss = 0.198511
I0503 06:02:05.600747 28586 solver.cpp:244]     Train net output #0: loss = 0.198507 (* 1 = 0.198507 loss)
I0503 06:02:05.600761 28586 sgd_solver.cpp:106] Iteration 10900, lr = 1e-10
I0503 06:04:21.696766 28586 solver.cpp:228] Iteration 10950, loss = 0.646094
I0503 06:04:21.696976 28586 solver.cpp:244]     Train net output #0: loss = 0.64609 (* 1 = 0.64609 loss)
I0503 06:04:21.696992 28586 sgd_solver.cpp:106] Iteration 10950, lr = 1e-10
I0503 06:06:35.139201 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_11000.caffemodel
I0503 06:06:38.159077 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_11000.solverstate
I0503 06:06:38.207748 28586 solver.cpp:337] Iteration 11000, Testing net (#0)
I0503 06:07:07.915285 28586 solver.cpp:404]     Test net output #0: loss = 0.328979 (* 1 = 0.328979 loss)
I0503 06:07:08.826023 28586 solver.cpp:228] Iteration 11000, loss = 0.137381
I0503 06:07:08.826086 28586 solver.cpp:244]     Train net output #0: loss = 0.137377 (* 1 = 0.137377 loss)
I0503 06:07:08.826098 28586 sgd_solver.cpp:106] Iteration 11000, lr = 1e-10
I0503 06:09:25.037824 28586 solver.cpp:228] Iteration 11050, loss = 0.279893
I0503 06:09:25.038590 28586 solver.cpp:244]     Train net output #0: loss = 0.279889 (* 1 = 0.279889 loss)
I0503 06:09:25.038640 28586 sgd_solver.cpp:106] Iteration 11050, lr = 1e-10
I0503 06:11:41.191519 28586 solver.cpp:228] Iteration 11100, loss = 0.371169
I0503 06:11:41.191707 28586 solver.cpp:244]     Train net output #0: loss = 0.371165 (* 1 = 0.371165 loss)
I0503 06:11:41.191721 28586 sgd_solver.cpp:106] Iteration 11100, lr = 1e-10
I0503 06:13:57.695839 28586 solver.cpp:228] Iteration 11150, loss = 0.150269
I0503 06:13:57.696439 28586 solver.cpp:244]     Train net output #0: loss = 0.150266 (* 1 = 0.150266 loss)
I0503 06:13:57.696455 28586 sgd_solver.cpp:106] Iteration 11150, lr = 1e-10
I0503 06:16:11.207763 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_11200.caffemodel
I0503 06:16:14.270603 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_11200.solverstate
I0503 06:16:14.325780 28586 solver.cpp:337] Iteration 11200, Testing net (#0)
I0503 06:16:44.257129 28586 solver.cpp:404]     Test net output #0: loss = 0.330978 (* 1 = 0.330978 loss)
I0503 06:16:45.152474 28586 solver.cpp:228] Iteration 11200, loss = 0.122809
I0503 06:16:45.152534 28586 solver.cpp:244]     Train net output #0: loss = 0.122805 (* 1 = 0.122805 loss)
I0503 06:16:45.152547 28586 sgd_solver.cpp:106] Iteration 11200, lr = 1e-10
I0503 06:19:01.233430 28586 solver.cpp:228] Iteration 11250, loss = 0.282165
I0503 06:19:01.233654 28586 solver.cpp:244]     Train net output #0: loss = 0.282161 (* 1 = 0.282161 loss)
I0503 06:19:01.233690 28586 sgd_solver.cpp:106] Iteration 11250, lr = 1e-10
I0503 06:21:17.468029 28586 solver.cpp:228] Iteration 11300, loss = 0.125157
I0503 06:21:17.468262 28586 solver.cpp:244]     Train net output #0: loss = 0.125153 (* 1 = 0.125153 loss)
I0503 06:21:17.468297 28586 sgd_solver.cpp:106] Iteration 11300, lr = 1e-10
I0503 06:23:33.414316 28586 solver.cpp:228] Iteration 11350, loss = 0.215549
I0503 06:23:33.414460 28586 solver.cpp:244]     Train net output #0: loss = 0.215546 (* 1 = 0.215546 loss)
I0503 06:23:33.414475 28586 sgd_solver.cpp:106] Iteration 11350, lr = 1e-10
I0503 06:25:47.029798 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_11400.caffemodel
I0503 06:25:50.043071 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_11400.solverstate
I0503 06:25:50.093823 28586 solver.cpp:337] Iteration 11400, Testing net (#0)
I0503 06:26:05.650462 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 06:26:20.510033 28586 solver.cpp:404]     Test net output #0: loss = 0.329277 (* 1 = 0.329277 loss)
I0503 06:26:21.412973 28586 solver.cpp:228] Iteration 11400, loss = 0.159662
I0503 06:26:21.413038 28586 solver.cpp:244]     Train net output #0: loss = 0.159658 (* 1 = 0.159658 loss)
I0503 06:26:21.413050 28586 sgd_solver.cpp:106] Iteration 11400, lr = 1e-10
I0503 06:28:37.194422 28586 solver.cpp:228] Iteration 11450, loss = 0.172997
I0503 06:28:37.194638 28586 solver.cpp:244]     Train net output #0: loss = 0.172994 (* 1 = 0.172994 loss)
I0503 06:28:37.194654 28586 sgd_solver.cpp:106] Iteration 11450, lr = 1e-10
I0503 06:30:53.620272 28586 solver.cpp:228] Iteration 11500, loss = 0.376622
I0503 06:30:53.620507 28586 solver.cpp:244]     Train net output #0: loss = 0.376618 (* 1 = 0.376618 loss)
I0503 06:30:53.620527 28586 sgd_solver.cpp:106] Iteration 11500, lr = 1e-10
I0503 06:33:09.901147 28586 solver.cpp:228] Iteration 11550, loss = 0.212846
I0503 06:33:09.901309 28586 solver.cpp:244]     Train net output #0: loss = 0.212842 (* 1 = 0.212842 loss)
I0503 06:33:09.901324 28586 sgd_solver.cpp:106] Iteration 11550, lr = 1e-10
I0503 06:35:23.079638 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_11600.caffemodel
I0503 06:35:26.129788 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_11600.solverstate
I0503 06:35:26.180341 28586 solver.cpp:337] Iteration 11600, Testing net (#0)
I0503 06:35:56.112337 28586 solver.cpp:404]     Test net output #0: loss = 0.32432 (* 1 = 0.32432 loss)
I0503 06:35:57.009975 28586 solver.cpp:228] Iteration 11600, loss = 0.0968705
I0503 06:35:57.010059 28586 solver.cpp:244]     Train net output #0: loss = 0.0968668 (* 1 = 0.0968668 loss)
I0503 06:35:57.010076 28586 sgd_solver.cpp:106] Iteration 11600, lr = 1e-10
I0503 06:38:13.248749 28586 solver.cpp:228] Iteration 11650, loss = 0.151153
I0503 06:38:13.248970 28586 solver.cpp:244]     Train net output #0: loss = 0.15115 (* 1 = 0.15115 loss)
I0503 06:38:13.248996 28586 sgd_solver.cpp:106] Iteration 11650, lr = 1e-10
I0503 06:40:29.258407 28586 solver.cpp:228] Iteration 11700, loss = 0.228595
I0503 06:40:29.258646 28586 solver.cpp:244]     Train net output #0: loss = 0.228591 (* 1 = 0.228591 loss)
I0503 06:40:29.258682 28586 sgd_solver.cpp:106] Iteration 11700, lr = 1e-10
I0503 06:42:45.574053 28586 solver.cpp:228] Iteration 11750, loss = 0.136129
I0503 06:42:45.574605 28586 solver.cpp:244]     Train net output #0: loss = 0.136126 (* 1 = 0.136126 loss)
I0503 06:42:45.574621 28586 sgd_solver.cpp:106] Iteration 11750, lr = 1e-10
I0503 06:44:59.284276 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_11800.caffemodel
I0503 06:45:11.754123 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_11800.solverstate
I0503 06:45:11.804648 28586 solver.cpp:337] Iteration 11800, Testing net (#0)
I0503 06:45:34.338096 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 06:45:41.730443 28586 solver.cpp:404]     Test net output #0: loss = 0.318607 (* 1 = 0.318607 loss)
I0503 06:45:42.627552 28586 solver.cpp:228] Iteration 11800, loss = 0.211974
I0503 06:45:42.627605 28586 solver.cpp:244]     Train net output #0: loss = 0.21197 (* 1 = 0.21197 loss)
I0503 06:45:42.627627 28586 sgd_solver.cpp:106] Iteration 11800, lr = 1e-10
I0503 06:47:58.802865 28586 solver.cpp:228] Iteration 11850, loss = 0.139147
I0503 06:47:58.803045 28586 solver.cpp:244]     Train net output #0: loss = 0.139143 (* 1 = 0.139143 loss)
I0503 06:47:58.803063 28586 sgd_solver.cpp:106] Iteration 11850, lr = 1e-10
I0503 06:50:14.845587 28586 solver.cpp:228] Iteration 11900, loss = 0.180885
I0503 06:50:14.845780 28586 solver.cpp:244]     Train net output #0: loss = 0.180881 (* 1 = 0.180881 loss)
I0503 06:50:14.845794 28586 sgd_solver.cpp:106] Iteration 11900, lr = 1e-10
I0503 06:52:30.914229 28586 solver.cpp:228] Iteration 11950, loss = 0.291254
I0503 06:52:30.914455 28586 solver.cpp:244]     Train net output #0: loss = 0.291251 (* 1 = 0.291251 loss)
I0503 06:52:30.914481 28586 sgd_solver.cpp:106] Iteration 11950, lr = 1e-10
I0503 06:54:44.428292 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_12000.caffemodel
I0503 06:54:50.663934 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_12000.solverstate
I0503 06:54:50.739375 28586 solver.cpp:337] Iteration 12000, Testing net (#0)
I0503 06:55:26.768057 28586 solver.cpp:404]     Test net output #0: loss = 0.34044 (* 1 = 0.34044 loss)
I0503 06:55:27.663231 28586 solver.cpp:228] Iteration 12000, loss = 0.154799
I0503 06:55:27.663300 28586 solver.cpp:244]     Train net output #0: loss = 0.154796 (* 1 = 0.154796 loss)
I0503 06:55:27.663314 28586 sgd_solver.cpp:106] Iteration 12000, lr = 1e-11
I0503 06:57:44.000217 28586 solver.cpp:228] Iteration 12050, loss = 0.20622
I0503 06:57:44.001902 28586 solver.cpp:244]     Train net output #0: loss = 0.206217 (* 1 = 0.206217 loss)
I0503 06:57:44.001935 28586 sgd_solver.cpp:106] Iteration 12050, lr = 1e-11
I0503 07:00:00.211665 28586 solver.cpp:228] Iteration 12100, loss = 0.132194
I0503 07:00:00.224113 28586 solver.cpp:244]     Train net output #0: loss = 0.13219 (* 1 = 0.13219 loss)
I0503 07:00:00.224133 28586 sgd_solver.cpp:106] Iteration 12100, lr = 1e-11
I0503 07:02:16.356029 28586 solver.cpp:228] Iteration 12150, loss = 0.0894203
I0503 07:02:16.356187 28586 solver.cpp:244]     Train net output #0: loss = 0.0894167 (* 1 = 0.0894167 loss)
I0503 07:02:16.356201 28586 sgd_solver.cpp:106] Iteration 12150, lr = 1e-11
I0503 07:04:29.850744 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_12200.caffemodel
I0503 07:04:32.883047 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_12200.solverstate
I0503 07:04:32.933322 28586 solver.cpp:337] Iteration 12200, Testing net (#0)
I0503 07:04:58.474126 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 07:05:02.383915 28586 solver.cpp:404]     Test net output #0: loss = 0.335962 (* 1 = 0.335962 loss)
I0503 07:05:03.279620 28586 solver.cpp:228] Iteration 12200, loss = 0.467147
I0503 07:05:03.279690 28586 solver.cpp:244]     Train net output #0: loss = 0.467143 (* 1 = 0.467143 loss)
I0503 07:05:03.279701 28586 sgd_solver.cpp:106] Iteration 12200, lr = 1e-11
I0503 07:07:19.521728 28586 solver.cpp:228] Iteration 12250, loss = 0.250759
I0503 07:07:19.521893 28586 solver.cpp:244]     Train net output #0: loss = 0.250756 (* 1 = 0.250756 loss)
I0503 07:07:19.521908 28586 sgd_solver.cpp:106] Iteration 12250, lr = 1e-11
I0503 07:09:35.776978 28586 solver.cpp:228] Iteration 12300, loss = 0.0949899
I0503 07:09:35.777129 28586 solver.cpp:244]     Train net output #0: loss = 0.0949863 (* 1 = 0.0949863 loss)
I0503 07:09:35.777143 28586 sgd_solver.cpp:106] Iteration 12300, lr = 1e-11
I0503 07:11:51.891608 28586 solver.cpp:228] Iteration 12350, loss = 0.173528
I0503 07:11:51.891772 28586 solver.cpp:244]     Train net output #0: loss = 0.173524 (* 1 = 0.173524 loss)
I0503 07:11:51.891785 28586 sgd_solver.cpp:106] Iteration 12350, lr = 1e-11
I0503 07:14:05.197618 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_12400.caffemodel
I0503 07:14:10.130198 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_12400.solverstate
I0503 07:14:10.179651 28586 solver.cpp:337] Iteration 12400, Testing net (#0)
I0503 07:14:39.381845 28586 solver.cpp:404]     Test net output #0: loss = 0.33587 (* 1 = 0.33587 loss)
I0503 07:14:40.287014 28586 solver.cpp:228] Iteration 12400, loss = 0.226203
I0503 07:14:40.287080 28586 solver.cpp:244]     Train net output #0: loss = 0.2262 (* 1 = 0.2262 loss)
I0503 07:14:40.287091 28586 sgd_solver.cpp:106] Iteration 12400, lr = 1e-11
I0503 07:16:56.598137 28586 solver.cpp:228] Iteration 12450, loss = 0.445864
I0503 07:16:56.598297 28586 solver.cpp:244]     Train net output #0: loss = 0.445861 (* 1 = 0.445861 loss)
I0503 07:16:56.598314 28586 sgd_solver.cpp:106] Iteration 12450, lr = 1e-11
I0503 07:19:12.882102 28586 solver.cpp:228] Iteration 12500, loss = 0.286793
I0503 07:19:12.882251 28586 solver.cpp:244]     Train net output #0: loss = 0.286789 (* 1 = 0.286789 loss)
I0503 07:19:12.882264 28586 sgd_solver.cpp:106] Iteration 12500, lr = 1e-11
I0503 07:21:28.918972 28586 solver.cpp:228] Iteration 12550, loss = 0.349153
I0503 07:21:28.919128 28586 solver.cpp:244]     Train net output #0: loss = 0.34915 (* 1 = 0.34915 loss)
I0503 07:21:28.919145 28586 sgd_solver.cpp:106] Iteration 12550, lr = 1e-11
I0503 07:23:42.224797 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_12600.caffemodel
I0503 07:23:45.516441 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_12600.solverstate
I0503 07:23:45.566308 28586 solver.cpp:337] Iteration 12600, Testing net (#0)
I0503 07:24:14.622653 28586 solver.cpp:404]     Test net output #0: loss = 0.340621 (* 1 = 0.340621 loss)
I0503 07:24:15.538972 28586 solver.cpp:228] Iteration 12600, loss = 0.351762
I0503 07:24:15.539027 28586 solver.cpp:244]     Train net output #0: loss = 0.351758 (* 1 = 0.351758 loss)
I0503 07:24:15.539048 28586 sgd_solver.cpp:106] Iteration 12600, lr = 1e-11
I0503 07:26:31.766882 28586 solver.cpp:228] Iteration 12650, loss = 0.171249
I0503 07:26:31.767052 28586 solver.cpp:244]     Train net output #0: loss = 0.171246 (* 1 = 0.171246 loss)
I0503 07:26:31.767067 28586 sgd_solver.cpp:106] Iteration 12650, lr = 1e-11
I0503 07:28:48.137882 28586 solver.cpp:228] Iteration 12700, loss = 0.107142
I0503 07:28:48.139114 28586 solver.cpp:244]     Train net output #0: loss = 0.107138 (* 1 = 0.107138 loss)
I0503 07:28:48.139128 28586 sgd_solver.cpp:106] Iteration 12700, lr = 1e-11
I0503 07:31:04.515982 28586 solver.cpp:228] Iteration 12750, loss = 0.329359
I0503 07:31:04.516201 28586 solver.cpp:244]     Train net output #0: loss = 0.329355 (* 1 = 0.329355 loss)
I0503 07:31:04.516237 28586 sgd_solver.cpp:106] Iteration 12750, lr = 1e-11
I0503 07:33:17.985594 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_12800.caffemodel
I0503 07:33:31.466578 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_12800.solverstate
I0503 07:33:31.518306 28586 solver.cpp:337] Iteration 12800, Testing net (#0)
I0503 07:34:00.979830 28586 solver.cpp:404]     Test net output #0: loss = 0.341513 (* 1 = 0.341513 loss)
I0503 07:34:01.877185 28586 solver.cpp:228] Iteration 12800, loss = 0.078992
I0503 07:34:01.877245 28586 solver.cpp:244]     Train net output #0: loss = 0.0789885 (* 1 = 0.0789885 loss)
I0503 07:34:01.877261 28586 sgd_solver.cpp:106] Iteration 12800, lr = 1e-11
I0503 07:36:17.894718 28586 solver.cpp:228] Iteration 12850, loss = 0.207544
I0503 07:36:17.896190 28586 solver.cpp:244]     Train net output #0: loss = 0.20754 (* 1 = 0.20754 loss)
I0503 07:36:17.896224 28586 sgd_solver.cpp:106] Iteration 12850, lr = 1e-11
I0503 07:38:34.032562 28586 solver.cpp:228] Iteration 12900, loss = 0.158436
I0503 07:38:34.032762 28586 solver.cpp:244]     Train net output #0: loss = 0.158432 (* 1 = 0.158432 loss)
I0503 07:38:34.032784 28586 sgd_solver.cpp:106] Iteration 12900, lr = 1e-11
I0503 07:40:50.416430 28586 solver.cpp:228] Iteration 12950, loss = 0.175504
I0503 07:40:50.416669 28586 solver.cpp:244]     Train net output #0: loss = 0.1755 (* 1 = 0.1755 loss)
I0503 07:40:50.416697 28586 sgd_solver.cpp:106] Iteration 12950, lr = 1e-11
I0503 07:43:03.925194 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_13000.caffemodel
I0503 07:43:19.153821 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_13000.solverstate
I0503 07:43:19.207299 28586 solver.cpp:337] Iteration 13000, Testing net (#0)
I0503 07:43:48.909379 28586 solver.cpp:404]     Test net output #0: loss = 0.328997 (* 1 = 0.328997 loss)
I0503 07:43:49.812067 28586 solver.cpp:228] Iteration 13000, loss = 0.22091
I0503 07:43:49.812131 28586 solver.cpp:244]     Train net output #0: loss = 0.220907 (* 1 = 0.220907 loss)
I0503 07:43:49.812142 28586 sgd_solver.cpp:106] Iteration 13000, lr = 1e-11
I0503 07:46:05.918483 28586 solver.cpp:228] Iteration 13050, loss = 0.14663
I0503 07:46:05.918707 28586 solver.cpp:244]     Train net output #0: loss = 0.146627 (* 1 = 0.146627 loss)
I0503 07:46:05.918736 28586 sgd_solver.cpp:106] Iteration 13050, lr = 1e-11
I0503 07:48:21.979934 28586 solver.cpp:228] Iteration 13100, loss = 0.0451786
I0503 07:48:21.980101 28586 solver.cpp:244]     Train net output #0: loss = 0.045175 (* 1 = 0.045175 loss)
I0503 07:48:21.980118 28586 sgd_solver.cpp:106] Iteration 13100, lr = 1e-11
I0503 07:50:38.144739 28586 solver.cpp:228] Iteration 13150, loss = 0.232511
I0503 07:50:38.144902 28586 solver.cpp:244]     Train net output #0: loss = 0.232507 (* 1 = 0.232507 loss)
I0503 07:50:38.144917 28586 sgd_solver.cpp:106] Iteration 13150, lr = 1e-11
I0503 07:52:51.634989 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_13200.caffemodel
I0503 07:52:58.401407 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_13200.solverstate
I0503 07:52:58.455157 28586 solver.cpp:337] Iteration 13200, Testing net (#0)
I0503 07:53:12.123335 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 07:53:28.690743 28586 solver.cpp:404]     Test net output #0: loss = 0.330992 (* 1 = 0.330992 loss)
I0503 07:53:29.597960 28586 solver.cpp:228] Iteration 13200, loss = 0.164731
I0503 07:53:29.598023 28586 solver.cpp:244]     Train net output #0: loss = 0.164728 (* 1 = 0.164728 loss)
I0503 07:53:29.598034 28586 sgd_solver.cpp:106] Iteration 13200, lr = 1e-11
I0503 07:55:45.768205 28586 solver.cpp:228] Iteration 13250, loss = 0.291512
I0503 07:55:45.768399 28586 solver.cpp:244]     Train net output #0: loss = 0.291509 (* 1 = 0.291509 loss)
I0503 07:55:45.768424 28586 sgd_solver.cpp:106] Iteration 13250, lr = 1e-11
I0503 07:58:01.973055 28586 solver.cpp:228] Iteration 13300, loss = 0.106308
I0503 07:58:01.973305 28586 solver.cpp:244]     Train net output #0: loss = 0.106304 (* 1 = 0.106304 loss)
I0503 07:58:01.973351 28586 sgd_solver.cpp:106] Iteration 13300, lr = 1e-11
I0503 08:00:17.694362 28586 solver.cpp:228] Iteration 13350, loss = 0.251927
I0503 08:00:17.694608 28586 solver.cpp:244]     Train net output #0: loss = 0.251923 (* 1 = 0.251923 loss)
I0503 08:00:17.694646 28586 sgd_solver.cpp:106] Iteration 13350, lr = 1e-11
I0503 08:02:30.935251 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_13400.caffemodel
I0503 08:02:38.825075 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_13400.solverstate
I0503 08:02:38.883699 28586 solver.cpp:337] Iteration 13400, Testing net (#0)
I0503 08:03:09.485927 28586 solver.cpp:404]     Test net output #0: loss = 0.329288 (* 1 = 0.329288 loss)
I0503 08:03:10.380933 28586 solver.cpp:228] Iteration 13400, loss = 0.102011
I0503 08:03:10.380996 28586 solver.cpp:244]     Train net output #0: loss = 0.102007 (* 1 = 0.102007 loss)
I0503 08:03:10.381007 28586 sgd_solver.cpp:106] Iteration 13400, lr = 1e-11
I0503 08:05:26.142074 28586 solver.cpp:228] Iteration 13450, loss = 0.0477642
I0503 08:05:26.144250 28586 solver.cpp:244]     Train net output #0: loss = 0.0477604 (* 1 = 0.0477604 loss)
I0503 08:05:26.144268 28586 sgd_solver.cpp:106] Iteration 13450, lr = 1e-11
I0503 08:07:42.054669 28586 solver.cpp:228] Iteration 13500, loss = 0.145644
I0503 08:07:42.054842 28586 solver.cpp:244]     Train net output #0: loss = 0.14564 (* 1 = 0.14564 loss)
I0503 08:07:42.054854 28586 sgd_solver.cpp:106] Iteration 13500, lr = 1e-11
I0503 08:09:58.044441 28586 solver.cpp:228] Iteration 13550, loss = 0.225211
I0503 08:09:58.044567 28586 solver.cpp:244]     Train net output #0: loss = 0.225207 (* 1 = 0.225207 loss)
I0503 08:09:58.044581 28586 sgd_solver.cpp:106] Iteration 13550, lr = 1e-11
I0503 08:12:11.320029 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_13600.caffemodel
I0503 08:12:26.335057 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_13600.solverstate
I0503 08:12:26.385107 28586 solver.cpp:337] Iteration 13600, Testing net (#0)
I0503 08:12:50.705708 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 08:12:56.114197 28586 solver.cpp:404]     Test net output #0: loss = 0.324327 (* 1 = 0.324327 loss)
I0503 08:12:57.011628 28586 solver.cpp:228] Iteration 13600, loss = 0.404064
I0503 08:12:57.011710 28586 solver.cpp:244]     Train net output #0: loss = 0.40406 (* 1 = 0.40406 loss)
I0503 08:12:57.011729 28586 sgd_solver.cpp:106] Iteration 13600, lr = 1e-11
I0503 08:15:13.183334 28586 solver.cpp:228] Iteration 13650, loss = 0.102018
I0503 08:15:13.183486 28586 solver.cpp:244]     Train net output #0: loss = 0.102014 (* 1 = 0.102014 loss)
I0503 08:15:13.183500 28586 sgd_solver.cpp:106] Iteration 13650, lr = 1e-11
I0503 08:17:29.220638 28586 solver.cpp:228] Iteration 13700, loss = 0.121036
I0503 08:17:29.233172 28586 solver.cpp:244]     Train net output #0: loss = 0.121033 (* 1 = 0.121033 loss)
I0503 08:17:29.233206 28586 sgd_solver.cpp:106] Iteration 13700, lr = 1e-11
I0503 08:19:45.461951 28586 solver.cpp:228] Iteration 13750, loss = 0.053307
I0503 08:19:45.462183 28586 solver.cpp:244]     Train net output #0: loss = 0.0533031 (* 1 = 0.0533031 loss)
I0503 08:19:45.462208 28586 sgd_solver.cpp:106] Iteration 13750, lr = 1e-11
I0503 08:21:58.915776 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_13800.caffemodel
I0503 08:22:17.851423 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_13800.solverstate
I0503 08:22:17.904070 28586 solver.cpp:337] Iteration 13800, Testing net (#0)
I0503 08:22:48.237633 28586 solver.cpp:404]     Test net output #0: loss = 0.318615 (* 1 = 0.318615 loss)
I0503 08:22:49.132460 28586 solver.cpp:228] Iteration 13800, loss = 0.19568
I0503 08:22:49.132555 28586 solver.cpp:244]     Train net output #0: loss = 0.195676 (* 1 = 0.195676 loss)
I0503 08:22:49.132572 28586 sgd_solver.cpp:106] Iteration 13800, lr = 1e-11
I0503 08:25:04.637599 28586 solver.cpp:228] Iteration 13850, loss = 0.050161
I0503 08:25:04.637758 28586 solver.cpp:244]     Train net output #0: loss = 0.0501572 (* 1 = 0.0501572 loss)
I0503 08:25:04.637771 28586 sgd_solver.cpp:106] Iteration 13850, lr = 1e-11
I0503 08:27:21.009413 28586 solver.cpp:228] Iteration 13900, loss = 0.119553
I0503 08:27:21.009611 28586 solver.cpp:244]     Train net output #0: loss = 0.11955 (* 1 = 0.11955 loss)
I0503 08:27:21.009636 28586 sgd_solver.cpp:106] Iteration 13900, lr = 1e-11
I0503 08:29:37.047348 28586 solver.cpp:228] Iteration 13950, loss = 0.21665
I0503 08:29:37.047513 28586 solver.cpp:244]     Train net output #0: loss = 0.216646 (* 1 = 0.216646 loss)
I0503 08:29:37.047528 28586 sgd_solver.cpp:106] Iteration 13950, lr = 1e-11
I0503 08:31:50.503712 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_14000.caffemodel
I0503 08:32:07.167557 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_14000.solverstate
I0503 08:32:07.218289 28586 solver.cpp:337] Iteration 14000, Testing net (#0)
I0503 08:32:36.770877 28586 solver.cpp:404]     Test net output #0: loss = 0.34044 (* 1 = 0.34044 loss)
I0503 08:32:37.665300 28586 solver.cpp:228] Iteration 14000, loss = 0.11931
I0503 08:32:37.665374 28586 solver.cpp:244]     Train net output #0: loss = 0.119306 (* 1 = 0.119306 loss)
I0503 08:32:37.665387 28586 sgd_solver.cpp:106] Iteration 14000, lr = 1e-12
I0503 08:34:53.695749 28586 solver.cpp:228] Iteration 14050, loss = 0.128251
I0503 08:34:53.695976 28586 solver.cpp:244]     Train net output #0: loss = 0.128247 (* 1 = 0.128247 loss)
I0503 08:34:53.695993 28586 sgd_solver.cpp:106] Iteration 14050, lr = 1e-12
I0503 08:37:09.869170 28586 solver.cpp:228] Iteration 14100, loss = 0.26602
I0503 08:37:09.869344 28586 solver.cpp:244]     Train net output #0: loss = 0.266016 (* 1 = 0.266016 loss)
I0503 08:37:09.869359 28586 sgd_solver.cpp:106] Iteration 14100, lr = 1e-12
I0503 08:39:25.994146 28586 solver.cpp:228] Iteration 14150, loss = 0.100206
I0503 08:39:25.994341 28586 solver.cpp:244]     Train net output #0: loss = 0.100203 (* 1 = 0.100203 loss)
I0503 08:39:25.994357 28586 sgd_solver.cpp:106] Iteration 14150, lr = 1e-12
I0503 08:41:39.332773 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_14200.caffemodel
I0503 08:41:52.915242 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_14200.solverstate
I0503 08:41:52.966246 28586 solver.cpp:337] Iteration 14200, Testing net (#0)
I0503 08:42:01.761961 28586 blocking_queue.cpp:50] Data layer prefetch queue empty
I0503 08:42:22.597754 28586 solver.cpp:404]     Test net output #0: loss = 0.335962 (* 1 = 0.335962 loss)
I0503 08:42:23.496227 28586 solver.cpp:228] Iteration 14200, loss = 0.29178
I0503 08:42:23.496299 28586 solver.cpp:244]     Train net output #0: loss = 0.291776 (* 1 = 0.291776 loss)
I0503 08:42:23.496315 28586 sgd_solver.cpp:106] Iteration 14200, lr = 1e-12
I0503 08:44:39.554188 28586 solver.cpp:228] Iteration 14250, loss = 0.420559
I0503 08:44:39.554419 28586 solver.cpp:244]     Train net output #0: loss = 0.420556 (* 1 = 0.420556 loss)
I0503 08:44:39.554432 28586 sgd_solver.cpp:106] Iteration 14250, lr = 1e-12
I0503 08:46:55.751829 28586 solver.cpp:228] Iteration 14300, loss = 0.440459
I0503 08:46:55.752029 28586 solver.cpp:244]     Train net output #0: loss = 0.440455 (* 1 = 0.440455 loss)
I0503 08:46:55.752058 28586 sgd_solver.cpp:106] Iteration 14300, lr = 1e-12
I0503 08:49:11.874495 28586 solver.cpp:228] Iteration 14350, loss = 0.171897
I0503 08:49:11.874706 28586 solver.cpp:244]     Train net output #0: loss = 0.171893 (* 1 = 0.171893 loss)
I0503 08:49:11.874742 28586 sgd_solver.cpp:106] Iteration 14350, lr = 1e-12
I0503 08:51:25.209612 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_14400.caffemodel
I0503 08:51:32.545349 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_14400.solverstate
I0503 08:51:32.600623 28586 solver.cpp:337] Iteration 14400, Testing net (#0)
I0503 08:52:02.084591 28586 solver.cpp:404]     Test net output #0: loss = 0.33587 (* 1 = 0.33587 loss)
I0503 08:52:02.990762 28586 solver.cpp:228] Iteration 14400, loss = 0.167153
I0503 08:52:02.990835 28586 solver.cpp:244]     Train net output #0: loss = 0.167149 (* 1 = 0.167149 loss)
I0503 08:52:02.990847 28586 sgd_solver.cpp:106] Iteration 14400, lr = 1e-12
I0503 08:54:19.259104 28586 solver.cpp:228] Iteration 14450, loss = 0.160785
I0503 08:54:19.259281 28586 solver.cpp:244]     Train net output #0: loss = 0.160781 (* 1 = 0.160781 loss)
I0503 08:54:19.259307 28586 sgd_solver.cpp:106] Iteration 14450, lr = 1e-12
I0503 08:56:35.443779 28586 solver.cpp:228] Iteration 14500, loss = 0.222876
I0503 08:56:35.443950 28586 solver.cpp:244]     Train net output #0: loss = 0.222872 (* 1 = 0.222872 loss)
I0503 08:56:35.443964 28586 sgd_solver.cpp:106] Iteration 14500, lr = 1e-12
I0503 08:58:51.620242 28586 solver.cpp:228] Iteration 14550, loss = 0.38366
I0503 08:58:51.620426 28586 solver.cpp:244]     Train net output #0: loss = 0.383657 (* 1 = 0.383657 loss)
I0503 08:58:51.620447 28586 sgd_solver.cpp:106] Iteration 14550, lr = 1e-12
I0503 09:01:05.144877 28586 solver.cpp:454] Snapshotting to binary proto file three_stream_triplet_loss_iter_14600.caffemodel
I0503 09:01:19.079764 28586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file three_stream_triplet_loss_iter_14600.solverstate
I0503 09:01:19.159540 28586 solver.cpp:337] Iteration 14600, Testing net (#0)
I0503 09:01:48.173050 28586 solver.cpp:404]     Test net output #0: loss = 0.340621 (* 1 = 0.340621 loss)
I0503 09:01:49.067044 28586 solver.cpp:228] Iteration 14600, loss = 0.333533
I0503 09:01:49.067106 28586 solver.cpp:244]     Train net output #0: loss = 0.333529 (* 1 = 0.333529 loss)
I0503 09:01:49.067117 28586 sgd_solver.cpp:106] Iteration 14600, lr = 1e-12
I0503 09:04:05.185554 28586 solver.cpp:228] Iteration 14650, loss = 0.201041
I0503 09:04:05.185714 28586 solver.cpp:244]     Train net output #0: loss = 0.201037 (* 1 = 0.201037 loss)
I0503 09:04:05.185727 28586 sgd_solver.cpp:106] Iteration 14650, lr = 1e-12
I0503 09:06:21.447126 28586 solver.cpp:228] Iteration 14700, loss = 0.254338
I0503 09:06:21.447293 28586 solver.cpp:244]     Train net output #0: loss = 0.254334 (* 1 = 0.254334 loss)
I0503 09:06:21.447309 28586 sgd_solver.cpp:106] Iteration 14700, lr = 1e-12
