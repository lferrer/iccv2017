I0507 17:39:06.096563  5077 caffe.cpp:217] Using GPUs 1
I0507 17:39:06.186945  5077 caffe.cpp:222] GPU 1: GeForce GTX TITAN X
I0507 17:39:07.186417  5077 solver.cpp:48] Initializing solver from parameters: 
test_iter: 500
test_interval: 200
base_lr: 1e-05
display: 50
max_iter: 60000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
stepsize: 20000
snapshot: 2000
snapshot_prefix: "../../weights/three_stream_triplet_loss"
solver_mode: GPU
device_id: 1
net: "../../models/three_stream_triplet_loss.prototxt"
train_state {
  level: 0
  stage: ""
}
test_state {
  stage: "test-on-val"
}
I0507 17:39:07.199417  5077 solver.cpp:91] Creating training net from net file: ../../models/three_stream_triplet_loss.prototxt
I0507 17:39:07.203500  5077 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0507 17:39:07.204947  5077 net.cpp:58] Initializing net from parameters: 
name: "C3D-Three-Streams"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "triplet"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 112
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
  }
  data_param {
    source: "/data/leo-data/Synthetic/LMDB/Triplets/train"
    batch_size: 18
    backend: LMDB
  }
}
layer {
  name: "slicer"
  type: "Slice"
  bottom: "triplet"
  top: "anchor_stacked"
  top: "positive_stacked"
  top: "negative_stacked"
  slice_param {
    slice_dim: 1
    slice_point: 48
    slice_point: 96
  }
}
layer {
  name: "reshape_anchor"
  type: "Reshape"
  bottom: "anchor_stacked"
  top: "anchor"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "reshape_positive"
  type: "Reshape"
  bottom: "positive_stacked"
  top: "positive"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "reshape_negative"
  type: "Reshape"
  bottom: "negative_stacked"
  top: "negative"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "conv1a"
  type: "NdConvolution"
  bottom: "anchor"
  top: "conv1a"
  param {
    name: "conv1a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "pool1"
  type: "NdPooling"
  bottom: "conv1a"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a"
  type: "NdConvolution"
  bottom: "pool1"
  top: "conv2a"
  param {
    name: "conv2a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a"
  type: "ReLU"
  bottom: "conv2a"
  top: "conv2a"
}
layer {
  name: "pool2"
  type: "NdPooling"
  bottom: "conv2a"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a"
  type: "NdConvolution"
  bottom: "pool2"
  top: "conv3a"
  param {
    name: "conv3a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a"
  type: "ReLU"
  bottom: "conv3a"
  top: "conv3a"
}
layer {
  name: "pool3"
  type: "NdPooling"
  bottom: "conv3a"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a"
  type: "NdConvolution"
  bottom: "pool3"
  top: "conv4a"
  param {
    name: "conv4a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a"
  type: "ReLU"
  bottom: "conv4a"
  top: "conv4a"
}
layer {
  name: "pool4"
  type: "NdPooling"
  bottom: "conv4a"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a"
  type: "NdConvolution"
  bottom: "pool4"
  top: "conv5a"
  param {
    name: "conv5a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a"
  type: "ReLU"
  bottom: "conv5a"
  top: "conv5a"
}
layer {
  name: "pool5"
  type: "NdPooling"
  bottom: "conv5a"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv1a_pos"
  type: "NdConvolution"
  bottom: "positive"
  top: "conv1a_pos"
  param {
    name: "conv1a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a_pos"
  type: "ReLU"
  bottom: "conv1a_pos"
  top: "conv1a_pos"
}
layer {
  name: "pool1_pos"
  type: "NdPooling"
  bottom: "conv1a_pos"
  top: "pool1_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a_pos"
  type: "NdConvolution"
  bottom: "pool1_pos"
  top: "conv2a_pos"
  param {
    name: "conv2a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a_pos"
  type: "ReLU"
  bottom: "conv2a_pos"
  top: "conv2a_pos"
}
layer {
  name: "pool2_pos"
  type: "NdPooling"
  bottom: "conv2a_pos"
  top: "pool2_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a_pos"
  type: "NdConvolution"
  bottom: "pool2_pos"
  top: "conv3a_pos"
  param {
    name: "conv3a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a_pos"
  type: "ReLU"
  bottom: "conv3a_pos"
  top: "conv3a_pos"
}
layer {
  name: "pool3_pos"
  type: "NdPooling"
  bottom: "conv3a_pos"
  top: "pool3_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a_pos"
  type: "NdConvolution"
  bottom: "pool3_pos"
  top: "conv4a_pos"
  param {
    name: "conv4a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a_pos"
  type: "ReLU"
  bottom: "conv4a_pos"
  top: "conv4a_pos"
}
layer {
  name: "pool4_pos"
  type: "NdPooling"
  bottom: "conv4a_pos"
  top: "pool4_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a_pos"
  type: "NdConvolution"
  bottom: "pool4_pos"
  top: "conv5a_pos"
  param {
    name: "conv5a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a_pos"
  type: "ReLU"
  bottom: "conv5a_pos"
  top: "conv5a_pos"
}
layer {
  name: "pool5_pos"
  type: "NdPooling"
  bottom: "conv5a_pos"
  top: "pool5_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6_pos"
  type: "InnerProduct"
  bottom: "pool5_pos"
  top: "fc6_pos"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6_pos"
  type: "ReLU"
  bottom: "fc6_pos"
  top: "fc6_pos"
}
layer {
  name: "drop6_pos"
  type: "Dropout"
  bottom: "fc6_pos"
  top: "fc6_pos"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_pos"
  type: "InnerProduct"
  bottom: "fc6_pos"
  top: "fc7_pos"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7_pos"
  type: "ReLU"
  bottom: "fc7_pos"
  top: "fc7_pos"
}
layer {
  name: "drop7_pos"
  type: "Dropout"
  bottom: "fc7_pos"
  top: "fc7_pos"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv1a_neg"
  type: "NdConvolution"
  bottom: "negative"
  top: "conv1a_neg"
  param {
    name: "conv1a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a_neg"
  type: "ReLU"
  bottom: "conv1a_neg"
  top: "conv1a_neg"
}
layer {
  name: "pool1_neg"
  type: "NdPooling"
  bottom: "conv1a_neg"
  top: "pool1_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a_neg"
  type: "NdConvolution"
  bottom: "pool1_neg"
  top: "conv2a_neg"
  param {
    name: "conv2a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a_neg"
  type: "ReLU"
  bottom: "conv2a_neg"
  top: "conv2a_neg"
}
layer {
  name: "pool2_neg"
  type: "NdPooling"
  bottom: "conv2a_neg"
  top: "pool2_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a_neg"
  type: "NdConvolution"
  bottom: "pool2_neg"
  top: "conv3a_neg"
  param {
    name: "conv3a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a_neg"
  type: "ReLU"
  bottom: "conv3a_neg"
  top: "conv3a_neg"
}
layer {
  name: "pool3_neg"
  type: "NdPooling"
  bottom: "conv3a_neg"
  top: "pool3_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a_neg"
  type: "NdConvolution"
  bottom: "pool3_neg"
  top: "conv4a_neg"
  param {
    name: "conv4a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a_neg"
  type: "ReLU"
  bottom: "conv4a_neg"
  top: "conv4a_neg"
}
layer {
  name: "pool4_neg"
  type: "NdPooling"
  bottom: "conv4a_neg"
  top: "pool4_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a_neg"
  type: "NdConvolution"
  bottom: "pool4_neg"
  top: "conv5a_neg"
  param {
    name: "conv5a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a_neg"
  type: "ReLU"
  bottom: "conv5a_neg"
  top: "conv5a_neg"
}
layer {
  name: "pool5_neg"
  type: "NdPooling"
  bottom: "conv5a_neg"
  top: "pool5_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6_neg"
  type: "InnerProduct"
  bottom: "pool5_neg"
  top: "fc6_neg"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6_neg"
  type: "ReLU"
  bottom: "fc6_neg"
  top: "fc6_neg"
}
layer {
  name: "drop6_neg"
  type: "Dropout"
  bottom: "fc6_neg"
  top: "fc6_neg"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_neg"
  type: "InnerProduct"
  bottom: "fc6_neg"
  top: "fc7_neg"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7_neg"
  type: "ReLU"
  bottom: "fc7_neg"
  top: "fc7_neg"
}
layer {
  name: "drop7_neg"
  type: "Dropout"
  bottom: "fc7_neg"
  top: "fc7_neg"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "TripletLoss"
  bottom: "fc7"
  bottom: "fc7_pos"
  bottom: "fc7_neg"
  top: "loss"
  threshold_param {
    threshold: 1
  }
}
I0507 17:39:07.205736  5077 layer_factory.hpp:77] Creating layer data
I0507 17:39:07.207190  5077 net.cpp:100] Creating Layer data
I0507 17:39:07.207223  5077 net.cpp:408] data -> triplet
I0507 17:39:07.209429  5087 db_lmdb.cpp:35] Opened lmdb /data/leo-data/Synthetic/LMDB/Triplets/train
I0507 17:39:07.282196  5077 data_layer.cpp:41] output data size: 18,144,112,112
I0507 17:39:07.611075  5077 net.cpp:150] Setting up data
I0507 17:39:07.611147  5077 net.cpp:157] Top shape: 18 144 112 112 (32514048)
I0507 17:39:07.611158  5077 net.cpp:165] Memory required for data: 130056192
I0507 17:39:07.611191  5077 layer_factory.hpp:77] Creating layer slicer
I0507 17:39:07.611244  5077 net.cpp:100] Creating Layer slicer
I0507 17:39:07.611258  5077 net.cpp:434] slicer <- triplet
I0507 17:39:07.611277  5077 net.cpp:408] slicer -> anchor_stacked
I0507 17:39:07.611304  5077 net.cpp:408] slicer -> positive_stacked
I0507 17:39:07.611315  5077 net.cpp:408] slicer -> negative_stacked
I0507 17:39:07.611500  5077 net.cpp:150] Setting up slicer
I0507 17:39:07.611511  5077 net.cpp:157] Top shape: 18 48 112 112 (10838016)
I0507 17:39:07.611518  5077 net.cpp:157] Top shape: 18 48 112 112 (10838016)
I0507 17:39:07.611524  5077 net.cpp:157] Top shape: 18 48 112 112 (10838016)
I0507 17:39:07.611528  5077 net.cpp:165] Memory required for data: 260112384
I0507 17:39:07.611541  5077 layer_factory.hpp:77] Creating layer reshape_anchor
I0507 17:39:07.611570  5077 net.cpp:100] Creating Layer reshape_anchor
I0507 17:39:07.611578  5077 net.cpp:434] reshape_anchor <- anchor_stacked
I0507 17:39:07.611593  5077 net.cpp:408] reshape_anchor -> anchor
I0507 17:39:07.611644  5077 net.cpp:150] Setting up reshape_anchor
I0507 17:39:07.611655  5077 net.cpp:157] Top shape: 18 3 16 112 112 (10838016)
I0507 17:39:07.611660  5077 net.cpp:165] Memory required for data: 303464448
I0507 17:39:07.611665  5077 layer_factory.hpp:77] Creating layer reshape_positive
I0507 17:39:07.611672  5077 net.cpp:100] Creating Layer reshape_positive
I0507 17:39:07.611680  5077 net.cpp:434] reshape_positive <- positive_stacked
I0507 17:39:07.611687  5077 net.cpp:408] reshape_positive -> positive
I0507 17:39:07.611755  5077 net.cpp:150] Setting up reshape_positive
I0507 17:39:07.611765  5077 net.cpp:157] Top shape: 18 3 16 112 112 (10838016)
I0507 17:39:07.611769  5077 net.cpp:165] Memory required for data: 346816512
I0507 17:39:07.611774  5077 layer_factory.hpp:77] Creating layer reshape_negative
I0507 17:39:07.611784  5077 net.cpp:100] Creating Layer reshape_negative
I0507 17:39:07.611791  5077 net.cpp:434] reshape_negative <- negative_stacked
I0507 17:39:07.611800  5077 net.cpp:408] reshape_negative -> negative
I0507 17:39:07.611826  5077 net.cpp:150] Setting up reshape_negative
I0507 17:39:07.611835  5077 net.cpp:157] Top shape: 18 3 16 112 112 (10838016)
I0507 17:39:07.611840  5077 net.cpp:165] Memory required for data: 390168576
I0507 17:39:07.611845  5077 layer_factory.hpp:77] Creating layer conv1a
I0507 17:39:07.611861  5077 net.cpp:100] Creating Layer conv1a
I0507 17:39:07.611867  5077 net.cpp:434] conv1a <- anchor
I0507 17:39:07.611879  5077 net.cpp:408] conv1a -> conv1a
I0507 17:39:08.308497  5077 net.cpp:150] Setting up conv1a
I0507 17:39:08.308547  5077 net.cpp:157] Top shape: 18 64 16 112 112 (231211008)
I0507 17:39:08.308550  5077 net.cpp:165] Memory required for data: 1315012608
I0507 17:39:08.308571  5077 layer_factory.hpp:77] Creating layer relu1a
I0507 17:39:08.308588  5077 net.cpp:100] Creating Layer relu1a
I0507 17:39:08.308591  5077 net.cpp:434] relu1a <- conv1a
I0507 17:39:08.308598  5077 net.cpp:395] relu1a -> conv1a (in-place)
I0507 17:39:08.309506  5077 net.cpp:150] Setting up relu1a
I0507 17:39:08.309520  5077 net.cpp:157] Top shape: 18 64 16 112 112 (231211008)
I0507 17:39:08.309535  5077 net.cpp:165] Memory required for data: 2239856640
I0507 17:39:08.309540  5077 layer_factory.hpp:77] Creating layer pool1
I0507 17:39:08.309550  5077 net.cpp:100] Creating Layer pool1
I0507 17:39:08.309552  5077 net.cpp:434] pool1 <- conv1a
I0507 17:39:08.309558  5077 net.cpp:408] pool1 -> pool1
I0507 17:39:08.309810  5077 net.cpp:150] Setting up pool1
I0507 17:39:08.309823  5077 net.cpp:157] Top shape: 18 64 16 56 56 (57802752)
I0507 17:39:08.309825  5077 net.cpp:165] Memory required for data: 2471067648
I0507 17:39:08.309844  5077 layer_factory.hpp:77] Creating layer conv2a
I0507 17:39:08.309857  5077 net.cpp:100] Creating Layer conv2a
I0507 17:39:08.309862  5077 net.cpp:434] conv2a <- pool1
I0507 17:39:08.309870  5077 net.cpp:408] conv2a -> conv2a
I0507 17:39:08.319475  5077 net.cpp:150] Setting up conv2a
I0507 17:39:08.319504  5077 net.cpp:157] Top shape: 18 128 16 56 56 (115605504)
I0507 17:39:08.319507  5077 net.cpp:165] Memory required for data: 2933489664
I0507 17:39:08.319517  5077 layer_factory.hpp:77] Creating layer relu2a
I0507 17:39:08.319524  5077 net.cpp:100] Creating Layer relu2a
I0507 17:39:08.319527  5077 net.cpp:434] relu2a <- conv2a
I0507 17:39:08.319532  5077 net.cpp:395] relu2a -> conv2a (in-place)
I0507 17:39:08.320354  5077 net.cpp:150] Setting up relu2a
I0507 17:39:08.320369  5077 net.cpp:157] Top shape: 18 128 16 56 56 (115605504)
I0507 17:39:08.320384  5077 net.cpp:165] Memory required for data: 3395911680
I0507 17:39:08.320387  5077 layer_factory.hpp:77] Creating layer pool2
I0507 17:39:08.320394  5077 net.cpp:100] Creating Layer pool2
I0507 17:39:08.320399  5077 net.cpp:434] pool2 <- conv2a
I0507 17:39:08.320405  5077 net.cpp:408] pool2 -> pool2
I0507 17:39:08.320606  5077 net.cpp:150] Setting up pool2
I0507 17:39:08.320617  5077 net.cpp:157] Top shape: 18 128 8 28 28 (14450688)
I0507 17:39:08.320621  5077 net.cpp:165] Memory required for data: 3453714432
I0507 17:39:08.320624  5077 layer_factory.hpp:77] Creating layer conv3a
I0507 17:39:08.320636  5077 net.cpp:100] Creating Layer conv3a
I0507 17:39:08.320641  5077 net.cpp:434] conv3a <- pool2
I0507 17:39:08.320647  5077 net.cpp:408] conv3a -> conv3a
I0507 17:39:08.359324  5077 net.cpp:150] Setting up conv3a
I0507 17:39:08.359370  5077 net.cpp:157] Top shape: 18 256 8 28 28 (28901376)
I0507 17:39:08.359375  5077 net.cpp:165] Memory required for data: 3569319936
I0507 17:39:08.359391  5077 layer_factory.hpp:77] Creating layer relu3a
I0507 17:39:08.359432  5077 net.cpp:100] Creating Layer relu3a
I0507 17:39:08.359437  5077 net.cpp:434] relu3a <- conv3a
I0507 17:39:08.359443  5077 net.cpp:395] relu3a -> conv3a (in-place)
I0507 17:39:08.360280  5077 net.cpp:150] Setting up relu3a
I0507 17:39:08.360292  5077 net.cpp:157] Top shape: 18 256 8 28 28 (28901376)
I0507 17:39:08.360307  5077 net.cpp:165] Memory required for data: 3684925440
I0507 17:39:08.360311  5077 layer_factory.hpp:77] Creating layer pool3
I0507 17:39:08.360323  5077 net.cpp:100] Creating Layer pool3
I0507 17:39:08.360327  5077 net.cpp:434] pool3 <- conv3a
I0507 17:39:08.360332  5077 net.cpp:408] pool3 -> pool3
I0507 17:39:08.360560  5077 net.cpp:150] Setting up pool3
I0507 17:39:08.360571  5077 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0507 17:39:08.360574  5077 net.cpp:165] Memory required for data: 3699376128
I0507 17:39:08.360594  5077 layer_factory.hpp:77] Creating layer conv4a
I0507 17:39:08.360607  5077 net.cpp:100] Creating Layer conv4a
I0507 17:39:08.360612  5077 net.cpp:434] conv4a <- pool3
I0507 17:39:08.360620  5077 net.cpp:408] conv4a -> conv4a
I0507 17:39:08.427525  5077 net.cpp:150] Setting up conv4a
I0507 17:39:08.427572  5077 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0507 17:39:08.427577  5077 net.cpp:165] Memory required for data: 3713826816
I0507 17:39:08.427587  5077 layer_factory.hpp:77] Creating layer relu4a
I0507 17:39:08.427600  5077 net.cpp:100] Creating Layer relu4a
I0507 17:39:08.427604  5077 net.cpp:434] relu4a <- conv4a
I0507 17:39:08.427611  5077 net.cpp:395] relu4a -> conv4a (in-place)
I0507 17:39:08.427804  5077 net.cpp:150] Setting up relu4a
I0507 17:39:08.427816  5077 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0507 17:39:08.427820  5077 net.cpp:165] Memory required for data: 3728277504
I0507 17:39:08.427824  5077 layer_factory.hpp:77] Creating layer pool4
I0507 17:39:08.427839  5077 net.cpp:100] Creating Layer pool4
I0507 17:39:08.427842  5077 net.cpp:434] pool4 <- conv4a
I0507 17:39:08.427856  5077 net.cpp:408] pool4 -> pool4
I0507 17:39:08.428755  5077 net.cpp:150] Setting up pool4
I0507 17:39:08.428771  5077 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0507 17:39:08.428777  5077 net.cpp:165] Memory required for data: 3730083840
I0507 17:39:08.428795  5077 layer_factory.hpp:77] Creating layer conv5a
I0507 17:39:08.428833  5077 net.cpp:100] Creating Layer conv5a
I0507 17:39:08.428839  5077 net.cpp:434] conv5a <- pool4
I0507 17:39:08.428851  5077 net.cpp:408] conv5a -> conv5a
I0507 17:39:08.496479  5077 net.cpp:150] Setting up conv5a
I0507 17:39:08.496520  5077 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0507 17:39:08.496526  5077 net.cpp:165] Memory required for data: 3731890176
I0507 17:39:08.496549  5077 layer_factory.hpp:77] Creating layer relu5a
I0507 17:39:08.496567  5077 net.cpp:100] Creating Layer relu5a
I0507 17:39:08.496573  5077 net.cpp:434] relu5a <- conv5a
I0507 17:39:08.496582  5077 net.cpp:395] relu5a -> conv5a (in-place)
I0507 17:39:08.496829  5077 net.cpp:150] Setting up relu5a
I0507 17:39:08.496845  5077 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0507 17:39:08.496850  5077 net.cpp:165] Memory required for data: 3733696512
I0507 17:39:08.496855  5077 layer_factory.hpp:77] Creating layer pool5
I0507 17:39:08.496872  5077 net.cpp:100] Creating Layer pool5
I0507 17:39:08.496881  5077 net.cpp:434] pool5 <- conv5a
I0507 17:39:08.496893  5077 net.cpp:408] pool5 -> pool5
I0507 17:39:08.500921  5077 net.cpp:150] Setting up pool5
I0507 17:39:08.500967  5077 net.cpp:157] Top shape: 18 256 1 4 4 (73728)
I0507 17:39:08.500972  5077 net.cpp:165] Memory required for data: 3733991424
I0507 17:39:08.500979  5077 layer_factory.hpp:77] Creating layer fc6
I0507 17:39:08.501041  5077 net.cpp:100] Creating Layer fc6
I0507 17:39:08.501049  5077 net.cpp:434] fc6 <- pool5
I0507 17:39:08.501065  5077 net.cpp:408] fc6 -> fc6
I0507 17:39:08.844204  5077 net.cpp:150] Setting up fc6
I0507 17:39:08.844236  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:08.844243  5077 net.cpp:165] Memory required for data: 3734138880
I0507 17:39:08.844290  5077 layer_factory.hpp:77] Creating layer relu6
I0507 17:39:08.844305  5077 net.cpp:100] Creating Layer relu6
I0507 17:39:08.844311  5077 net.cpp:434] relu6 <- fc6
I0507 17:39:08.844321  5077 net.cpp:395] relu6 -> fc6 (in-place)
I0507 17:39:08.844616  5077 net.cpp:150] Setting up relu6
I0507 17:39:08.844626  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:08.844630  5077 net.cpp:165] Memory required for data: 3734286336
I0507 17:39:08.844633  5077 layer_factory.hpp:77] Creating layer drop6
I0507 17:39:08.844643  5077 net.cpp:100] Creating Layer drop6
I0507 17:39:08.844647  5077 net.cpp:434] drop6 <- fc6
I0507 17:39:08.844655  5077 net.cpp:395] drop6 -> fc6 (in-place)
I0507 17:39:08.844686  5077 net.cpp:150] Setting up drop6
I0507 17:39:08.844692  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:08.844696  5077 net.cpp:165] Memory required for data: 3734433792
I0507 17:39:08.844698  5077 layer_factory.hpp:77] Creating layer fc7
I0507 17:39:08.844709  5077 net.cpp:100] Creating Layer fc7
I0507 17:39:08.844714  5077 net.cpp:434] fc7 <- fc6
I0507 17:39:08.844720  5077 net.cpp:408] fc7 -> fc7
I0507 17:39:08.976377  5077 net.cpp:150] Setting up fc7
I0507 17:39:08.976420  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:08.976428  5077 net.cpp:165] Memory required for data: 3734581248
I0507 17:39:08.976441  5077 layer_factory.hpp:77] Creating layer relu7
I0507 17:39:08.976454  5077 net.cpp:100] Creating Layer relu7
I0507 17:39:08.976459  5077 net.cpp:434] relu7 <- fc7
I0507 17:39:08.976464  5077 net.cpp:395] relu7 -> fc7 (in-place)
I0507 17:39:08.977429  5077 net.cpp:150] Setting up relu7
I0507 17:39:08.977442  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:08.977457  5077 net.cpp:165] Memory required for data: 3734728704
I0507 17:39:08.977460  5077 layer_factory.hpp:77] Creating layer drop7
I0507 17:39:08.977470  5077 net.cpp:100] Creating Layer drop7
I0507 17:39:08.977474  5077 net.cpp:434] drop7 <- fc7
I0507 17:39:08.977479  5077 net.cpp:395] drop7 -> fc7 (in-place)
I0507 17:39:08.977510  5077 net.cpp:150] Setting up drop7
I0507 17:39:08.977516  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:08.977519  5077 net.cpp:165] Memory required for data: 3734876160
I0507 17:39:08.977522  5077 layer_factory.hpp:77] Creating layer conv1a_pos
I0507 17:39:08.977533  5077 net.cpp:100] Creating Layer conv1a_pos
I0507 17:39:08.977548  5077 net.cpp:434] conv1a_pos <- positive
I0507 17:39:08.977560  5077 net.cpp:408] conv1a_pos -> conv1a_pos
I0507 17:39:08.981866  5077 net.cpp:150] Setting up conv1a_pos
I0507 17:39:08.981883  5077 net.cpp:157] Top shape: 18 64 16 112 112 (231211008)
I0507 17:39:08.981897  5077 net.cpp:165] Memory required for data: 4659720192
I0507 17:39:08.981901  5077 net.cpp:493] Sharing parameters 'conv1a_w' owned by layer 'conv1a', param index 0
I0507 17:39:08.981907  5077 net.cpp:493] Sharing parameters 'conv1a_b' owned by layer 'conv1a', param index 1
I0507 17:39:08.981910  5077 layer_factory.hpp:77] Creating layer relu1a_pos
I0507 17:39:08.981917  5077 net.cpp:100] Creating Layer relu1a_pos
I0507 17:39:08.981921  5077 net.cpp:434] relu1a_pos <- conv1a_pos
I0507 17:39:08.981946  5077 net.cpp:395] relu1a_pos -> conv1a_pos (in-place)
I0507 17:39:08.982146  5077 net.cpp:150] Setting up relu1a_pos
I0507 17:39:08.982156  5077 net.cpp:157] Top shape: 18 64 16 112 112 (231211008)
I0507 17:39:08.982161  5077 net.cpp:165] Memory required for data: 5584564224
I0507 17:39:08.982165  5077 layer_factory.hpp:77] Creating layer pool1_pos
I0507 17:39:08.982173  5077 net.cpp:100] Creating Layer pool1_pos
I0507 17:39:08.982177  5077 net.cpp:434] pool1_pos <- conv1a_pos
I0507 17:39:08.982185  5077 net.cpp:408] pool1_pos -> pool1_pos
I0507 17:39:08.984509  5077 net.cpp:150] Setting up pool1_pos
I0507 17:39:08.984557  5077 net.cpp:157] Top shape: 18 64 16 56 56 (57802752)
I0507 17:39:08.984565  5077 net.cpp:165] Memory required for data: 5815775232
I0507 17:39:08.984575  5077 layer_factory.hpp:77] Creating layer conv2a_pos
I0507 17:39:08.984597  5077 net.cpp:100] Creating Layer conv2a_pos
I0507 17:39:08.984639  5077 net.cpp:434] conv2a_pos <- pool1_pos
I0507 17:39:08.984659  5077 net.cpp:408] conv2a_pos -> conv2a_pos
I0507 17:39:08.995846  5077 net.cpp:150] Setting up conv2a_pos
I0507 17:39:08.995863  5077 net.cpp:157] Top shape: 18 128 16 56 56 (115605504)
I0507 17:39:08.995867  5077 net.cpp:165] Memory required for data: 6278197248
I0507 17:39:08.995877  5077 net.cpp:493] Sharing parameters 'conv2a_w' owned by layer 'conv2a', param index 0
I0507 17:39:08.995883  5077 net.cpp:493] Sharing parameters 'conv2a_b' owned by layer 'conv2a', param index 1
I0507 17:39:08.995887  5077 layer_factory.hpp:77] Creating layer relu2a_pos
I0507 17:39:08.995893  5077 net.cpp:100] Creating Layer relu2a_pos
I0507 17:39:08.995898  5077 net.cpp:434] relu2a_pos <- conv2a_pos
I0507 17:39:08.995903  5077 net.cpp:395] relu2a_pos -> conv2a_pos (in-place)
I0507 17:39:08.996089  5077 net.cpp:150] Setting up relu2a_pos
I0507 17:39:08.996099  5077 net.cpp:157] Top shape: 18 128 16 56 56 (115605504)
I0507 17:39:08.996103  5077 net.cpp:165] Memory required for data: 6740619264
I0507 17:39:08.996107  5077 layer_factory.hpp:77] Creating layer pool2_pos
I0507 17:39:08.996114  5077 net.cpp:100] Creating Layer pool2_pos
I0507 17:39:08.996119  5077 net.cpp:434] pool2_pos <- conv2a_pos
I0507 17:39:08.996126  5077 net.cpp:408] pool2_pos -> pool2_pos
I0507 17:39:08.996985  5077 net.cpp:150] Setting up pool2_pos
I0507 17:39:08.996999  5077 net.cpp:157] Top shape: 18 128 8 28 28 (14450688)
I0507 17:39:08.997004  5077 net.cpp:165] Memory required for data: 6798422016
I0507 17:39:08.997009  5077 layer_factory.hpp:77] Creating layer conv3a_pos
I0507 17:39:08.997026  5077 net.cpp:100] Creating Layer conv3a_pos
I0507 17:39:08.997031  5077 net.cpp:434] conv3a_pos <- pool2_pos
I0507 17:39:08.997040  5077 net.cpp:408] conv3a_pos -> conv3a_pos
I0507 17:39:09.026480  5077 net.cpp:150] Setting up conv3a_pos
I0507 17:39:09.026510  5077 net.cpp:157] Top shape: 18 256 8 28 28 (28901376)
I0507 17:39:09.026513  5077 net.cpp:165] Memory required for data: 6914027520
I0507 17:39:09.026520  5077 net.cpp:493] Sharing parameters 'conv3a_w' owned by layer 'conv3a', param index 0
I0507 17:39:09.026525  5077 net.cpp:493] Sharing parameters 'conv3a_b' owned by layer 'conv3a', param index 1
I0507 17:39:09.026530  5077 layer_factory.hpp:77] Creating layer relu3a_pos
I0507 17:39:09.026542  5077 net.cpp:100] Creating Layer relu3a_pos
I0507 17:39:09.026549  5077 net.cpp:434] relu3a_pos <- conv3a_pos
I0507 17:39:09.026556  5077 net.cpp:395] relu3a_pos -> conv3a_pos (in-place)
I0507 17:39:09.027395  5077 net.cpp:150] Setting up relu3a_pos
I0507 17:39:09.027408  5077 net.cpp:157] Top shape: 18 256 8 28 28 (28901376)
I0507 17:39:09.027423  5077 net.cpp:165] Memory required for data: 7029633024
I0507 17:39:09.027426  5077 layer_factory.hpp:77] Creating layer pool3_pos
I0507 17:39:09.027436  5077 net.cpp:100] Creating Layer pool3_pos
I0507 17:39:09.027439  5077 net.cpp:434] pool3_pos <- conv3a_pos
I0507 17:39:09.027446  5077 net.cpp:408] pool3_pos -> pool3_pos
I0507 17:39:09.028295  5077 net.cpp:150] Setting up pool3_pos
I0507 17:39:09.028308  5077 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0507 17:39:09.028311  5077 net.cpp:165] Memory required for data: 7044083712
I0507 17:39:09.028314  5077 layer_factory.hpp:77] Creating layer conv4a_pos
I0507 17:39:09.028327  5077 net.cpp:100] Creating Layer conv4a_pos
I0507 17:39:09.028332  5077 net.cpp:434] conv4a_pos <- pool3_pos
I0507 17:39:09.028340  5077 net.cpp:408] conv4a_pos -> conv4a_pos
I0507 17:39:09.097404  5077 net.cpp:150] Setting up conv4a_pos
I0507 17:39:09.097432  5077 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0507 17:39:09.097439  5077 net.cpp:165] Memory required for data: 7058534400
I0507 17:39:09.097457  5077 net.cpp:493] Sharing parameters 'conv4a_w' owned by layer 'conv4a', param index 0
I0507 17:39:09.097465  5077 net.cpp:493] Sharing parameters 'conv4a_b' owned by layer 'conv4a', param index 1
I0507 17:39:09.097470  5077 layer_factory.hpp:77] Creating layer relu4a_pos
I0507 17:39:09.097509  5077 net.cpp:100] Creating Layer relu4a_pos
I0507 17:39:09.097519  5077 net.cpp:434] relu4a_pos <- conv4a_pos
I0507 17:39:09.097532  5077 net.cpp:395] relu4a_pos -> conv4a_pos (in-place)
I0507 17:39:09.098392  5077 net.cpp:150] Setting up relu4a_pos
I0507 17:39:09.098408  5077 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0507 17:39:09.098413  5077 net.cpp:165] Memory required for data: 7072985088
I0507 17:39:09.098430  5077 layer_factory.hpp:77] Creating layer pool4_pos
I0507 17:39:09.098448  5077 net.cpp:100] Creating Layer pool4_pos
I0507 17:39:09.098453  5077 net.cpp:434] pool4_pos <- conv4a_pos
I0507 17:39:09.098462  5077 net.cpp:408] pool4_pos -> pool4_pos
I0507 17:39:09.098736  5077 net.cpp:150] Setting up pool4_pos
I0507 17:39:09.098749  5077 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0507 17:39:09.098755  5077 net.cpp:165] Memory required for data: 7074791424
I0507 17:39:09.098760  5077 layer_factory.hpp:77] Creating layer conv5a_pos
I0507 17:39:09.098778  5077 net.cpp:100] Creating Layer conv5a_pos
I0507 17:39:09.098783  5077 net.cpp:434] conv5a_pos <- pool4_pos
I0507 17:39:09.098795  5077 net.cpp:408] conv5a_pos -> conv5a_pos
I0507 17:39:09.162782  5077 net.cpp:150] Setting up conv5a_pos
I0507 17:39:09.162827  5077 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0507 17:39:09.162832  5077 net.cpp:165] Memory required for data: 7076597760
I0507 17:39:09.162843  5077 net.cpp:493] Sharing parameters 'conv5a_w' owned by layer 'conv5a', param index 0
I0507 17:39:09.162849  5077 net.cpp:493] Sharing parameters 'conv5a_b' owned by layer 'conv5a', param index 1
I0507 17:39:09.162853  5077 layer_factory.hpp:77] Creating layer relu5a_pos
I0507 17:39:09.162863  5077 net.cpp:100] Creating Layer relu5a_pos
I0507 17:39:09.162868  5077 net.cpp:434] relu5a_pos <- conv5a_pos
I0507 17:39:09.162876  5077 net.cpp:395] relu5a_pos -> conv5a_pos (in-place)
I0507 17:39:09.168107  5077 net.cpp:150] Setting up relu5a_pos
I0507 17:39:09.168143  5077 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0507 17:39:09.168148  5077 net.cpp:165] Memory required for data: 7078404096
I0507 17:39:09.168154  5077 layer_factory.hpp:77] Creating layer pool5_pos
I0507 17:39:09.168167  5077 net.cpp:100] Creating Layer pool5_pos
I0507 17:39:09.168172  5077 net.cpp:434] pool5_pos <- conv5a_pos
I0507 17:39:09.168179  5077 net.cpp:408] pool5_pos -> pool5_pos
I0507 17:39:09.168440  5077 net.cpp:150] Setting up pool5_pos
I0507 17:39:09.168453  5077 net.cpp:157] Top shape: 18 256 1 4 4 (73728)
I0507 17:39:09.168457  5077 net.cpp:165] Memory required for data: 7078699008
I0507 17:39:09.168459  5077 layer_factory.hpp:77] Creating layer fc6_pos
I0507 17:39:09.168471  5077 net.cpp:100] Creating Layer fc6_pos
I0507 17:39:09.168476  5077 net.cpp:434] fc6_pos <- pool5_pos
I0507 17:39:09.168483  5077 net.cpp:408] fc6_pos -> fc6_pos
I0507 17:39:09.447149  5077 net.cpp:150] Setting up fc6_pos
I0507 17:39:09.447182  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:09.447186  5077 net.cpp:165] Memory required for data: 7078846464
I0507 17:39:09.447196  5077 net.cpp:493] Sharing parameters 'fc6_w' owned by layer 'fc6', param index 0
I0507 17:39:09.447202  5077 net.cpp:493] Sharing parameters 'fc6_b' owned by layer 'fc6', param index 1
I0507 17:39:09.447207  5077 layer_factory.hpp:77] Creating layer relu6_pos
I0507 17:39:09.447222  5077 net.cpp:100] Creating Layer relu6_pos
I0507 17:39:09.447228  5077 net.cpp:434] relu6_pos <- fc6_pos
I0507 17:39:09.447234  5077 net.cpp:395] relu6_pos -> fc6_pos (in-place)
I0507 17:39:09.448709  5077 net.cpp:150] Setting up relu6_pos
I0507 17:39:09.448724  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:09.448726  5077 net.cpp:165] Memory required for data: 7078993920
I0507 17:39:09.448730  5077 layer_factory.hpp:77] Creating layer drop6_pos
I0507 17:39:09.448740  5077 net.cpp:100] Creating Layer drop6_pos
I0507 17:39:09.448743  5077 net.cpp:434] drop6_pos <- fc6_pos
I0507 17:39:09.448747  5077 net.cpp:395] drop6_pos -> fc6_pos (in-place)
I0507 17:39:09.448784  5077 net.cpp:150] Setting up drop6_pos
I0507 17:39:09.448812  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:09.448817  5077 net.cpp:165] Memory required for data: 7079141376
I0507 17:39:09.448819  5077 layer_factory.hpp:77] Creating layer fc7_pos
I0507 17:39:09.448830  5077 net.cpp:100] Creating Layer fc7_pos
I0507 17:39:09.448835  5077 net.cpp:434] fc7_pos <- fc6_pos
I0507 17:39:09.448843  5077 net.cpp:408] fc7_pos -> fc7_pos
I0507 17:39:09.583747  5077 net.cpp:150] Setting up fc7_pos
I0507 17:39:09.583788  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:09.583792  5077 net.cpp:165] Memory required for data: 7079288832
I0507 17:39:09.583799  5077 net.cpp:493] Sharing parameters 'fc7_w' owned by layer 'fc7', param index 0
I0507 17:39:09.583806  5077 net.cpp:493] Sharing parameters 'fc7_b' owned by layer 'fc7', param index 1
I0507 17:39:09.583809  5077 layer_factory.hpp:77] Creating layer relu7_pos
I0507 17:39:09.583832  5077 net.cpp:100] Creating Layer relu7_pos
I0507 17:39:09.583837  5077 net.cpp:434] relu7_pos <- fc7_pos
I0507 17:39:09.583842  5077 net.cpp:395] relu7_pos -> fc7_pos (in-place)
I0507 17:39:09.584107  5077 net.cpp:150] Setting up relu7_pos
I0507 17:39:09.584120  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:09.584123  5077 net.cpp:165] Memory required for data: 7079436288
I0507 17:39:09.584126  5077 layer_factory.hpp:77] Creating layer drop7_pos
I0507 17:39:09.584136  5077 net.cpp:100] Creating Layer drop7_pos
I0507 17:39:09.584141  5077 net.cpp:434] drop7_pos <- fc7_pos
I0507 17:39:09.584144  5077 net.cpp:395] drop7_pos -> fc7_pos (in-place)
I0507 17:39:09.584177  5077 net.cpp:150] Setting up drop7_pos
I0507 17:39:09.584183  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:09.584187  5077 net.cpp:165] Memory required for data: 7079583744
I0507 17:39:09.584189  5077 layer_factory.hpp:77] Creating layer conv1a_neg
I0507 17:39:09.584208  5077 net.cpp:100] Creating Layer conv1a_neg
I0507 17:39:09.584213  5077 net.cpp:434] conv1a_neg <- negative
I0507 17:39:09.584220  5077 net.cpp:408] conv1a_neg -> conv1a_neg
I0507 17:39:09.592252  5077 net.cpp:150] Setting up conv1a_neg
I0507 17:39:09.592270  5077 net.cpp:157] Top shape: 18 64 16 112 112 (231211008)
I0507 17:39:09.592275  5077 net.cpp:165] Memory required for data: 8004427776
I0507 17:39:09.592280  5077 net.cpp:493] Sharing parameters 'conv1a_w' owned by layer 'conv1a', param index 0
I0507 17:39:09.592285  5077 net.cpp:493] Sharing parameters 'conv1a_b' owned by layer 'conv1a', param index 1
I0507 17:39:09.592291  5077 layer_factory.hpp:77] Creating layer relu1a_neg
I0507 17:39:09.592299  5077 net.cpp:100] Creating Layer relu1a_neg
I0507 17:39:09.592304  5077 net.cpp:434] relu1a_neg <- conv1a_neg
I0507 17:39:09.592310  5077 net.cpp:395] relu1a_neg -> conv1a_neg (in-place)
I0507 17:39:09.593154  5077 net.cpp:150] Setting up relu1a_neg
I0507 17:39:09.593168  5077 net.cpp:157] Top shape: 18 64 16 112 112 (231211008)
I0507 17:39:09.593173  5077 net.cpp:165] Memory required for data: 8929271808
I0507 17:39:09.593176  5077 layer_factory.hpp:77] Creating layer pool1_neg
I0507 17:39:09.593183  5077 net.cpp:100] Creating Layer pool1_neg
I0507 17:39:09.593189  5077 net.cpp:434] pool1_neg <- conv1a_neg
I0507 17:39:09.593195  5077 net.cpp:408] pool1_neg -> pool1_neg
I0507 17:39:09.593444  5077 net.cpp:150] Setting up pool1_neg
I0507 17:39:09.593457  5077 net.cpp:157] Top shape: 18 64 16 56 56 (57802752)
I0507 17:39:09.593462  5077 net.cpp:165] Memory required for data: 9160482816
I0507 17:39:09.593466  5077 layer_factory.hpp:77] Creating layer conv2a_neg
I0507 17:39:09.593477  5077 net.cpp:100] Creating Layer conv2a_neg
I0507 17:39:09.593482  5077 net.cpp:434] conv2a_neg <- pool1_neg
I0507 17:39:09.593492  5077 net.cpp:408] conv2a_neg -> conv2a_neg
I0507 17:39:09.612066  5077 net.cpp:150] Setting up conv2a_neg
I0507 17:39:09.612095  5077 net.cpp:157] Top shape: 18 128 16 56 56 (115605504)
I0507 17:39:09.612100  5077 net.cpp:165] Memory required for data: 9622904832
I0507 17:39:09.612110  5077 net.cpp:493] Sharing parameters 'conv2a_w' owned by layer 'conv2a', param index 0
I0507 17:39:09.612138  5077 net.cpp:493] Sharing parameters 'conv2a_b' owned by layer 'conv2a', param index 1
I0507 17:39:09.612143  5077 layer_factory.hpp:77] Creating layer relu2a_neg
I0507 17:39:09.612152  5077 net.cpp:100] Creating Layer relu2a_neg
I0507 17:39:09.612164  5077 net.cpp:434] relu2a_neg <- conv2a_neg
I0507 17:39:09.612171  5077 net.cpp:395] relu2a_neg -> conv2a_neg (in-place)
I0507 17:39:09.613030  5077 net.cpp:150] Setting up relu2a_neg
I0507 17:39:09.613044  5077 net.cpp:157] Top shape: 18 128 16 56 56 (115605504)
I0507 17:39:09.613047  5077 net.cpp:165] Memory required for data: 10085326848
I0507 17:39:09.613051  5077 layer_factory.hpp:77] Creating layer pool2_neg
I0507 17:39:09.613060  5077 net.cpp:100] Creating Layer pool2_neg
I0507 17:39:09.613065  5077 net.cpp:434] pool2_neg <- conv2a_neg
I0507 17:39:09.613073  5077 net.cpp:408] pool2_neg -> pool2_neg
I0507 17:39:09.613315  5077 net.cpp:150] Setting up pool2_neg
I0507 17:39:09.613327  5077 net.cpp:157] Top shape: 18 128 8 28 28 (14450688)
I0507 17:39:09.613330  5077 net.cpp:165] Memory required for data: 10143129600
I0507 17:39:09.613333  5077 layer_factory.hpp:77] Creating layer conv3a_neg
I0507 17:39:09.613344  5077 net.cpp:100] Creating Layer conv3a_neg
I0507 17:39:09.613349  5077 net.cpp:434] conv3a_neg <- pool2_neg
I0507 17:39:09.613363  5077 net.cpp:408] conv3a_neg -> conv3a_neg
I0507 17:39:09.649858  5077 net.cpp:150] Setting up conv3a_neg
I0507 17:39:09.649874  5077 net.cpp:157] Top shape: 18 256 8 28 28 (28901376)
I0507 17:39:09.649878  5077 net.cpp:165] Memory required for data: 10258735104
I0507 17:39:09.649912  5077 net.cpp:493] Sharing parameters 'conv3a_w' owned by layer 'conv3a', param index 0
I0507 17:39:09.649919  5077 net.cpp:493] Sharing parameters 'conv3a_b' owned by layer 'conv3a', param index 1
I0507 17:39:09.649925  5077 layer_factory.hpp:77] Creating layer relu3a_neg
I0507 17:39:09.649932  5077 net.cpp:100] Creating Layer relu3a_neg
I0507 17:39:09.649936  5077 net.cpp:434] relu3a_neg <- conv3a_neg
I0507 17:39:09.649943  5077 net.cpp:395] relu3a_neg -> conv3a_neg (in-place)
I0507 17:39:09.650141  5077 net.cpp:150] Setting up relu3a_neg
I0507 17:39:09.650151  5077 net.cpp:157] Top shape: 18 256 8 28 28 (28901376)
I0507 17:39:09.650154  5077 net.cpp:165] Memory required for data: 10374340608
I0507 17:39:09.650158  5077 layer_factory.hpp:77] Creating layer pool3_neg
I0507 17:39:09.650167  5077 net.cpp:100] Creating Layer pool3_neg
I0507 17:39:09.650174  5077 net.cpp:434] pool3_neg <- conv3a_neg
I0507 17:39:09.650179  5077 net.cpp:408] pool3_neg -> pool3_neg
I0507 17:39:09.651064  5077 net.cpp:150] Setting up pool3_neg
I0507 17:39:09.651080  5077 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0507 17:39:09.651084  5077 net.cpp:165] Memory required for data: 10388791296
I0507 17:39:09.651088  5077 layer_factory.hpp:77] Creating layer conv4a_neg
I0507 17:39:09.651098  5077 net.cpp:100] Creating Layer conv4a_neg
I0507 17:39:09.651103  5077 net.cpp:434] conv4a_neg <- pool3_neg
I0507 17:39:09.651110  5077 net.cpp:408] conv4a_neg -> conv4a_neg
I0507 17:39:09.712525  5077 net.cpp:150] Setting up conv4a_neg
I0507 17:39:09.712546  5077 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0507 17:39:09.712550  5077 net.cpp:165] Memory required for data: 10403241984
I0507 17:39:09.712554  5077 net.cpp:493] Sharing parameters 'conv4a_w' owned by layer 'conv4a', param index 0
I0507 17:39:09.712559  5077 net.cpp:493] Sharing parameters 'conv4a_b' owned by layer 'conv4a', param index 1
I0507 17:39:09.712563  5077 layer_factory.hpp:77] Creating layer relu4a_neg
I0507 17:39:09.712573  5077 net.cpp:100] Creating Layer relu4a_neg
I0507 17:39:09.712576  5077 net.cpp:434] relu4a_neg <- conv4a_neg
I0507 17:39:09.712581  5077 net.cpp:395] relu4a_neg -> conv4a_neg (in-place)
I0507 17:39:09.712774  5077 net.cpp:150] Setting up relu4a_neg
I0507 17:39:09.712787  5077 net.cpp:157] Top shape: 18 256 4 14 14 (3612672)
I0507 17:39:09.712791  5077 net.cpp:165] Memory required for data: 10417692672
I0507 17:39:09.712795  5077 layer_factory.hpp:77] Creating layer pool4_neg
I0507 17:39:09.712821  5077 net.cpp:100] Creating Layer pool4_neg
I0507 17:39:09.712826  5077 net.cpp:434] pool4_neg <- conv4a_neg
I0507 17:39:09.712834  5077 net.cpp:408] pool4_neg -> pool4_neg
I0507 17:39:09.713706  5077 net.cpp:150] Setting up pool4_neg
I0507 17:39:09.713719  5077 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0507 17:39:09.713723  5077 net.cpp:165] Memory required for data: 10419499008
I0507 17:39:09.713726  5077 layer_factory.hpp:77] Creating layer conv5a_neg
I0507 17:39:09.713737  5077 net.cpp:100] Creating Layer conv5a_neg
I0507 17:39:09.713743  5077 net.cpp:434] conv5a_neg <- pool4_neg
I0507 17:39:09.713750  5077 net.cpp:408] conv5a_neg -> conv5a_neg
I0507 17:39:09.778830  5077 net.cpp:150] Setting up conv5a_neg
I0507 17:39:09.778861  5077 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0507 17:39:09.778865  5077 net.cpp:165] Memory required for data: 10421305344
I0507 17:39:09.778872  5077 net.cpp:493] Sharing parameters 'conv5a_w' owned by layer 'conv5a', param index 0
I0507 17:39:09.778878  5077 net.cpp:493] Sharing parameters 'conv5a_b' owned by layer 'conv5a', param index 1
I0507 17:39:09.778882  5077 layer_factory.hpp:77] Creating layer relu5a_neg
I0507 17:39:09.778894  5077 net.cpp:100] Creating Layer relu5a_neg
I0507 17:39:09.778899  5077 net.cpp:434] relu5a_neg <- conv5a_neg
I0507 17:39:09.778909  5077 net.cpp:395] relu5a_neg -> conv5a_neg (in-place)
I0507 17:39:09.779105  5077 net.cpp:150] Setting up relu5a_neg
I0507 17:39:09.779116  5077 net.cpp:157] Top shape: 18 256 2 7 7 (451584)
I0507 17:39:09.779119  5077 net.cpp:165] Memory required for data: 10423111680
I0507 17:39:09.779126  5077 layer_factory.hpp:77] Creating layer pool5_neg
I0507 17:39:09.779142  5077 net.cpp:100] Creating Layer pool5_neg
I0507 17:39:09.779148  5077 net.cpp:434] pool5_neg <- conv5a_neg
I0507 17:39:09.779157  5077 net.cpp:408] pool5_neg -> pool5_neg
I0507 17:39:09.780055  5077 net.cpp:150] Setting up pool5_neg
I0507 17:39:09.780071  5077 net.cpp:157] Top shape: 18 256 1 4 4 (73728)
I0507 17:39:09.780076  5077 net.cpp:165] Memory required for data: 10423406592
I0507 17:39:09.780081  5077 layer_factory.hpp:77] Creating layer fc6_neg
I0507 17:39:09.780097  5077 net.cpp:100] Creating Layer fc6_neg
I0507 17:39:09.780103  5077 net.cpp:434] fc6_neg <- pool5_neg
I0507 17:39:09.780117  5077 net.cpp:408] fc6_neg -> fc6_neg
I0507 17:39:10.054239  5077 net.cpp:150] Setting up fc6_neg
I0507 17:39:10.054275  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:10.054281  5077 net.cpp:165] Memory required for data: 10423554048
I0507 17:39:10.054298  5077 net.cpp:493] Sharing parameters 'fc6_w' owned by layer 'fc6', param index 0
I0507 17:39:10.054308  5077 net.cpp:493] Sharing parameters 'fc6_b' owned by layer 'fc6', param index 1
I0507 17:39:10.054318  5077 layer_factory.hpp:77] Creating layer relu6_neg
I0507 17:39:10.054342  5077 net.cpp:100] Creating Layer relu6_neg
I0507 17:39:10.054350  5077 net.cpp:434] relu6_neg <- fc6_neg
I0507 17:39:10.054361  5077 net.cpp:395] relu6_neg -> fc6_neg (in-place)
I0507 17:39:10.054735  5077 net.cpp:150] Setting up relu6_neg
I0507 17:39:10.054749  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:10.054754  5077 net.cpp:165] Memory required for data: 10423701504
I0507 17:39:10.054759  5077 layer_factory.hpp:77] Creating layer drop6_neg
I0507 17:39:10.054811  5077 net.cpp:100] Creating Layer drop6_neg
I0507 17:39:10.054818  5077 net.cpp:434] drop6_neg <- fc6_neg
I0507 17:39:10.054826  5077 net.cpp:395] drop6_neg -> fc6_neg (in-place)
I0507 17:39:10.054872  5077 net.cpp:150] Setting up drop6_neg
I0507 17:39:10.054883  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:10.054888  5077 net.cpp:165] Memory required for data: 10423848960
I0507 17:39:10.054893  5077 layer_factory.hpp:77] Creating layer fc7_neg
I0507 17:39:10.054919  5077 net.cpp:100] Creating Layer fc7_neg
I0507 17:39:10.054927  5077 net.cpp:434] fc7_neg <- fc6_neg
I0507 17:39:10.054939  5077 net.cpp:408] fc7_neg -> fc7_neg
I0507 17:39:10.197350  5077 net.cpp:150] Setting up fc7_neg
I0507 17:39:10.197381  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:10.197420  5077 net.cpp:165] Memory required for data: 10423996416
I0507 17:39:10.197434  5077 net.cpp:493] Sharing parameters 'fc7_w' owned by layer 'fc7', param index 0
I0507 17:39:10.197444  5077 net.cpp:493] Sharing parameters 'fc7_b' owned by layer 'fc7', param index 1
I0507 17:39:10.197451  5077 layer_factory.hpp:77] Creating layer relu7_neg
I0507 17:39:10.197474  5077 net.cpp:100] Creating Layer relu7_neg
I0507 17:39:10.197482  5077 net.cpp:434] relu7_neg <- fc7_neg
I0507 17:39:10.197495  5077 net.cpp:395] relu7_neg -> fc7_neg (in-place)
I0507 17:39:10.199903  5077 net.cpp:150] Setting up relu7_neg
I0507 17:39:10.199920  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:10.199926  5077 net.cpp:165] Memory required for data: 10424143872
I0507 17:39:10.199932  5077 layer_factory.hpp:77] Creating layer drop7_neg
I0507 17:39:10.199944  5077 net.cpp:100] Creating Layer drop7_neg
I0507 17:39:10.199983  5077 net.cpp:434] drop7_neg <- fc7_neg
I0507 17:39:10.199995  5077 net.cpp:395] drop7_neg -> fc7_neg (in-place)
I0507 17:39:10.200042  5077 net.cpp:150] Setting up drop7_neg
I0507 17:39:10.200054  5077 net.cpp:157] Top shape: 18 2048 (36864)
I0507 17:39:10.200059  5077 net.cpp:165] Memory required for data: 10424291328
I0507 17:39:10.200065  5077 layer_factory.hpp:77] Creating layer loss
I0507 17:39:10.200094  5077 net.cpp:100] Creating Layer loss
I0507 17:39:10.200101  5077 net.cpp:434] loss <- fc7
I0507 17:39:10.200109  5077 net.cpp:434] loss <- fc7_pos
I0507 17:39:10.200117  5077 net.cpp:434] loss <- fc7_neg
I0507 17:39:10.200127  5077 net.cpp:408] loss -> loss
I0507 17:39:10.200212  5077 net.cpp:150] Setting up loss
I0507 17:39:10.200222  5077 net.cpp:157] Top shape: (1)
I0507 17:39:10.200227  5077 net.cpp:160]     with loss weight 1
I0507 17:39:10.200265  5077 net.cpp:165] Memory required for data: 10424291332
I0507 17:39:10.200271  5077 net.cpp:226] loss needs backward computation.
I0507 17:39:10.200280  5077 net.cpp:226] drop7_neg needs backward computation.
I0507 17:39:10.200285  5077 net.cpp:226] relu7_neg needs backward computation.
I0507 17:39:10.200291  5077 net.cpp:226] fc7_neg needs backward computation.
I0507 17:39:10.200297  5077 net.cpp:226] drop6_neg needs backward computation.
I0507 17:39:10.200304  5077 net.cpp:226] relu6_neg needs backward computation.
I0507 17:39:10.200309  5077 net.cpp:226] fc6_neg needs backward computation.
I0507 17:39:10.200315  5077 net.cpp:226] pool5_neg needs backward computation.
I0507 17:39:10.200322  5077 net.cpp:226] relu5a_neg needs backward computation.
I0507 17:39:10.200328  5077 net.cpp:226] conv5a_neg needs backward computation.
I0507 17:39:10.200335  5077 net.cpp:226] pool4_neg needs backward computation.
I0507 17:39:10.200342  5077 net.cpp:226] relu4a_neg needs backward computation.
I0507 17:39:10.200348  5077 net.cpp:226] conv4a_neg needs backward computation.
I0507 17:39:10.200356  5077 net.cpp:226] pool3_neg needs backward computation.
I0507 17:39:10.200361  5077 net.cpp:226] relu3a_neg needs backward computation.
I0507 17:39:10.200367  5077 net.cpp:226] conv3a_neg needs backward computation.
I0507 17:39:10.200373  5077 net.cpp:226] pool2_neg needs backward computation.
I0507 17:39:10.200381  5077 net.cpp:226] relu2a_neg needs backward computation.
I0507 17:39:10.200386  5077 net.cpp:226] conv2a_neg needs backward computation.
I0507 17:39:10.200392  5077 net.cpp:226] pool1_neg needs backward computation.
I0507 17:39:10.200397  5077 net.cpp:226] relu1a_neg needs backward computation.
I0507 17:39:10.200403  5077 net.cpp:226] conv1a_neg needs backward computation.
I0507 17:39:10.200412  5077 net.cpp:226] drop7_pos needs backward computation.
I0507 17:39:10.200419  5077 net.cpp:226] relu7_pos needs backward computation.
I0507 17:39:10.200425  5077 net.cpp:226] fc7_pos needs backward computation.
I0507 17:39:10.200431  5077 net.cpp:226] drop6_pos needs backward computation.
I0507 17:39:10.200438  5077 net.cpp:226] relu6_pos needs backward computation.
I0507 17:39:10.200443  5077 net.cpp:226] fc6_pos needs backward computation.
I0507 17:39:10.200465  5077 net.cpp:226] pool5_pos needs backward computation.
I0507 17:39:10.200474  5077 net.cpp:226] relu5a_pos needs backward computation.
I0507 17:39:10.200480  5077 net.cpp:226] conv5a_pos needs backward computation.
I0507 17:39:10.200487  5077 net.cpp:226] pool4_pos needs backward computation.
I0507 17:39:10.200495  5077 net.cpp:226] relu4a_pos needs backward computation.
I0507 17:39:10.200500  5077 net.cpp:226] conv4a_pos needs backward computation.
I0507 17:39:10.200508  5077 net.cpp:226] pool3_pos needs backward computation.
I0507 17:39:10.200515  5077 net.cpp:226] relu3a_pos needs backward computation.
I0507 17:39:10.200522  5077 net.cpp:226] conv3a_pos needs backward computation.
I0507 17:39:10.200529  5077 net.cpp:226] pool2_pos needs backward computation.
I0507 17:39:10.200536  5077 net.cpp:226] relu2a_pos needs backward computation.
I0507 17:39:10.200541  5077 net.cpp:226] conv2a_pos needs backward computation.
I0507 17:39:10.200547  5077 net.cpp:226] pool1_pos needs backward computation.
I0507 17:39:10.200552  5077 net.cpp:226] relu1a_pos needs backward computation.
I0507 17:39:10.200559  5077 net.cpp:226] conv1a_pos needs backward computation.
I0507 17:39:10.200567  5077 net.cpp:226] drop7 needs backward computation.
I0507 17:39:10.200573  5077 net.cpp:226] relu7 needs backward computation.
I0507 17:39:10.200580  5077 net.cpp:226] fc7 needs backward computation.
I0507 17:39:10.200585  5077 net.cpp:226] drop6 needs backward computation.
I0507 17:39:10.200593  5077 net.cpp:226] relu6 needs backward computation.
I0507 17:39:10.200601  5077 net.cpp:226] fc6 needs backward computation.
I0507 17:39:10.200606  5077 net.cpp:226] pool5 needs backward computation.
I0507 17:39:10.200613  5077 net.cpp:226] relu5a needs backward computation.
I0507 17:39:10.200618  5077 net.cpp:226] conv5a needs backward computation.
I0507 17:39:10.200625  5077 net.cpp:226] pool4 needs backward computation.
I0507 17:39:10.200630  5077 net.cpp:226] relu4a needs backward computation.
I0507 17:39:10.200637  5077 net.cpp:226] conv4a needs backward computation.
I0507 17:39:10.200644  5077 net.cpp:226] pool3 needs backward computation.
I0507 17:39:10.200650  5077 net.cpp:226] relu3a needs backward computation.
I0507 17:39:10.200655  5077 net.cpp:226] conv3a needs backward computation.
I0507 17:39:10.200664  5077 net.cpp:226] pool2 needs backward computation.
I0507 17:39:10.200670  5077 net.cpp:226] relu2a needs backward computation.
I0507 17:39:10.200675  5077 net.cpp:226] conv2a needs backward computation.
I0507 17:39:10.200682  5077 net.cpp:226] pool1 needs backward computation.
I0507 17:39:10.200705  5077 net.cpp:226] relu1a needs backward computation.
I0507 17:39:10.200711  5077 net.cpp:226] conv1a needs backward computation.
I0507 17:39:10.200717  5077 net.cpp:228] reshape_negative does not need backward computation.
I0507 17:39:10.200736  5077 net.cpp:228] reshape_positive does not need backward computation.
I0507 17:39:10.200742  5077 net.cpp:228] reshape_anchor does not need backward computation.
I0507 17:39:10.200748  5077 net.cpp:228] slicer does not need backward computation.
I0507 17:39:10.200764  5077 net.cpp:228] data does not need backward computation.
I0507 17:39:10.200769  5077 net.cpp:270] This network produces output loss
I0507 17:39:10.237825  5077 net.cpp:283] Network initialization done.
I0507 17:39:10.241665  5077 solver.cpp:181] Creating test net (#0) specified by net file: ../../models/three_stream_triplet_loss.prototxt
I0507 17:39:10.241907  5077 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0507 17:39:10.243111  5077 net.cpp:58] Initializing net from parameters: 
name: "C3D-Three-Streams"
state {
  phase: TEST
  stage: "test-on-val"
}
layer {
  name: "data"
  type: "Data"
  top: "triplet"
  include {
    phase: TEST
    stage: "test-on-val"
  }
  transform_param {
    mirror: false
    crop_size: 112
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
  }
  data_param {
    source: "/data/leo-data/Synthetic/LMDB/Triplets/val"
    batch_size: 1
    backend: LMDB
  }
}
layer {
  name: "slicer"
  type: "Slice"
  bottom: "triplet"
  top: "anchor_stacked"
  top: "positive_stacked"
  top: "negative_stacked"
  slice_param {
    slice_dim: 1
    slice_point: 48
    slice_point: 96
  }
}
layer {
  name: "reshape_anchor"
  type: "Reshape"
  bottom: "anchor_stacked"
  top: "anchor"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "reshape_positive"
  type: "Reshape"
  bottom: "positive_stacked"
  top: "positive"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "reshape_negative"
  type: "Reshape"
  bottom: "negative_stacked"
  top: "negative"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "conv1a"
  type: "NdConvolution"
  bottom: "anchor"
  top: "conv1a"
  param {
    name: "conv1a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "pool1"
  type: "NdPooling"
  bottom: "conv1a"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a"
  type: "NdConvolution"
  bottom: "pool1"
  top: "conv2a"
  param {
    name: "conv2a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a"
  type: "ReLU"
  bottom: "conv2a"
  top: "conv2a"
}
layer {
  name: "pool2"
  type: "NdPooling"
  bottom: "conv2a"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a"
  type: "NdConvolution"
  bottom: "pool2"
  top: "conv3a"
  param {
    name: "conv3a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a"
  type: "ReLU"
  bottom: "conv3a"
  top: "conv3a"
}
layer {
  name: "pool3"
  type: "NdPooling"
  bottom: "conv3a"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a"
  type: "NdConvolution"
  bottom: "pool3"
  top: "conv4a"
  param {
    name: "conv4a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a"
  type: "ReLU"
  bottom: "conv4a"
  top: "conv4a"
}
layer {
  name: "pool4"
  type: "NdPooling"
  bottom: "conv4a"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a"
  type: "NdConvolution"
  bottom: "pool4"
  top: "conv5a"
  param {
    name: "conv5a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a"
  type: "ReLU"
  bottom: "conv5a"
  top: "conv5a"
}
layer {
  name: "pool5"
  type: "NdPooling"
  bottom: "conv5a"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv1a_pos"
  type: "NdConvolution"
  bottom: "positive"
  top: "conv1a_pos"
  param {
    name: "conv1a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a_pos"
  type: "ReLU"
  bottom: "conv1a_pos"
  top: "conv1a_pos"
}
layer {
  name: "pool1_pos"
  type: "NdPooling"
  bottom: "conv1a_pos"
  top: "pool1_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a_pos"
  type: "NdConvolution"
  bottom: "pool1_pos"
  top: "conv2a_pos"
  param {
    name: "conv2a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a_pos"
  type: "ReLU"
  bottom: "conv2a_pos"
  top: "conv2a_pos"
}
layer {
  name: "pool2_pos"
  type: "NdPooling"
  bottom: "conv2a_pos"
  top: "pool2_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a_pos"
  type: "NdConvolution"
  bottom: "pool2_pos"
  top: "conv3a_pos"
  param {
    name: "conv3a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a_pos"
  type: "ReLU"
  bottom: "conv3a_pos"
  top: "conv3a_pos"
}
layer {
  name: "pool3_pos"
  type: "NdPooling"
  bottom: "conv3a_pos"
  top: "pool3_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a_pos"
  type: "NdConvolution"
  bottom: "pool3_pos"
  top: "conv4a_pos"
  param {
    name: "conv4a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a_pos"
  type: "ReLU"
  bottom: "conv4a_pos"
  top: "conv4a_pos"
}
layer {
  name: "pool4_pos"
  type: "NdPooling"
  bottom: "conv4a_pos"
  top: "pool4_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a_pos"
  type: "NdConvolution"
  bottom: "pool4_pos"
  top: "conv5a_pos"
  param {
    name: "conv5a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a_pos"
  type: "ReLU"
  bottom: "conv5a_pos"
  top: "conv5a_pos"
}
layer {
  name: "pool5_pos"
  type: "NdPooling"
  bottom: "conv5a_pos"
  top: "pool5_pos"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6_pos"
  type: "InnerProduct"
  bottom: "pool5_pos"
  top: "fc6_pos"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6_pos"
  type: "ReLU"
  bottom: "fc6_pos"
  top: "fc6_pos"
}
layer {
  name: "drop6_pos"
  type: "Dropout"
  bottom: "fc6_pos"
  top: "fc6_pos"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_pos"
  type: "InnerProduct"
  bottom: "fc6_pos"
  top: "fc7_pos"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7_pos"
  type: "ReLU"
  bottom: "fc7_pos"
  top: "fc7_pos"
}
layer {
  name: "drop7_pos"
  type: "Dropout"
  bottom: "fc7_pos"
  top: "fc7_pos"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv1a_neg"
  type: "NdConvolution"
  bottom: "negative"
  top: "conv1a_neg"
  param {
    name: "conv1a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a_neg"
  type: "ReLU"
  bottom: "conv1a_neg"
  top: "conv1a_neg"
}
layer {
  name: "pool1_neg"
  type: "NdPooling"
  bottom: "conv1a_neg"
  top: "pool1_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a_neg"
  type: "NdConvolution"
  bottom: "pool1_neg"
  top: "conv2a_neg"
  param {
    name: "conv2a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a_neg"
  type: "ReLU"
  bottom: "conv2a_neg"
  top: "conv2a_neg"
}
layer {
  name: "pool2_neg"
  type: "NdPooling"
  bottom: "conv2a_neg"
  top: "pool2_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a_neg"
  type: "NdConvolution"
  bottom: "pool2_neg"
  top: "conv3a_neg"
  param {
    name: "conv3a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a_neg"
  type: "ReLU"
  bottom: "conv3a_neg"
  top: "conv3a_neg"
}
layer {
  name: "pool3_neg"
  type: "NdPooling"
  bottom: "conv3a_neg"
  top: "pool3_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a_neg"
  type: "NdConvolution"
  bottom: "pool3_neg"
  top: "conv4a_neg"
  param {
    name: "conv4a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a_neg"
  type: "ReLU"
  bottom: "conv4a_neg"
  top: "conv4a_neg"
}
layer {
  name: "pool4_neg"
  type: "NdPooling"
  bottom: "conv4a_neg"
  top: "pool4_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a_neg"
  type: "NdConvolution"
  bottom: "pool4_neg"
  top: "conv5a_neg"
  param {
    name: "conv5a_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5a_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a_neg"
  type: "ReLU"
  bottom: "conv5a_neg"
  top: "conv5a_neg"
}
layer {
  name: "pool5_neg"
  type: "NdPooling"
  bottom: "conv5a_neg"
  top: "pool5_neg"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6_neg"
  type: "InnerProduct"
  bottom: "pool5_neg"
  top: "fc6_neg"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6_neg"
  type: "ReLU"
  bottom: "fc6_neg"
  top: "fc6_neg"
}
layer {
  name: "drop6_neg"
  type: "Dropout"
  bottom: "fc6_neg"
  top: "fc6_neg"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_neg"
  type: "InnerProduct"
  bottom: "fc6_neg"
  top: "fc7_neg"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7_neg"
  type: "ReLU"
  bottom: "fc7_neg"
  top: "fc7_neg"
}
layer {
  name: "drop7_neg"
  type: "Dropout"
  bottom: "fc7_neg"
  top: "fc7_neg"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "TripletLoss"
  bottom: "fc7"
  bottom: "fc7_pos"
  bottom: "fc7_neg"
  top: "loss"
  threshold_param {
    threshold: 1
  }
}
I0507 17:39:10.243499  5077 layer_factory.hpp:77] Creating layer data
I0507 17:39:10.243607  5077 net.cpp:100] Creating Layer data
I0507 17:39:10.243618  5077 net.cpp:408] data -> triplet
I0507 17:39:10.251036  5104 db_lmdb.cpp:35] Opened lmdb /data/leo-data/Synthetic/LMDB/Triplets/val
I0507 17:39:10.264612  5077 data_layer.cpp:41] output data size: 1,144,112,112
I0507 17:39:10.293303  5077 net.cpp:150] Setting up data
I0507 17:39:10.293395  5077 net.cpp:157] Top shape: 1 144 112 112 (1806336)
I0507 17:39:10.293402  5077 net.cpp:165] Memory required for data: 7225344
I0507 17:39:10.293412  5077 layer_factory.hpp:77] Creating layer slicer
I0507 17:39:10.293434  5077 net.cpp:100] Creating Layer slicer
I0507 17:39:10.293442  5077 net.cpp:434] slicer <- triplet
I0507 17:39:10.293453  5077 net.cpp:408] slicer -> anchor_stacked
I0507 17:39:10.293469  5077 net.cpp:408] slicer -> positive_stacked
I0507 17:39:10.293479  5077 net.cpp:408] slicer -> negative_stacked
I0507 17:39:10.293648  5077 net.cpp:150] Setting up slicer
I0507 17:39:10.293663  5077 net.cpp:157] Top shape: 1 48 112 112 (602112)
I0507 17:39:10.293669  5077 net.cpp:157] Top shape: 1 48 112 112 (602112)
I0507 17:39:10.293675  5077 net.cpp:157] Top shape: 1 48 112 112 (602112)
I0507 17:39:10.293680  5077 net.cpp:165] Memory required for data: 14450688
I0507 17:39:10.293685  5077 layer_factory.hpp:77] Creating layer reshape_anchor
I0507 17:39:10.293700  5077 net.cpp:100] Creating Layer reshape_anchor
I0507 17:39:10.293705  5077 net.cpp:434] reshape_anchor <- anchor_stacked
I0507 17:39:10.293715  5077 net.cpp:408] reshape_anchor -> anchor
I0507 17:39:10.293781  5077 net.cpp:150] Setting up reshape_anchor
I0507 17:39:10.293793  5077 net.cpp:157] Top shape: 1 3 16 112 112 (602112)
I0507 17:39:10.293798  5077 net.cpp:165] Memory required for data: 16859136
I0507 17:39:10.293803  5077 layer_factory.hpp:77] Creating layer reshape_positive
I0507 17:39:10.293812  5077 net.cpp:100] Creating Layer reshape_positive
I0507 17:39:10.293818  5077 net.cpp:434] reshape_positive <- positive_stacked
I0507 17:39:10.293838  5077 net.cpp:408] reshape_positive -> positive
I0507 17:39:10.293923  5077 net.cpp:150] Setting up reshape_positive
I0507 17:39:10.293962  5077 net.cpp:157] Top shape: 1 3 16 112 112 (602112)
I0507 17:39:10.293979  5077 net.cpp:165] Memory required for data: 19267584
I0507 17:39:10.293997  5077 layer_factory.hpp:77] Creating layer reshape_negative
I0507 17:39:10.294018  5077 net.cpp:100] Creating Layer reshape_negative
I0507 17:39:10.294035  5077 net.cpp:434] reshape_negative <- negative_stacked
I0507 17:39:10.294057  5077 net.cpp:408] reshape_negative -> negative
I0507 17:39:10.294124  5077 net.cpp:150] Setting up reshape_negative
I0507 17:39:10.294150  5077 net.cpp:157] Top shape: 1 3 16 112 112 (602112)
I0507 17:39:10.294164  5077 net.cpp:165] Memory required for data: 21676032
I0507 17:39:10.294178  5077 layer_factory.hpp:77] Creating layer conv1a
I0507 17:39:10.294204  5077 net.cpp:100] Creating Layer conv1a
I0507 17:39:10.294219  5077 net.cpp:434] conv1a <- anchor
I0507 17:39:10.294239  5077 net.cpp:408] conv1a -> conv1a
I0507 17:39:10.310477  5077 net.cpp:150] Setting up conv1a
I0507 17:39:10.310520  5077 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0507 17:39:10.310524  5077 net.cpp:165] Memory required for data: 73056256
I0507 17:39:10.310681  5077 layer_factory.hpp:77] Creating layer relu1a
I0507 17:39:10.310696  5077 net.cpp:100] Creating Layer relu1a
I0507 17:39:10.310703  5077 net.cpp:434] relu1a <- conv1a
I0507 17:39:10.310714  5077 net.cpp:395] relu1a -> conv1a (in-place)
I0507 17:39:10.310997  5077 net.cpp:150] Setting up relu1a
I0507 17:39:10.311010  5077 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0507 17:39:10.311014  5077 net.cpp:165] Memory required for data: 124436480
I0507 17:39:10.311017  5077 layer_factory.hpp:77] Creating layer pool1
I0507 17:39:10.311027  5077 net.cpp:100] Creating Layer pool1
I0507 17:39:10.311031  5077 net.cpp:434] pool1 <- conv1a
I0507 17:39:10.311043  5077 net.cpp:408] pool1 -> pool1
I0507 17:39:10.312250  5077 net.cpp:150] Setting up pool1
I0507 17:39:10.312265  5077 net.cpp:157] Top shape: 1 64 16 56 56 (3211264)
I0507 17:39:10.312269  5077 net.cpp:165] Memory required for data: 137281536
I0507 17:39:10.312278  5077 layer_factory.hpp:77] Creating layer conv2a
I0507 17:39:10.312292  5077 net.cpp:100] Creating Layer conv2a
I0507 17:39:10.312299  5077 net.cpp:434] conv2a <- pool1
I0507 17:39:10.312312  5077 net.cpp:408] conv2a -> conv2a
I0507 17:39:10.322307  5077 net.cpp:150] Setting up conv2a
I0507 17:39:10.322330  5077 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0507 17:39:10.322341  5077 net.cpp:165] Memory required for data: 162971648
I0507 17:39:10.322352  5077 layer_factory.hpp:77] Creating layer relu2a
I0507 17:39:10.322365  5077 net.cpp:100] Creating Layer relu2a
I0507 17:39:10.322371  5077 net.cpp:434] relu2a <- conv2a
I0507 17:39:10.322381  5077 net.cpp:395] relu2a -> conv2a (in-place)
I0507 17:39:10.324002  5077 net.cpp:150] Setting up relu2a
I0507 17:39:10.324020  5077 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0507 17:39:10.324033  5077 net.cpp:165] Memory required for data: 188661760
I0507 17:39:10.324035  5077 layer_factory.hpp:77] Creating layer pool2
I0507 17:39:10.324045  5077 net.cpp:100] Creating Layer pool2
I0507 17:39:10.324049  5077 net.cpp:434] pool2 <- conv2a
I0507 17:39:10.324055  5077 net.cpp:408] pool2 -> pool2
I0507 17:39:10.331018  5077 net.cpp:150] Setting up pool2
I0507 17:39:10.331034  5077 net.cpp:157] Top shape: 1 128 8 28 28 (802816)
I0507 17:39:10.331038  5077 net.cpp:165] Memory required for data: 191873024
I0507 17:39:10.331043  5077 layer_factory.hpp:77] Creating layer conv3a
I0507 17:39:10.331054  5077 net.cpp:100] Creating Layer conv3a
I0507 17:39:10.331058  5077 net.cpp:434] conv3a <- pool2
I0507 17:39:10.331076  5077 net.cpp:408] conv3a -> conv3a
I0507 17:39:10.361006  5077 net.cpp:150] Setting up conv3a
I0507 17:39:10.361033  5077 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0507 17:39:10.361039  5077 net.cpp:165] Memory required for data: 198295552
I0507 17:39:10.361054  5077 layer_factory.hpp:77] Creating layer relu3a
I0507 17:39:10.361065  5077 net.cpp:100] Creating Layer relu3a
I0507 17:39:10.361070  5077 net.cpp:434] relu3a <- conv3a
I0507 17:39:10.361083  5077 net.cpp:395] relu3a -> conv3a (in-place)
I0507 17:39:10.362118  5077 net.cpp:150] Setting up relu3a
I0507 17:39:10.362140  5077 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0507 17:39:10.362146  5077 net.cpp:165] Memory required for data: 204718080
I0507 17:39:10.362152  5077 layer_factory.hpp:77] Creating layer pool3
I0507 17:39:10.362166  5077 net.cpp:100] Creating Layer pool3
I0507 17:39:10.362218  5077 net.cpp:434] pool3 <- conv3a
I0507 17:39:10.362243  5077 net.cpp:408] pool3 -> pool3
I0507 17:39:10.362515  5077 net.cpp:150] Setting up pool3
I0507 17:39:10.362529  5077 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0507 17:39:10.362531  5077 net.cpp:165] Memory required for data: 205520896
I0507 17:39:10.362534  5077 layer_factory.hpp:77] Creating layer conv4a
I0507 17:39:10.362547  5077 net.cpp:100] Creating Layer conv4a
I0507 17:39:10.362565  5077 net.cpp:434] conv4a <- pool3
I0507 17:39:10.362571  5077 net.cpp:408] conv4a -> conv4a
I0507 17:39:10.439687  5077 net.cpp:150] Setting up conv4a
I0507 17:39:10.439731  5077 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0507 17:39:10.439776  5077 net.cpp:165] Memory required for data: 206323712
I0507 17:39:10.439813  5077 layer_factory.hpp:77] Creating layer relu4a
I0507 17:39:10.439832  5077 net.cpp:100] Creating Layer relu4a
I0507 17:39:10.439842  5077 net.cpp:434] relu4a <- conv4a
I0507 17:39:10.439852  5077 net.cpp:395] relu4a -> conv4a (in-place)
I0507 17:39:10.441175  5077 net.cpp:150] Setting up relu4a
I0507 17:39:10.441192  5077 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0507 17:39:10.441202  5077 net.cpp:165] Memory required for data: 207126528
I0507 17:39:10.441206  5077 layer_factory.hpp:77] Creating layer pool4
I0507 17:39:10.441218  5077 net.cpp:100] Creating Layer pool4
I0507 17:39:10.441226  5077 net.cpp:434] pool4 <- conv4a
I0507 17:39:10.441251  5077 net.cpp:408] pool4 -> pool4
I0507 17:39:10.441555  5077 net.cpp:150] Setting up pool4
I0507 17:39:10.441570  5077 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0507 17:39:10.441578  5077 net.cpp:165] Memory required for data: 207226880
I0507 17:39:10.441584  5077 layer_factory.hpp:77] Creating layer conv5a
I0507 17:39:10.441601  5077 net.cpp:100] Creating Layer conv5a
I0507 17:39:10.441611  5077 net.cpp:434] conv5a <- pool4
I0507 17:39:10.441620  5077 net.cpp:408] conv5a -> conv5a
I0507 17:39:10.516708  5077 net.cpp:150] Setting up conv5a
I0507 17:39:10.516748  5077 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0507 17:39:10.516753  5077 net.cpp:165] Memory required for data: 207327232
I0507 17:39:10.516772  5077 layer_factory.hpp:77] Creating layer relu5a
I0507 17:39:10.516783  5077 net.cpp:100] Creating Layer relu5a
I0507 17:39:10.516790  5077 net.cpp:434] relu5a <- conv5a
I0507 17:39:10.516805  5077 net.cpp:395] relu5a -> conv5a (in-place)
I0507 17:39:10.517709  5077 net.cpp:150] Setting up relu5a
I0507 17:39:10.517724  5077 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0507 17:39:10.517735  5077 net.cpp:165] Memory required for data: 207427584
I0507 17:39:10.517740  5077 layer_factory.hpp:77] Creating layer pool5
I0507 17:39:10.517746  5077 net.cpp:100] Creating Layer pool5
I0507 17:39:10.517751  5077 net.cpp:434] pool5 <- conv5a
I0507 17:39:10.517763  5077 net.cpp:408] pool5 -> pool5
I0507 17:39:10.518059  5077 net.cpp:150] Setting up pool5
I0507 17:39:10.518074  5077 net.cpp:157] Top shape: 1 256 1 4 4 (4096)
I0507 17:39:10.518081  5077 net.cpp:165] Memory required for data: 207443968
I0507 17:39:10.518090  5077 layer_factory.hpp:77] Creating layer fc6
I0507 17:39:10.518105  5077 net.cpp:100] Creating Layer fc6
I0507 17:39:10.518110  5077 net.cpp:434] fc6 <- pool5
I0507 17:39:10.518121  5077 net.cpp:408] fc6 -> fc6
I0507 17:39:10.803905  5077 net.cpp:150] Setting up fc6
I0507 17:39:10.803946  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:10.803951  5077 net.cpp:165] Memory required for data: 207452160
I0507 17:39:10.803984  5077 layer_factory.hpp:77] Creating layer relu6
I0507 17:39:10.804011  5077 net.cpp:100] Creating Layer relu6
I0507 17:39:10.804028  5077 net.cpp:434] relu6 <- fc6
I0507 17:39:10.804040  5077 net.cpp:395] relu6 -> fc6 (in-place)
I0507 17:39:10.810530  5077 net.cpp:150] Setting up relu6
I0507 17:39:10.810549  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:10.810556  5077 net.cpp:165] Memory required for data: 207460352
I0507 17:39:10.810562  5077 layer_factory.hpp:77] Creating layer drop6
I0507 17:39:10.810583  5077 net.cpp:100] Creating Layer drop6
I0507 17:39:10.810592  5077 net.cpp:434] drop6 <- fc6
I0507 17:39:10.810601  5077 net.cpp:395] drop6 -> fc6 (in-place)
I0507 17:39:10.810652  5077 net.cpp:150] Setting up drop6
I0507 17:39:10.810667  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:10.810672  5077 net.cpp:165] Memory required for data: 207468544
I0507 17:39:10.810678  5077 layer_factory.hpp:77] Creating layer fc7
I0507 17:39:10.810705  5077 net.cpp:100] Creating Layer fc7
I0507 17:39:10.810729  5077 net.cpp:434] fc7 <- fc6
I0507 17:39:10.810739  5077 net.cpp:408] fc7 -> fc7
I0507 17:39:10.955487  5077 net.cpp:150] Setting up fc7
I0507 17:39:10.955520  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:10.955569  5077 net.cpp:165] Memory required for data: 207476736
I0507 17:39:10.955588  5077 layer_factory.hpp:77] Creating layer relu7
I0507 17:39:10.955605  5077 net.cpp:100] Creating Layer relu7
I0507 17:39:10.955612  5077 net.cpp:434] relu7 <- fc7
I0507 17:39:10.955622  5077 net.cpp:395] relu7 -> fc7 (in-place)
I0507 17:39:10.955938  5077 net.cpp:150] Setting up relu7
I0507 17:39:10.955953  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:10.955958  5077 net.cpp:165] Memory required for data: 207484928
I0507 17:39:10.955974  5077 layer_factory.hpp:77] Creating layer drop7
I0507 17:39:10.955987  5077 net.cpp:100] Creating Layer drop7
I0507 17:39:10.955996  5077 net.cpp:434] drop7 <- fc7
I0507 17:39:10.956007  5077 net.cpp:395] drop7 -> fc7 (in-place)
I0507 17:39:10.956054  5077 net.cpp:150] Setting up drop7
I0507 17:39:10.956069  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:10.956075  5077 net.cpp:165] Memory required for data: 207493120
I0507 17:39:10.956080  5077 layer_factory.hpp:77] Creating layer conv1a_pos
I0507 17:39:10.956095  5077 net.cpp:100] Creating Layer conv1a_pos
I0507 17:39:10.956111  5077 net.cpp:434] conv1a_pos <- positive
I0507 17:39:10.956146  5077 net.cpp:408] conv1a_pos -> conv1a_pos
I0507 17:39:10.959535  5077 net.cpp:150] Setting up conv1a_pos
I0507 17:39:10.959553  5077 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0507 17:39:10.959558  5077 net.cpp:165] Memory required for data: 258873344
I0507 17:39:10.959575  5077 net.cpp:493] Sharing parameters 'conv1a_w' owned by layer 'conv1a', param index 0
I0507 17:39:10.959588  5077 net.cpp:493] Sharing parameters 'conv1a_b' owned by layer 'conv1a', param index 1
I0507 17:39:10.959595  5077 layer_factory.hpp:77] Creating layer relu1a_pos
I0507 17:39:10.959605  5077 net.cpp:100] Creating Layer relu1a_pos
I0507 17:39:10.959612  5077 net.cpp:434] relu1a_pos <- conv1a_pos
I0507 17:39:10.959625  5077 net.cpp:395] relu1a_pos -> conv1a_pos (in-place)
I0507 17:39:10.960541  5077 net.cpp:150] Setting up relu1a_pos
I0507 17:39:10.960557  5077 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0507 17:39:10.960563  5077 net.cpp:165] Memory required for data: 310253568
I0507 17:39:10.960569  5077 layer_factory.hpp:77] Creating layer pool1_pos
I0507 17:39:10.960590  5077 net.cpp:100] Creating Layer pool1_pos
I0507 17:39:10.960598  5077 net.cpp:434] pool1_pos <- conv1a_pos
I0507 17:39:10.960609  5077 net.cpp:408] pool1_pos -> pool1_pos
I0507 17:39:10.960894  5077 net.cpp:150] Setting up pool1_pos
I0507 17:39:10.960909  5077 net.cpp:157] Top shape: 1 64 16 56 56 (3211264)
I0507 17:39:10.960914  5077 net.cpp:165] Memory required for data: 323098624
I0507 17:39:10.960927  5077 layer_factory.hpp:77] Creating layer conv2a_pos
I0507 17:39:10.960952  5077 net.cpp:100] Creating Layer conv2a_pos
I0507 17:39:10.960958  5077 net.cpp:434] conv2a_pos <- pool1_pos
I0507 17:39:10.960979  5077 net.cpp:408] conv2a_pos -> conv2a_pos
I0507 17:39:10.971113  5077 net.cpp:150] Setting up conv2a_pos
I0507 17:39:10.971132  5077 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0507 17:39:10.971138  5077 net.cpp:165] Memory required for data: 348788736
I0507 17:39:10.971153  5077 net.cpp:493] Sharing parameters 'conv2a_w' owned by layer 'conv2a', param index 0
I0507 17:39:10.971163  5077 net.cpp:493] Sharing parameters 'conv2a_b' owned by layer 'conv2a', param index 1
I0507 17:39:10.971169  5077 layer_factory.hpp:77] Creating layer relu2a_pos
I0507 17:39:10.971180  5077 net.cpp:100] Creating Layer relu2a_pos
I0507 17:39:10.971189  5077 net.cpp:434] relu2a_pos <- conv2a_pos
I0507 17:39:10.971197  5077 net.cpp:395] relu2a_pos -> conv2a_pos (in-place)
I0507 17:39:10.971472  5077 net.cpp:150] Setting up relu2a_pos
I0507 17:39:10.971487  5077 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0507 17:39:10.971491  5077 net.cpp:165] Memory required for data: 374478848
I0507 17:39:10.971498  5077 layer_factory.hpp:77] Creating layer pool2_pos
I0507 17:39:10.971508  5077 net.cpp:100] Creating Layer pool2_pos
I0507 17:39:10.971515  5077 net.cpp:434] pool2_pos <- conv2a_pos
I0507 17:39:10.971545  5077 net.cpp:408] pool2_pos -> pool2_pos
I0507 17:39:10.972492  5077 net.cpp:150] Setting up pool2_pos
I0507 17:39:10.972508  5077 net.cpp:157] Top shape: 1 128 8 28 28 (802816)
I0507 17:39:10.972514  5077 net.cpp:165] Memory required for data: 377690112
I0507 17:39:10.972522  5077 layer_factory.hpp:77] Creating layer conv3a_pos
I0507 17:39:10.972568  5077 net.cpp:100] Creating Layer conv3a_pos
I0507 17:39:10.972575  5077 net.cpp:434] conv3a_pos <- pool2_pos
I0507 17:39:10.972591  5077 net.cpp:408] conv3a_pos -> conv3a_pos
I0507 17:39:11.004338  5077 net.cpp:150] Setting up conv3a_pos
I0507 17:39:11.004356  5077 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0507 17:39:11.004362  5077 net.cpp:165] Memory required for data: 384112640
I0507 17:39:11.004370  5077 net.cpp:493] Sharing parameters 'conv3a_w' owned by layer 'conv3a', param index 0
I0507 17:39:11.004384  5077 net.cpp:493] Sharing parameters 'conv3a_b' owned by layer 'conv3a', param index 1
I0507 17:39:11.004392  5077 layer_factory.hpp:77] Creating layer relu3a_pos
I0507 17:39:11.004405  5077 net.cpp:100] Creating Layer relu3a_pos
I0507 17:39:11.004411  5077 net.cpp:434] relu3a_pos <- conv3a_pos
I0507 17:39:11.004420  5077 net.cpp:395] relu3a_pos -> conv3a_pos (in-place)
I0507 17:39:11.004638  5077 net.cpp:150] Setting up relu3a_pos
I0507 17:39:11.004652  5077 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0507 17:39:11.004658  5077 net.cpp:165] Memory required for data: 390535168
I0507 17:39:11.004663  5077 layer_factory.hpp:77] Creating layer pool3_pos
I0507 17:39:11.004676  5077 net.cpp:100] Creating Layer pool3_pos
I0507 17:39:11.004684  5077 net.cpp:434] pool3_pos <- conv3a_pos
I0507 17:39:11.004693  5077 net.cpp:408] pool3_pos -> pool3_pos
I0507 17:39:11.008369  5077 net.cpp:150] Setting up pool3_pos
I0507 17:39:11.008388  5077 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0507 17:39:11.008394  5077 net.cpp:165] Memory required for data: 391337984
I0507 17:39:11.008400  5077 layer_factory.hpp:77] Creating layer conv4a_pos
I0507 17:39:11.008419  5077 net.cpp:100] Creating Layer conv4a_pos
I0507 17:39:11.008426  5077 net.cpp:434] conv4a_pos <- pool3_pos
I0507 17:39:11.008437  5077 net.cpp:408] conv4a_pos -> conv4a_pos
I0507 17:39:11.112717  5077 net.cpp:150] Setting up conv4a_pos
I0507 17:39:11.112758  5077 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0507 17:39:11.112766  5077 net.cpp:165] Memory required for data: 392140800
I0507 17:39:11.112782  5077 net.cpp:493] Sharing parameters 'conv4a_w' owned by layer 'conv4a', param index 0
I0507 17:39:11.112793  5077 net.cpp:493] Sharing parameters 'conv4a_b' owned by layer 'conv4a', param index 1
I0507 17:39:11.112810  5077 layer_factory.hpp:77] Creating layer relu4a_pos
I0507 17:39:11.112835  5077 net.cpp:100] Creating Layer relu4a_pos
I0507 17:39:11.112854  5077 net.cpp:434] relu4a_pos <- conv4a_pos
I0507 17:39:11.112879  5077 net.cpp:395] relu4a_pos -> conv4a_pos (in-place)
I0507 17:39:11.113384  5077 net.cpp:150] Setting up relu4a_pos
I0507 17:39:11.113406  5077 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0507 17:39:11.113420  5077 net.cpp:165] Memory required for data: 392943616
I0507 17:39:11.113435  5077 layer_factory.hpp:77] Creating layer pool4_pos
I0507 17:39:11.113466  5077 net.cpp:100] Creating Layer pool4_pos
I0507 17:39:11.113481  5077 net.cpp:434] pool4_pos <- conv4a_pos
I0507 17:39:11.113502  5077 net.cpp:408] pool4_pos -> pool4_pos
I0507 17:39:11.115249  5077 net.cpp:150] Setting up pool4_pos
I0507 17:39:11.115278  5077 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0507 17:39:11.115290  5077 net.cpp:165] Memory required for data: 393043968
I0507 17:39:11.115304  5077 layer_factory.hpp:77] Creating layer conv5a_pos
I0507 17:39:11.115339  5077 net.cpp:100] Creating Layer conv5a_pos
I0507 17:39:11.115355  5077 net.cpp:434] conv5a_pos <- pool4_pos
I0507 17:39:11.115384  5077 net.cpp:408] conv5a_pos -> conv5a_pos
I0507 17:39:11.179553  5077 net.cpp:150] Setting up conv5a_pos
I0507 17:39:11.179581  5077 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0507 17:39:11.179585  5077 net.cpp:165] Memory required for data: 393144320
I0507 17:39:11.179620  5077 net.cpp:493] Sharing parameters 'conv5a_w' owned by layer 'conv5a', param index 0
I0507 17:39:11.179632  5077 net.cpp:493] Sharing parameters 'conv5a_b' owned by layer 'conv5a', param index 1
I0507 17:39:11.179637  5077 layer_factory.hpp:77] Creating layer relu5a_pos
I0507 17:39:11.179651  5077 net.cpp:100] Creating Layer relu5a_pos
I0507 17:39:11.179659  5077 net.cpp:434] relu5a_pos <- conv5a_pos
I0507 17:39:11.179668  5077 net.cpp:395] relu5a_pos -> conv5a_pos (in-place)
I0507 17:39:11.180601  5077 net.cpp:150] Setting up relu5a_pos
I0507 17:39:11.180616  5077 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0507 17:39:11.180620  5077 net.cpp:165] Memory required for data: 393244672
I0507 17:39:11.180625  5077 layer_factory.hpp:77] Creating layer pool5_pos
I0507 17:39:11.180639  5077 net.cpp:100] Creating Layer pool5_pos
I0507 17:39:11.180647  5077 net.cpp:434] pool5_pos <- conv5a_pos
I0507 17:39:11.180660  5077 net.cpp:408] pool5_pos -> pool5_pos
I0507 17:39:11.181633  5077 net.cpp:150] Setting up pool5_pos
I0507 17:39:11.181648  5077 net.cpp:157] Top shape: 1 256 1 4 4 (4096)
I0507 17:39:11.181651  5077 net.cpp:165] Memory required for data: 393261056
I0507 17:39:11.181659  5077 layer_factory.hpp:77] Creating layer fc6_pos
I0507 17:39:11.181673  5077 net.cpp:100] Creating Layer fc6_pos
I0507 17:39:11.181676  5077 net.cpp:434] fc6_pos <- pool5_pos
I0507 17:39:11.181686  5077 net.cpp:408] fc6_pos -> fc6_pos
I0507 17:39:11.482815  5077 net.cpp:150] Setting up fc6_pos
I0507 17:39:11.482849  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:11.482853  5077 net.cpp:165] Memory required for data: 393269248
I0507 17:39:11.482866  5077 net.cpp:493] Sharing parameters 'fc6_w' owned by layer 'fc6', param index 0
I0507 17:39:11.482875  5077 net.cpp:493] Sharing parameters 'fc6_b' owned by layer 'fc6', param index 1
I0507 17:39:11.482882  5077 layer_factory.hpp:77] Creating layer relu6_pos
I0507 17:39:11.482900  5077 net.cpp:100] Creating Layer relu6_pos
I0507 17:39:11.482918  5077 net.cpp:434] relu6_pos <- fc6_pos
I0507 17:39:11.482930  5077 net.cpp:395] relu6_pos -> fc6_pos (in-place)
I0507 17:39:11.483371  5077 net.cpp:150] Setting up relu6_pos
I0507 17:39:11.483384  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:11.483391  5077 net.cpp:165] Memory required for data: 393277440
I0507 17:39:11.483395  5077 layer_factory.hpp:77] Creating layer drop6_pos
I0507 17:39:11.483413  5077 net.cpp:100] Creating Layer drop6_pos
I0507 17:39:11.483419  5077 net.cpp:434] drop6_pos <- fc6_pos
I0507 17:39:11.483433  5077 net.cpp:395] drop6_pos -> fc6_pos (in-place)
I0507 17:39:11.483487  5077 net.cpp:150] Setting up drop6_pos
I0507 17:39:11.483499  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:11.483508  5077 net.cpp:165] Memory required for data: 393285632
I0507 17:39:11.483513  5077 layer_factory.hpp:77] Creating layer fc7_pos
I0507 17:39:11.483538  5077 net.cpp:100] Creating Layer fc7_pos
I0507 17:39:11.483546  5077 net.cpp:434] fc7_pos <- fc6_pos
I0507 17:39:11.483558  5077 net.cpp:408] fc7_pos -> fc7_pos
I0507 17:39:11.646265  5077 net.cpp:150] Setting up fc7_pos
I0507 17:39:11.646309  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:11.646313  5077 net.cpp:165] Memory required for data: 393293824
I0507 17:39:11.646325  5077 net.cpp:493] Sharing parameters 'fc7_w' owned by layer 'fc7', param index 0
I0507 17:39:11.646333  5077 net.cpp:493] Sharing parameters 'fc7_b' owned by layer 'fc7', param index 1
I0507 17:39:11.646338  5077 layer_factory.hpp:77] Creating layer relu7_pos
I0507 17:39:11.646351  5077 net.cpp:100] Creating Layer relu7_pos
I0507 17:39:11.646356  5077 net.cpp:434] relu7_pos <- fc7_pos
I0507 17:39:11.646363  5077 net.cpp:395] relu7_pos -> fc7_pos (in-place)
I0507 17:39:11.650637  5077 net.cpp:150] Setting up relu7_pos
I0507 17:39:11.650655  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:11.650660  5077 net.cpp:165] Memory required for data: 393302016
I0507 17:39:11.650663  5077 layer_factory.hpp:77] Creating layer drop7_pos
I0507 17:39:11.650698  5077 net.cpp:100] Creating Layer drop7_pos
I0507 17:39:11.650713  5077 net.cpp:434] drop7_pos <- fc7_pos
I0507 17:39:11.650720  5077 net.cpp:395] drop7_pos -> fc7_pos (in-place)
I0507 17:39:11.650761  5077 net.cpp:150] Setting up drop7_pos
I0507 17:39:11.650768  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:11.650771  5077 net.cpp:165] Memory required for data: 393310208
I0507 17:39:11.650774  5077 layer_factory.hpp:77] Creating layer conv1a_neg
I0507 17:39:11.650789  5077 net.cpp:100] Creating Layer conv1a_neg
I0507 17:39:11.650795  5077 net.cpp:434] conv1a_neg <- negative
I0507 17:39:11.650806  5077 net.cpp:408] conv1a_neg -> conv1a_neg
I0507 17:39:11.653205  5077 net.cpp:150] Setting up conv1a_neg
I0507 17:39:11.653220  5077 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0507 17:39:11.653223  5077 net.cpp:165] Memory required for data: 444690432
I0507 17:39:11.653230  5077 net.cpp:493] Sharing parameters 'conv1a_w' owned by layer 'conv1a', param index 0
I0507 17:39:11.653235  5077 net.cpp:493] Sharing parameters 'conv1a_b' owned by layer 'conv1a', param index 1
I0507 17:39:11.653239  5077 layer_factory.hpp:77] Creating layer relu1a_neg
I0507 17:39:11.653244  5077 net.cpp:100] Creating Layer relu1a_neg
I0507 17:39:11.653251  5077 net.cpp:434] relu1a_neg <- conv1a_neg
I0507 17:39:11.653256  5077 net.cpp:395] relu1a_neg -> conv1a_neg (in-place)
I0507 17:39:11.654110  5077 net.cpp:150] Setting up relu1a_neg
I0507 17:39:11.654124  5077 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0507 17:39:11.654127  5077 net.cpp:165] Memory required for data: 496070656
I0507 17:39:11.654131  5077 layer_factory.hpp:77] Creating layer pool1_neg
I0507 17:39:11.654139  5077 net.cpp:100] Creating Layer pool1_neg
I0507 17:39:11.654145  5077 net.cpp:434] pool1_neg <- conv1a_neg
I0507 17:39:11.654153  5077 net.cpp:408] pool1_neg -> pool1_neg
I0507 17:39:11.654415  5077 net.cpp:150] Setting up pool1_neg
I0507 17:39:11.654427  5077 net.cpp:157] Top shape: 1 64 16 56 56 (3211264)
I0507 17:39:11.654430  5077 net.cpp:165] Memory required for data: 508915712
I0507 17:39:11.654434  5077 layer_factory.hpp:77] Creating layer conv2a_neg
I0507 17:39:11.654443  5077 net.cpp:100] Creating Layer conv2a_neg
I0507 17:39:11.654448  5077 net.cpp:434] conv2a_neg <- pool1_neg
I0507 17:39:11.654456  5077 net.cpp:408] conv2a_neg -> conv2a_neg
I0507 17:39:11.665732  5077 net.cpp:150] Setting up conv2a_neg
I0507 17:39:11.665750  5077 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0507 17:39:11.665752  5077 net.cpp:165] Memory required for data: 534605824
I0507 17:39:11.665758  5077 net.cpp:493] Sharing parameters 'conv2a_w' owned by layer 'conv2a', param index 0
I0507 17:39:11.665765  5077 net.cpp:493] Sharing parameters 'conv2a_b' owned by layer 'conv2a', param index 1
I0507 17:39:11.665767  5077 layer_factory.hpp:77] Creating layer relu2a_neg
I0507 17:39:11.665774  5077 net.cpp:100] Creating Layer relu2a_neg
I0507 17:39:11.665781  5077 net.cpp:434] relu2a_neg <- conv2a_neg
I0507 17:39:11.665786  5077 net.cpp:395] relu2a_neg -> conv2a_neg (in-place)
I0507 17:39:11.666632  5077 net.cpp:150] Setting up relu2a_neg
I0507 17:39:11.666646  5077 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0507 17:39:11.666651  5077 net.cpp:165] Memory required for data: 560295936
I0507 17:39:11.666653  5077 layer_factory.hpp:77] Creating layer pool2_neg
I0507 17:39:11.666661  5077 net.cpp:100] Creating Layer pool2_neg
I0507 17:39:11.666666  5077 net.cpp:434] pool2_neg <- conv2a_neg
I0507 17:39:11.666671  5077 net.cpp:408] pool2_neg -> pool2_neg
I0507 17:39:11.666941  5077 net.cpp:150] Setting up pool2_neg
I0507 17:39:11.666954  5077 net.cpp:157] Top shape: 1 128 8 28 28 (802816)
I0507 17:39:11.666956  5077 net.cpp:165] Memory required for data: 563507200
I0507 17:39:11.666961  5077 layer_factory.hpp:77] Creating layer conv3a_neg
I0507 17:39:11.666970  5077 net.cpp:100] Creating Layer conv3a_neg
I0507 17:39:11.666980  5077 net.cpp:434] conv3a_neg <- pool2_neg
I0507 17:39:11.666988  5077 net.cpp:408] conv3a_neg -> conv3a_neg
I0507 17:39:11.703368  5077 net.cpp:150] Setting up conv3a_neg
I0507 17:39:11.703408  5077 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0507 17:39:11.703413  5077 net.cpp:165] Memory required for data: 569929728
I0507 17:39:11.703449  5077 net.cpp:493] Sharing parameters 'conv3a_w' owned by layer 'conv3a', param index 0
I0507 17:39:11.703459  5077 net.cpp:493] Sharing parameters 'conv3a_b' owned by layer 'conv3a', param index 1
I0507 17:39:11.703467  5077 layer_factory.hpp:77] Creating layer relu3a_neg
I0507 17:39:11.703485  5077 net.cpp:100] Creating Layer relu3a_neg
I0507 17:39:11.703493  5077 net.cpp:434] relu3a_neg <- conv3a_neg
I0507 17:39:11.703506  5077 net.cpp:395] relu3a_neg -> conv3a_neg (in-place)
I0507 17:39:11.704444  5077 net.cpp:150] Setting up relu3a_neg
I0507 17:39:11.704462  5077 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0507 17:39:11.704468  5077 net.cpp:165] Memory required for data: 576352256
I0507 17:39:11.704473  5077 layer_factory.hpp:77] Creating layer pool3_neg
I0507 17:39:11.704491  5077 net.cpp:100] Creating Layer pool3_neg
I0507 17:39:11.704500  5077 net.cpp:434] pool3_neg <- conv3a_neg
I0507 17:39:11.704514  5077 net.cpp:408] pool3_neg -> pool3_neg
I0507 17:39:11.704808  5077 net.cpp:150] Setting up pool3_neg
I0507 17:39:11.704823  5077 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0507 17:39:11.704828  5077 net.cpp:165] Memory required for data: 577155072
I0507 17:39:11.704834  5077 layer_factory.hpp:77] Creating layer conv4a_neg
I0507 17:39:11.704879  5077 net.cpp:100] Creating Layer conv4a_neg
I0507 17:39:11.704886  5077 net.cpp:434] conv4a_neg <- pool3_neg
I0507 17:39:11.704900  5077 net.cpp:408] conv4a_neg -> conv4a_neg
I0507 17:39:11.765847  5077 net.cpp:150] Setting up conv4a_neg
I0507 17:39:11.765885  5077 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0507 17:39:11.765892  5077 net.cpp:165] Memory required for data: 577957888
I0507 17:39:11.765902  5077 net.cpp:493] Sharing parameters 'conv4a_w' owned by layer 'conv4a', param index 0
I0507 17:39:11.765909  5077 net.cpp:493] Sharing parameters 'conv4a_b' owned by layer 'conv4a', param index 1
I0507 17:39:11.765915  5077 layer_factory.hpp:77] Creating layer relu4a_neg
I0507 17:39:11.765933  5077 net.cpp:100] Creating Layer relu4a_neg
I0507 17:39:11.765939  5077 net.cpp:434] relu4a_neg <- conv4a_neg
I0507 17:39:11.765949  5077 net.cpp:395] relu4a_neg -> conv4a_neg (in-place)
I0507 17:39:11.767246  5077 net.cpp:150] Setting up relu4a_neg
I0507 17:39:11.767269  5077 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0507 17:39:11.767277  5077 net.cpp:165] Memory required for data: 578760704
I0507 17:39:11.767282  5077 layer_factory.hpp:77] Creating layer pool4_neg
I0507 17:39:11.767294  5077 net.cpp:100] Creating Layer pool4_neg
I0507 17:39:11.767300  5077 net.cpp:434] pool4_neg <- conv4a_neg
I0507 17:39:11.767313  5077 net.cpp:408] pool4_neg -> pool4_neg
I0507 17:39:11.767722  5077 net.cpp:150] Setting up pool4_neg
I0507 17:39:11.767737  5077 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0507 17:39:11.767743  5077 net.cpp:165] Memory required for data: 578861056
I0507 17:39:11.767750  5077 layer_factory.hpp:77] Creating layer conv5a_neg
I0507 17:39:11.767769  5077 net.cpp:100] Creating Layer conv5a_neg
I0507 17:39:11.767776  5077 net.cpp:434] conv5a_neg <- pool4_neg
I0507 17:39:11.767789  5077 net.cpp:408] conv5a_neg -> conv5a_neg
I0507 17:39:11.827894  5077 net.cpp:150] Setting up conv5a_neg
I0507 17:39:11.827935  5077 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0507 17:39:11.827941  5077 net.cpp:165] Memory required for data: 578961408
I0507 17:39:11.827963  5077 net.cpp:493] Sharing parameters 'conv5a_w' owned by layer 'conv5a', param index 0
I0507 17:39:11.827981  5077 net.cpp:493] Sharing parameters 'conv5a_b' owned by layer 'conv5a', param index 1
I0507 17:39:11.827988  5077 layer_factory.hpp:77] Creating layer relu5a_neg
I0507 17:39:11.828002  5077 net.cpp:100] Creating Layer relu5a_neg
I0507 17:39:11.828011  5077 net.cpp:434] relu5a_neg <- conv5a_neg
I0507 17:39:11.828022  5077 net.cpp:395] relu5a_neg -> conv5a_neg (in-place)
I0507 17:39:11.828307  5077 net.cpp:150] Setting up relu5a_neg
I0507 17:39:11.828322  5077 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0507 17:39:11.828328  5077 net.cpp:165] Memory required for data: 579061760
I0507 17:39:11.828333  5077 layer_factory.hpp:77] Creating layer pool5_neg
I0507 17:39:11.828348  5077 net.cpp:100] Creating Layer pool5_neg
I0507 17:39:11.828356  5077 net.cpp:434] pool5_neg <- conv5a_neg
I0507 17:39:11.828364  5077 net.cpp:408] pool5_neg -> pool5_neg
I0507 17:39:11.834862  5077 net.cpp:150] Setting up pool5_neg
I0507 17:39:11.834903  5077 net.cpp:157] Top shape: 1 256 1 4 4 (4096)
I0507 17:39:11.834928  5077 net.cpp:165] Memory required for data: 579078144
I0507 17:39:11.834933  5077 layer_factory.hpp:77] Creating layer fc6_neg
I0507 17:39:11.834950  5077 net.cpp:100] Creating Layer fc6_neg
I0507 17:39:11.834981  5077 net.cpp:434] fc6_neg <- pool5_neg
I0507 17:39:11.834991  5077 net.cpp:408] fc6_neg -> fc6_neg
I0507 17:39:12.186733  5077 net.cpp:150] Setting up fc6_neg
I0507 17:39:12.186777  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:12.186781  5077 net.cpp:165] Memory required for data: 579086336
I0507 17:39:12.186794  5077 net.cpp:493] Sharing parameters 'fc6_w' owned by layer 'fc6', param index 0
I0507 17:39:12.186802  5077 net.cpp:493] Sharing parameters 'fc6_b' owned by layer 'fc6', param index 1
I0507 17:39:12.186816  5077 layer_factory.hpp:77] Creating layer relu6_neg
I0507 17:39:12.186830  5077 net.cpp:100] Creating Layer relu6_neg
I0507 17:39:12.186837  5077 net.cpp:434] relu6_neg <- fc6_neg
I0507 17:39:12.186844  5077 net.cpp:395] relu6_neg -> fc6_neg (in-place)
I0507 17:39:12.187419  5077 net.cpp:150] Setting up relu6_neg
I0507 17:39:12.187433  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:12.187438  5077 net.cpp:165] Memory required for data: 579094528
I0507 17:39:12.187446  5077 layer_factory.hpp:77] Creating layer drop6_neg
I0507 17:39:12.187503  5077 net.cpp:100] Creating Layer drop6_neg
I0507 17:39:12.187510  5077 net.cpp:434] drop6_neg <- fc6_neg
I0507 17:39:12.187517  5077 net.cpp:395] drop6_neg -> fc6_neg (in-place)
I0507 17:39:12.187566  5077 net.cpp:150] Setting up drop6_neg
I0507 17:39:12.187573  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:12.187579  5077 net.cpp:165] Memory required for data: 579102720
I0507 17:39:12.187584  5077 layer_factory.hpp:77] Creating layer fc7_neg
I0507 17:39:12.187614  5077 net.cpp:100] Creating Layer fc7_neg
I0507 17:39:12.187619  5077 net.cpp:434] fc7_neg <- fc6_neg
I0507 17:39:12.187630  5077 net.cpp:408] fc7_neg -> fc7_neg
I0507 17:39:12.361894  5077 net.cpp:150] Setting up fc7_neg
I0507 17:39:12.361960  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:12.361968  5077 net.cpp:165] Memory required for data: 579110912
I0507 17:39:12.361985  5077 net.cpp:493] Sharing parameters 'fc7_w' owned by layer 'fc7', param index 0
I0507 17:39:12.361994  5077 net.cpp:493] Sharing parameters 'fc7_b' owned by layer 'fc7', param index 1
I0507 17:39:12.362004  5077 layer_factory.hpp:77] Creating layer relu7_neg
I0507 17:39:12.362025  5077 net.cpp:100] Creating Layer relu7_neg
I0507 17:39:12.362035  5077 net.cpp:434] relu7_neg <- fc7_neg
I0507 17:39:12.362053  5077 net.cpp:395] relu7_neg -> fc7_neg (in-place)
I0507 17:39:12.363437  5077 net.cpp:150] Setting up relu7_neg
I0507 17:39:12.363458  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:12.363483  5077 net.cpp:165] Memory required for data: 579119104
I0507 17:39:12.363494  5077 layer_factory.hpp:77] Creating layer drop7_neg
I0507 17:39:12.363507  5077 net.cpp:100] Creating Layer drop7_neg
I0507 17:39:12.363512  5077 net.cpp:434] drop7_neg <- fc7_neg
I0507 17:39:12.363525  5077 net.cpp:395] drop7_neg -> fc7_neg (in-place)
I0507 17:39:12.363611  5077 net.cpp:150] Setting up drop7_neg
I0507 17:39:12.363627  5077 net.cpp:157] Top shape: 1 2048 (2048)
I0507 17:39:12.363636  5077 net.cpp:165] Memory required for data: 579127296
I0507 17:39:12.363647  5077 layer_factory.hpp:77] Creating layer loss
I0507 17:39:12.363672  5077 net.cpp:100] Creating Layer loss
I0507 17:39:12.363715  5077 net.cpp:434] loss <- fc7
I0507 17:39:12.363731  5077 net.cpp:434] loss <- fc7_pos
I0507 17:39:12.363744  5077 net.cpp:434] loss <- fc7_neg
I0507 17:39:12.363761  5077 net.cpp:408] loss -> loss
I0507 17:39:12.363894  5077 net.cpp:150] Setting up loss
I0507 17:39:12.363904  5077 net.cpp:157] Top shape: (1)
I0507 17:39:12.363910  5077 net.cpp:160]     with loss weight 1
I0507 17:39:12.363940  5077 net.cpp:165] Memory required for data: 579127300
I0507 17:39:12.363946  5077 net.cpp:226] loss needs backward computation.
I0507 17:39:12.363953  5077 net.cpp:226] drop7_neg needs backward computation.
I0507 17:39:12.363960  5077 net.cpp:226] relu7_neg needs backward computation.
I0507 17:39:12.363966  5077 net.cpp:226] fc7_neg needs backward computation.
I0507 17:39:12.363972  5077 net.cpp:226] drop6_neg needs backward computation.
I0507 17:39:12.363981  5077 net.cpp:226] relu6_neg needs backward computation.
I0507 17:39:12.363987  5077 net.cpp:226] fc6_neg needs backward computation.
I0507 17:39:12.364003  5077 net.cpp:226] pool5_neg needs backward computation.
I0507 17:39:12.364011  5077 net.cpp:226] relu5a_neg needs backward computation.
I0507 17:39:12.364020  5077 net.cpp:226] conv5a_neg needs backward computation.
I0507 17:39:12.364033  5077 net.cpp:226] pool4_neg needs backward computation.
I0507 17:39:12.364042  5077 net.cpp:226] relu4a_neg needs backward computation.
I0507 17:39:12.364053  5077 net.cpp:226] conv4a_neg needs backward computation.
I0507 17:39:12.364065  5077 net.cpp:226] pool3_neg needs backward computation.
I0507 17:39:12.364073  5077 net.cpp:226] relu3a_neg needs backward computation.
I0507 17:39:12.364084  5077 net.cpp:226] conv3a_neg needs backward computation.
I0507 17:39:12.364099  5077 net.cpp:226] pool2_neg needs backward computation.
I0507 17:39:12.364109  5077 net.cpp:226] relu2a_neg needs backward computation.
I0507 17:39:12.364118  5077 net.cpp:226] conv2a_neg needs backward computation.
I0507 17:39:12.364126  5077 net.cpp:226] pool1_neg needs backward computation.
I0507 17:39:12.364131  5077 net.cpp:226] relu1a_neg needs backward computation.
I0507 17:39:12.364136  5077 net.cpp:226] conv1a_neg needs backward computation.
I0507 17:39:12.364140  5077 net.cpp:226] drop7_pos needs backward computation.
I0507 17:39:12.364143  5077 net.cpp:226] relu7_pos needs backward computation.
I0507 17:39:12.364151  5077 net.cpp:226] fc7_pos needs backward computation.
I0507 17:39:12.364159  5077 net.cpp:226] drop6_pos needs backward computation.
I0507 17:39:12.364166  5077 net.cpp:226] relu6_pos needs backward computation.
I0507 17:39:12.364171  5077 net.cpp:226] fc6_pos needs backward computation.
I0507 17:39:12.364177  5077 net.cpp:226] pool5_pos needs backward computation.
I0507 17:39:12.364181  5077 net.cpp:226] relu5a_pos needs backward computation.
I0507 17:39:12.364187  5077 net.cpp:226] conv5a_pos needs backward computation.
I0507 17:39:12.364198  5077 net.cpp:226] pool4_pos needs backward computation.
I0507 17:39:12.364208  5077 net.cpp:226] relu4a_pos needs backward computation.
I0507 17:39:12.364214  5077 net.cpp:226] conv4a_pos needs backward computation.
I0507 17:39:12.364220  5077 net.cpp:226] pool3_pos needs backward computation.
I0507 17:39:12.364226  5077 net.cpp:226] relu3a_pos needs backward computation.
I0507 17:39:12.364235  5077 net.cpp:226] conv3a_pos needs backward computation.
I0507 17:39:12.364245  5077 net.cpp:226] pool2_pos needs backward computation.
I0507 17:39:12.364250  5077 net.cpp:226] relu2a_pos needs backward computation.
I0507 17:39:12.364255  5077 net.cpp:226] conv2a_pos needs backward computation.
I0507 17:39:12.364262  5077 net.cpp:226] pool1_pos needs backward computation.
I0507 17:39:12.364270  5077 net.cpp:226] relu1a_pos needs backward computation.
I0507 17:39:12.364274  5077 net.cpp:226] conv1a_pos needs backward computation.
I0507 17:39:12.364284  5077 net.cpp:226] drop7 needs backward computation.
I0507 17:39:12.364290  5077 net.cpp:226] relu7 needs backward computation.
I0507 17:39:12.364293  5077 net.cpp:226] fc7 needs backward computation.
I0507 17:39:12.364310  5077 net.cpp:226] drop6 needs backward computation.
I0507 17:39:12.364313  5077 net.cpp:226] relu6 needs backward computation.
I0507 17:39:12.364320  5077 net.cpp:226] fc6 needs backward computation.
I0507 17:39:12.364327  5077 net.cpp:226] pool5 needs backward computation.
I0507 17:39:12.364333  5077 net.cpp:226] relu5a needs backward computation.
I0507 17:39:12.364336  5077 net.cpp:226] conv5a needs backward computation.
I0507 17:39:12.364339  5077 net.cpp:226] pool4 needs backward computation.
I0507 17:39:12.364342  5077 net.cpp:226] relu4a needs backward computation.
I0507 17:39:12.364346  5077 net.cpp:226] conv4a needs backward computation.
I0507 17:39:12.364348  5077 net.cpp:226] pool3 needs backward computation.
I0507 17:39:12.364352  5077 net.cpp:226] relu3a needs backward computation.
I0507 17:39:12.364356  5077 net.cpp:226] conv3a needs backward computation.
I0507 17:39:12.364363  5077 net.cpp:226] pool2 needs backward computation.
I0507 17:39:12.364372  5077 net.cpp:226] relu2a needs backward computation.
I0507 17:39:12.364375  5077 net.cpp:226] conv2a needs backward computation.
I0507 17:39:12.364379  5077 net.cpp:226] pool1 needs backward computation.
I0507 17:39:12.364382  5077 net.cpp:226] relu1a needs backward computation.
I0507 17:39:12.364385  5077 net.cpp:226] conv1a needs backward computation.
I0507 17:39:12.364388  5077 net.cpp:228] reshape_negative does not need backward computation.
I0507 17:39:12.364392  5077 net.cpp:228] reshape_positive does not need backward computation.
I0507 17:39:12.364397  5077 net.cpp:228] reshape_anchor does not need backward computation.
I0507 17:39:12.364404  5077 net.cpp:228] slicer does not need backward computation.
I0507 17:39:12.364413  5077 net.cpp:228] data does not need backward computation.
I0507 17:39:12.364415  5077 net.cpp:270] This network produces output loss
I0507 17:39:12.401774  5077 net.cpp:283] Network initialization done.
I0507 17:39:12.402297  5077 solver.cpp:60] Solver scaffolding done.
I0507 17:39:12.403383  5077 caffe.cpp:155] Finetuning from ../../../c3d_ucf101_iter_38000.caffemodel
I0507 17:39:13.168479  5077 net.cpp:761] Ignoring source layer fc8
I0507 17:39:13.374428  5077 net.cpp:761] Ignoring source layer fc8
I0507 17:39:13.377951  5077 caffe.cpp:251] Starting Optimization
I0507 17:39:13.377969  5077 solver.cpp:279] Solving C3D-Three-Streams
I0507 17:39:13.377974  5077 solver.cpp:280] Learning Rate Policy: step
I0507 17:39:13.387666  5077 solver.cpp:337] Iteration 0, Testing net (#0)
I0507 17:39:19.045555  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 17:39:41.744232  5077 solver.cpp:404]     Test net output #0: loss = 35.3498 (* 1 = 35.3498 loss)
I0507 17:39:42.701836  5077 solver.cpp:228] Iteration 0, loss = 406.083
I0507 17:39:42.701920  5077 solver.cpp:244]     Train net output #0: loss = 406.083 (* 1 = 406.083 loss)
I0507 17:39:42.701953  5077 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0507 17:41:53.808161  5077 solver.cpp:228] Iteration 50, loss = 0.639278
I0507 17:41:53.808423  5077 solver.cpp:244]     Train net output #0: loss = 0.639277 (* 1 = 0.639277 loss)
I0507 17:41:53.808459  5077 sgd_solver.cpp:106] Iteration 50, lr = 1e-05
I0507 17:44:04.163514  5077 solver.cpp:228] Iteration 100, loss = 0.335307
I0507 17:44:04.167160  5077 solver.cpp:244]     Train net output #0: loss = 0.335307 (* 1 = 0.335307 loss)
I0507 17:44:04.167177  5077 sgd_solver.cpp:106] Iteration 100, lr = 1e-05
I0507 17:46:13.979393  5077 solver.cpp:228] Iteration 150, loss = 0.872717
I0507 17:46:13.979544  5077 solver.cpp:244]     Train net output #0: loss = 0.872717 (* 1 = 0.872717 loss)
I0507 17:46:13.979558  5077 sgd_solver.cpp:106] Iteration 150, lr = 1e-05
I0507 17:48:21.293453  5077 solver.cpp:337] Iteration 200, Testing net (#0)
I0507 17:48:51.490531  5077 solver.cpp:404]     Test net output #0: loss = 0.446729 (* 1 = 0.446729 loss)
I0507 17:48:52.371639  5077 solver.cpp:228] Iteration 200, loss = 0.368789
I0507 17:48:52.371719  5077 solver.cpp:244]     Train net output #0: loss = 0.368789 (* 1 = 0.368789 loss)
I0507 17:48:52.371737  5077 sgd_solver.cpp:106] Iteration 200, lr = 1e-05
I0507 17:51:02.600222  5077 solver.cpp:228] Iteration 250, loss = 0.281179
I0507 17:51:02.600394  5077 solver.cpp:244]     Train net output #0: loss = 0.281179 (* 1 = 0.281179 loss)
I0507 17:51:02.600409  5077 sgd_solver.cpp:106] Iteration 250, lr = 1e-05
I0507 17:53:12.171231  5077 solver.cpp:228] Iteration 300, loss = 0.28332
I0507 17:53:12.171406  5077 solver.cpp:244]     Train net output #0: loss = 0.28332 (* 1 = 0.28332 loss)
I0507 17:53:12.171422  5077 sgd_solver.cpp:106] Iteration 300, lr = 1e-05
I0507 17:55:21.966367  5077 solver.cpp:228] Iteration 350, loss = 0.269844
I0507 17:55:21.966745  5077 solver.cpp:244]     Train net output #0: loss = 0.269843 (* 1 = 0.269843 loss)
I0507 17:55:21.966789  5077 sgd_solver.cpp:106] Iteration 350, lr = 1e-05
I0507 17:57:29.602636  5077 solver.cpp:337] Iteration 400, Testing net (#0)
I0507 17:57:55.453608  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 17:58:00.629182  5077 solver.cpp:404]     Test net output #0: loss = 0.462278 (* 1 = 0.462278 loss)
I0507 17:58:01.497318  5077 solver.cpp:228] Iteration 400, loss = 0.279681
I0507 17:58:01.497382  5077 solver.cpp:244]     Train net output #0: loss = 0.279681 (* 1 = 0.279681 loss)
I0507 17:58:01.497400  5077 sgd_solver.cpp:106] Iteration 400, lr = 1e-05
I0507 18:00:11.442354  5077 solver.cpp:228] Iteration 450, loss = 1.66216
I0507 18:00:11.442510  5077 solver.cpp:244]     Train net output #0: loss = 1.66216 (* 1 = 1.66216 loss)
I0507 18:00:11.442524  5077 sgd_solver.cpp:106] Iteration 450, lr = 1e-05
I0507 18:02:21.698518  5077 solver.cpp:228] Iteration 500, loss = 0.364358
I0507 18:02:21.698761  5077 solver.cpp:244]     Train net output #0: loss = 0.364358 (* 1 = 0.364358 loss)
I0507 18:02:21.698793  5077 sgd_solver.cpp:106] Iteration 500, lr = 1e-05
I0507 18:04:31.357305  5077 solver.cpp:228] Iteration 550, loss = 0.199774
I0507 18:04:31.357460  5077 solver.cpp:244]     Train net output #0: loss = 0.199774 (* 1 = 0.199774 loss)
I0507 18:04:31.357475  5077 sgd_solver.cpp:106] Iteration 550, lr = 1e-05
I0507 18:06:38.573485  5077 solver.cpp:337] Iteration 600, Testing net (#0)
I0507 18:07:09.779026  5077 solver.cpp:404]     Test net output #0: loss = 0.413015 (* 1 = 0.413015 loss)
I0507 18:07:10.647963  5077 solver.cpp:228] Iteration 600, loss = 0.210796
I0507 18:07:10.648044  5077 solver.cpp:244]     Train net output #0: loss = 0.210795 (* 1 = 0.210795 loss)
I0507 18:07:10.648058  5077 sgd_solver.cpp:106] Iteration 600, lr = 1e-05
I0507 18:09:20.767237  5077 solver.cpp:228] Iteration 650, loss = 0.165282
I0507 18:09:20.767448  5077 solver.cpp:244]     Train net output #0: loss = 0.165282 (* 1 = 0.165282 loss)
I0507 18:09:20.767472  5077 sgd_solver.cpp:106] Iteration 650, lr = 1e-05
I0507 18:11:30.703017  5077 solver.cpp:228] Iteration 700, loss = 0.40724
I0507 18:11:30.715024  5077 solver.cpp:244]     Train net output #0: loss = 0.407239 (* 1 = 0.407239 loss)
I0507 18:11:30.715060  5077 sgd_solver.cpp:106] Iteration 700, lr = 1e-05
I0507 18:13:40.806728  5077 solver.cpp:228] Iteration 750, loss = 0.386159
I0507 18:13:40.808691  5077 solver.cpp:244]     Train net output #0: loss = 0.386159 (* 1 = 0.386159 loss)
I0507 18:13:40.808714  5077 sgd_solver.cpp:106] Iteration 750, lr = 1e-05
I0507 18:15:48.233393  5077 solver.cpp:337] Iteration 800, Testing net (#0)
I0507 18:16:19.104667  5077 solver.cpp:404]     Test net output #0: loss = 0.391747 (* 1 = 0.391747 loss)
I0507 18:16:19.972594  5077 solver.cpp:228] Iteration 800, loss = 0.272175
I0507 18:16:19.972667  5077 solver.cpp:244]     Train net output #0: loss = 0.272175 (* 1 = 0.272175 loss)
I0507 18:16:19.972681  5077 sgd_solver.cpp:106] Iteration 800, lr = 1e-05
I0507 18:18:29.948807  5077 solver.cpp:228] Iteration 850, loss = 0.347538
I0507 18:18:29.949031  5077 solver.cpp:244]     Train net output #0: loss = 0.347538 (* 1 = 0.347538 loss)
I0507 18:18:29.949064  5077 sgd_solver.cpp:106] Iteration 850, lr = 1e-05
I0507 18:20:39.948968  5077 solver.cpp:228] Iteration 900, loss = 0.393669
I0507 18:20:41.886793  5077 solver.cpp:244]     Train net output #0: loss = 0.393669 (* 1 = 0.393669 loss)
I0507 18:20:41.886831  5077 sgd_solver.cpp:106] Iteration 900, lr = 1e-05
I0507 18:22:50.078619  5077 solver.cpp:228] Iteration 950, loss = 0.251774
I0507 18:22:50.078824  5077 solver.cpp:244]     Train net output #0: loss = 0.251774 (* 1 = 0.251774 loss)
I0507 18:22:50.078840  5077 sgd_solver.cpp:106] Iteration 950, lr = 1e-05
I0507 18:24:57.736090  5077 solver.cpp:337] Iteration 1000, Testing net (#0)
I0507 18:25:16.316655  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 18:25:41.648037  5077 solver.cpp:404]     Test net output #0: loss = 0.387572 (* 1 = 0.387572 loss)
I0507 18:25:42.514693  5077 solver.cpp:228] Iteration 1000, loss = 0.0594745
I0507 18:25:42.514766  5077 solver.cpp:244]     Train net output #0: loss = 0.0594745 (* 1 = 0.0594745 loss)
I0507 18:25:42.514780  5077 sgd_solver.cpp:106] Iteration 1000, lr = 1e-05
I0507 18:27:53.287962  5077 solver.cpp:228] Iteration 1050, loss = 0.144443
I0507 18:27:53.288661  5077 solver.cpp:244]     Train net output #0: loss = 0.144444 (* 1 = 0.144444 loss)
I0507 18:27:53.288693  5077 sgd_solver.cpp:106] Iteration 1050, lr = 1e-05
I0507 18:30:03.436157  5077 solver.cpp:228] Iteration 1100, loss = 0.242628
I0507 18:30:03.436336  5077 solver.cpp:244]     Train net output #0: loss = 0.242628 (* 1 = 0.242628 loss)
I0507 18:30:03.436362  5077 sgd_solver.cpp:106] Iteration 1100, lr = 1e-05
I0507 18:32:13.522238  5077 solver.cpp:228] Iteration 1150, loss = 0.202354
I0507 18:32:13.522451  5077 solver.cpp:244]     Train net output #0: loss = 0.202354 (* 1 = 0.202354 loss)
I0507 18:32:13.522467  5077 sgd_solver.cpp:106] Iteration 1150, lr = 1e-05
I0507 18:34:20.742738  5077 solver.cpp:337] Iteration 1200, Testing net (#0)
I0507 18:35:15.304451  5077 solver.cpp:404]     Test net output #0: loss = 0.347402 (* 1 = 0.347402 loss)
I0507 18:35:16.163758  5077 solver.cpp:228] Iteration 1200, loss = 0.199876
I0507 18:35:16.163827  5077 solver.cpp:244]     Train net output #0: loss = 0.199876 (* 1 = 0.199876 loss)
I0507 18:35:16.163846  5077 sgd_solver.cpp:106] Iteration 1200, lr = 1e-05
I0507 18:37:27.195467  5077 solver.cpp:228] Iteration 1250, loss = 0.166482
I0507 18:37:27.195686  5077 solver.cpp:244]     Train net output #0: loss = 0.166482 (* 1 = 0.166482 loss)
I0507 18:37:27.195734  5077 sgd_solver.cpp:106] Iteration 1250, lr = 1e-05
I0507 18:39:37.490907  5077 solver.cpp:228] Iteration 1300, loss = 0.122881
I0507 18:39:37.492539  5077 solver.cpp:244]     Train net output #0: loss = 0.122881 (* 1 = 0.122881 loss)
I0507 18:39:37.492558  5077 sgd_solver.cpp:106] Iteration 1300, lr = 1e-05
I0507 18:41:47.523367  5077 solver.cpp:228] Iteration 1350, loss = 0.12306
I0507 18:41:47.523562  5077 solver.cpp:244]     Train net output #0: loss = 0.12306 (* 1 = 0.12306 loss)
I0507 18:41:47.523581  5077 sgd_solver.cpp:106] Iteration 1350, lr = 1e-05
I0507 18:43:55.220695  5077 solver.cpp:337] Iteration 1400, Testing net (#0)
I0507 18:44:25.443589  5077 solver.cpp:404]     Test net output #0: loss = 0.368955 (* 1 = 0.368955 loss)
I0507 18:44:26.313382  5077 solver.cpp:228] Iteration 1400, loss = 0.599954
I0507 18:44:26.313474  5077 solver.cpp:244]     Train net output #0: loss = 0.599954 (* 1 = 0.599954 loss)
I0507 18:44:26.313496  5077 sgd_solver.cpp:106] Iteration 1400, lr = 1e-05
I0507 18:46:36.179427  5077 solver.cpp:228] Iteration 1450, loss = 0.183605
I0507 18:46:36.179651  5077 solver.cpp:244]     Train net output #0: loss = 0.183605 (* 1 = 0.183605 loss)
I0507 18:46:36.179677  5077 sgd_solver.cpp:106] Iteration 1450, lr = 1e-05
I0507 18:48:46.173363  5077 solver.cpp:228] Iteration 1500, loss = 0.165242
I0507 18:48:46.173521  5077 solver.cpp:244]     Train net output #0: loss = 0.165242 (* 1 = 0.165242 loss)
I0507 18:48:46.173540  5077 sgd_solver.cpp:106] Iteration 1500, lr = 1e-05
I0507 18:50:56.179014  5077 solver.cpp:228] Iteration 1550, loss = 0.279096
I0507 18:50:56.191700  5077 solver.cpp:244]     Train net output #0: loss = 0.279096 (* 1 = 0.279096 loss)
I0507 18:50:56.191730  5077 sgd_solver.cpp:106] Iteration 1550, lr = 1e-05
I0507 18:53:03.508044  5077 solver.cpp:337] Iteration 1600, Testing net (#0)
I0507 18:53:24.118520  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 18:53:33.938993  5077 solver.cpp:404]     Test net output #0: loss = 0.300894 (* 1 = 0.300894 loss)
I0507 18:53:34.807680  5077 solver.cpp:228] Iteration 1600, loss = 0.144768
I0507 18:53:34.807766  5077 solver.cpp:244]     Train net output #0: loss = 0.144768 (* 1 = 0.144768 loss)
I0507 18:53:34.807783  5077 sgd_solver.cpp:106] Iteration 1600, lr = 1e-05
I0507 18:55:45.139240  5077 solver.cpp:228] Iteration 1650, loss = 0
I0507 18:55:45.139412  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0507 18:55:45.139428  5077 sgd_solver.cpp:106] Iteration 1650, lr = 1e-05
I0507 18:57:55.037137  5077 solver.cpp:228] Iteration 1700, loss = 0.0071681
I0507 18:57:55.037312  5077 solver.cpp:244]     Train net output #0: loss = 0.00716804 (* 1 = 0.00716804 loss)
I0507 18:57:55.037331  5077 sgd_solver.cpp:106] Iteration 1700, lr = 1e-05
I0507 19:00:04.958001  5077 solver.cpp:228] Iteration 1750, loss = 0.186077
I0507 19:00:04.958173  5077 solver.cpp:244]     Train net output #0: loss = 0.186077 (* 1 = 0.186077 loss)
I0507 19:00:04.958194  5077 sgd_solver.cpp:106] Iteration 1750, lr = 1e-05
I0507 19:02:12.627035  5077 solver.cpp:337] Iteration 1800, Testing net (#0)
I0507 19:02:43.008437  5077 solver.cpp:404]     Test net output #0: loss = 0.309683 (* 1 = 0.309683 loss)
I0507 19:02:43.879217  5077 solver.cpp:228] Iteration 1800, loss = 0.0246274
I0507 19:02:43.879294  5077 solver.cpp:244]     Train net output #0: loss = 0.0246275 (* 1 = 0.0246275 loss)
I0507 19:02:43.879310  5077 sgd_solver.cpp:106] Iteration 1800, lr = 1e-05
I0507 19:04:53.710678  5077 solver.cpp:228] Iteration 1850, loss = 0.0345379
I0507 19:04:53.710932  5077 solver.cpp:244]     Train net output #0: loss = 0.0345379 (* 1 = 0.0345379 loss)
I0507 19:04:53.710970  5077 sgd_solver.cpp:106] Iteration 1850, lr = 1e-05
I0507 19:07:03.630188  5077 solver.cpp:228] Iteration 1900, loss = 0.203352
I0507 19:07:03.631736  5077 solver.cpp:244]     Train net output #0: loss = 0.203352 (* 1 = 0.203352 loss)
I0507 19:07:03.631753  5077 sgd_solver.cpp:106] Iteration 1900, lr = 1e-05
I0507 19:09:13.522032  5077 solver.cpp:228] Iteration 1950, loss = 0.451155
I0507 19:09:13.522299  5077 solver.cpp:244]     Train net output #0: loss = 0.451156 (* 1 = 0.451156 loss)
I0507 19:09:13.522356  5077 sgd_solver.cpp:106] Iteration 1950, lr = 1e-05
I0507 19:11:20.715369  5077 solver.cpp:454] Snapshotting to binary proto file ../../weights/three_stream_triplet_loss_iter_2000.caffemodel
I0507 19:11:24.184718  5077 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../../weights/three_stream_triplet_loss_iter_2000.solverstate
I0507 19:11:24.288511  5077 solver.cpp:337] Iteration 2000, Testing net (#0)
I0507 19:11:53.372738  5077 solver.cpp:404]     Test net output #0: loss = 0.334624 (* 1 = 0.334624 loss)
I0507 19:11:54.239856  5077 solver.cpp:228] Iteration 2000, loss = 0.141559
I0507 19:11:54.239919  5077 solver.cpp:244]     Train net output #0: loss = 0.14156 (* 1 = 0.14156 loss)
I0507 19:11:54.239934  5077 sgd_solver.cpp:106] Iteration 2000, lr = 1e-05
I0507 19:14:04.192893  5077 solver.cpp:228] Iteration 2050, loss = 0.234382
I0507 19:14:04.199241  5077 solver.cpp:244]     Train net output #0: loss = 0.234382 (* 1 = 0.234382 loss)
I0507 19:14:04.199262  5077 sgd_solver.cpp:106] Iteration 2050, lr = 1e-05
I0507 19:16:14.050726  5077 solver.cpp:228] Iteration 2100, loss = 0.0473254
I0507 19:16:14.050925  5077 solver.cpp:244]     Train net output #0: loss = 0.0473255 (* 1 = 0.0473255 loss)
I0507 19:16:14.050948  5077 sgd_solver.cpp:106] Iteration 2100, lr = 1e-05
I0507 19:18:24.074280  5077 solver.cpp:228] Iteration 2150, loss = 0.211168
I0507 19:18:24.076593  5077 solver.cpp:244]     Train net output #0: loss = 0.211168 (* 1 = 0.211168 loss)
I0507 19:18:24.076637  5077 sgd_solver.cpp:106] Iteration 2150, lr = 1e-05
I0507 19:20:31.335995  5077 solver.cpp:337] Iteration 2200, Testing net (#0)
I0507 19:20:58.447224  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 19:21:02.618000  5077 solver.cpp:404]     Test net output #0: loss = 0.357886 (* 1 = 0.357886 loss)
I0507 19:21:03.484175  5077 solver.cpp:228] Iteration 2200, loss = 0.207688
I0507 19:21:03.484253  5077 solver.cpp:244]     Train net output #0: loss = 0.207688 (* 1 = 0.207688 loss)
I0507 19:21:03.484269  5077 sgd_solver.cpp:106] Iteration 2200, lr = 1e-05
I0507 19:23:13.497215  5077 solver.cpp:228] Iteration 2250, loss = 0.111847
I0507 19:23:13.499861  5077 solver.cpp:244]     Train net output #0: loss = 0.111847 (* 1 = 0.111847 loss)
I0507 19:23:13.499876  5077 sgd_solver.cpp:106] Iteration 2250, lr = 1e-05
I0507 19:25:23.429958  5077 solver.cpp:228] Iteration 2300, loss = 0.126469
I0507 19:25:23.430184  5077 solver.cpp:244]     Train net output #0: loss = 0.126469 (* 1 = 0.126469 loss)
I0507 19:25:23.430223  5077 sgd_solver.cpp:106] Iteration 2300, lr = 1e-05
I0507 19:27:33.226514  5077 solver.cpp:228] Iteration 2350, loss = 0.279702
I0507 19:27:33.227077  5077 solver.cpp:244]     Train net output #0: loss = 0.279702 (* 1 = 0.279702 loss)
I0507 19:27:33.227151  5077 sgd_solver.cpp:106] Iteration 2350, lr = 1e-05
I0507 19:29:40.548147  5077 solver.cpp:337] Iteration 2400, Testing net (#0)
I0507 19:30:11.937968  5077 solver.cpp:404]     Test net output #0: loss = 0.411041 (* 1 = 0.411041 loss)
I0507 19:30:12.805176  5077 solver.cpp:228] Iteration 2400, loss = 0.230557
I0507 19:30:12.805259  5077 solver.cpp:244]     Train net output #0: loss = 0.230557 (* 1 = 0.230557 loss)
I0507 19:30:12.805276  5077 sgd_solver.cpp:106] Iteration 2400, lr = 1e-05
I0507 19:32:22.442528  5077 solver.cpp:228] Iteration 2450, loss = 0.176708
I0507 19:32:22.442694  5077 solver.cpp:244]     Train net output #0: loss = 0.176708 (* 1 = 0.176708 loss)
I0507 19:32:22.442715  5077 sgd_solver.cpp:106] Iteration 2450, lr = 1e-05
I0507 19:34:32.371724  5077 solver.cpp:228] Iteration 2500, loss = 0.195551
I0507 19:34:32.373517  5077 solver.cpp:244]     Train net output #0: loss = 0.195551 (* 1 = 0.195551 loss)
I0507 19:34:32.373549  5077 sgd_solver.cpp:106] Iteration 2500, lr = 1e-05
I0507 19:36:42.437296  5077 solver.cpp:228] Iteration 2550, loss = 0.124087
I0507 19:36:42.437472  5077 solver.cpp:244]     Train net output #0: loss = 0.124087 (* 1 = 0.124087 loss)
I0507 19:36:42.437489  5077 sgd_solver.cpp:106] Iteration 2550, lr = 1e-05
I0507 19:38:49.678879  5077 solver.cpp:337] Iteration 2600, Testing net (#0)
I0507 19:39:20.167906  5077 solver.cpp:404]     Test net output #0: loss = 0.266473 (* 1 = 0.266473 loss)
I0507 19:39:21.038055  5077 solver.cpp:228] Iteration 2600, loss = 0.0641015
I0507 19:39:21.038132  5077 solver.cpp:244]     Train net output #0: loss = 0.0641014 (* 1 = 0.0641014 loss)
I0507 19:39:21.038151  5077 sgd_solver.cpp:106] Iteration 2600, lr = 1e-05
I0507 19:41:30.948861  5077 solver.cpp:228] Iteration 2650, loss = 0.207574
I0507 19:41:30.957028  5077 solver.cpp:244]     Train net output #0: loss = 0.207574 (* 1 = 0.207574 loss)
I0507 19:41:30.957053  5077 sgd_solver.cpp:106] Iteration 2650, lr = 1e-05
I0507 19:43:40.709628  5077 solver.cpp:228] Iteration 2700, loss = 0.35535
I0507 19:43:40.709873  5077 solver.cpp:244]     Train net output #0: loss = 0.35535 (* 1 = 0.35535 loss)
I0507 19:43:40.709903  5077 sgd_solver.cpp:106] Iteration 2700, lr = 1e-05
I0507 19:45:50.583952  5077 solver.cpp:228] Iteration 2750, loss = 0.0320332
I0507 19:45:50.584146  5077 solver.cpp:244]     Train net output #0: loss = 0.0320331 (* 1 = 0.0320331 loss)
I0507 19:45:50.584169  5077 sgd_solver.cpp:106] Iteration 2750, lr = 1e-05
I0507 19:47:57.664011  5077 solver.cpp:337] Iteration 2800, Testing net (#0)
I0507 19:48:24.081100  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 19:48:28.634412  5077 solver.cpp:404]     Test net output #0: loss = 0.277448 (* 1 = 0.277448 loss)
I0507 19:48:31.442456  5077 solver.cpp:228] Iteration 2800, loss = 0.0927084
I0507 19:48:31.442513  5077 solver.cpp:244]     Train net output #0: loss = 0.0927083 (* 1 = 0.0927083 loss)
I0507 19:48:31.442528  5077 sgd_solver.cpp:106] Iteration 2800, lr = 1e-05
I0507 19:50:41.220453  5077 solver.cpp:228] Iteration 2850, loss = 0.148235
I0507 19:50:41.220634  5077 solver.cpp:244]     Train net output #0: loss = 0.148235 (* 1 = 0.148235 loss)
I0507 19:50:41.220664  5077 sgd_solver.cpp:106] Iteration 2850, lr = 1e-05
I0507 19:52:51.209038  5077 solver.cpp:228] Iteration 2900, loss = 0.379798
I0507 19:52:51.209259  5077 solver.cpp:244]     Train net output #0: loss = 0.379798 (* 1 = 0.379798 loss)
I0507 19:52:51.209312  5077 sgd_solver.cpp:106] Iteration 2900, lr = 1e-05
I0507 19:55:01.179236  5077 solver.cpp:228] Iteration 2950, loss = 0.0566869
I0507 19:55:01.179427  5077 solver.cpp:244]     Train net output #0: loss = 0.0566868 (* 1 = 0.0566868 loss)
I0507 19:55:01.179442  5077 sgd_solver.cpp:106] Iteration 2950, lr = 1e-05
I0507 19:57:08.491873  5077 solver.cpp:337] Iteration 3000, Testing net (#0)
I0507 19:58:02.915215  5077 solver.cpp:404]     Test net output #0: loss = 0.328491 (* 1 = 0.328491 loss)
I0507 19:58:03.774286  5077 solver.cpp:228] Iteration 3000, loss = 0.264172
I0507 19:58:03.774348  5077 solver.cpp:244]     Train net output #0: loss = 0.264172 (* 1 = 0.264172 loss)
I0507 19:58:03.774363  5077 sgd_solver.cpp:106] Iteration 3000, lr = 1e-05
I0507 20:00:14.202391  5077 solver.cpp:228] Iteration 3050, loss = 0.0162401
I0507 20:00:14.202612  5077 solver.cpp:244]     Train net output #0: loss = 0.0162401 (* 1 = 0.0162401 loss)
I0507 20:00:14.202637  5077 sgd_solver.cpp:106] Iteration 3050, lr = 1e-05
I0507 20:02:24.040731  5077 solver.cpp:228] Iteration 3100, loss = 0.195192
I0507 20:02:24.040901  5077 solver.cpp:244]     Train net output #0: loss = 0.195192 (* 1 = 0.195192 loss)
I0507 20:02:24.040927  5077 sgd_solver.cpp:106] Iteration 3100, lr = 1e-05
I0507 20:04:34.180132  5077 solver.cpp:228] Iteration 3150, loss = 0.151942
I0507 20:04:34.180326  5077 solver.cpp:244]     Train net output #0: loss = 0.151942 (* 1 = 0.151942 loss)
I0507 20:04:34.180353  5077 sgd_solver.cpp:106] Iteration 3150, lr = 1e-05
I0507 20:06:41.591845  5077 solver.cpp:337] Iteration 3200, Testing net (#0)
I0507 20:07:26.188612  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 20:07:30.184885  5077 solver.cpp:404]     Test net output #0: loss = 0.338599 (* 1 = 0.338599 loss)
I0507 20:07:31.049118  5077 solver.cpp:228] Iteration 3200, loss = 0.0923747
I0507 20:07:31.049187  5077 solver.cpp:244]     Train net output #0: loss = 0.0923747 (* 1 = 0.0923747 loss)
I0507 20:07:31.049214  5077 sgd_solver.cpp:106] Iteration 3200, lr = 1e-05
I0507 20:09:42.225481  5077 solver.cpp:228] Iteration 3250, loss = 0.244622
I0507 20:09:42.225651  5077 solver.cpp:244]     Train net output #0: loss = 0.244622 (* 1 = 0.244622 loss)
I0507 20:09:42.225666  5077 sgd_solver.cpp:106] Iteration 3250, lr = 1e-05
I0507 20:11:52.358069  5077 solver.cpp:228] Iteration 3300, loss = 0.195975
I0507 20:11:52.358773  5077 solver.cpp:244]     Train net output #0: loss = 0.195975 (* 1 = 0.195975 loss)
I0507 20:11:52.358800  5077 sgd_solver.cpp:106] Iteration 3300, lr = 1e-05
I0507 20:14:02.284227  5077 solver.cpp:228] Iteration 3350, loss = 0.039317
I0507 20:14:02.284384  5077 solver.cpp:244]     Train net output #0: loss = 0.039317 (* 1 = 0.039317 loss)
I0507 20:14:02.284404  5077 sgd_solver.cpp:106] Iteration 3350, lr = 1e-05
I0507 20:16:09.732950  5077 solver.cpp:337] Iteration 3400, Testing net (#0)
I0507 20:16:49.882792  5077 solver.cpp:404]     Test net output #0: loss = 0.359862 (* 1 = 0.359862 loss)
I0507 20:16:50.747726  5077 solver.cpp:228] Iteration 3400, loss = 0.15964
I0507 20:16:50.747803  5077 solver.cpp:244]     Train net output #0: loss = 0.15964 (* 1 = 0.15964 loss)
I0507 20:16:50.747817  5077 sgd_solver.cpp:106] Iteration 3400, lr = 1e-05
I0507 20:19:00.746110  5077 solver.cpp:228] Iteration 3450, loss = 0.238521
I0507 20:19:00.749128  5077 solver.cpp:244]     Train net output #0: loss = 0.238521 (* 1 = 0.238521 loss)
I0507 20:19:00.749171  5077 sgd_solver.cpp:106] Iteration 3450, lr = 1e-05
I0507 20:21:10.688555  5077 solver.cpp:228] Iteration 3500, loss = 0.293491
I0507 20:21:10.688727  5077 solver.cpp:244]     Train net output #0: loss = 0.293491 (* 1 = 0.293491 loss)
I0507 20:21:10.688745  5077 sgd_solver.cpp:106] Iteration 3500, lr = 1e-05
I0507 20:23:20.675384  5077 solver.cpp:228] Iteration 3550, loss = 0.0782125
I0507 20:23:20.675645  5077 solver.cpp:244]     Train net output #0: loss = 0.0782124 (* 1 = 0.0782124 loss)
I0507 20:23:20.675689  5077 sgd_solver.cpp:106] Iteration 3550, lr = 1e-05
I0507 20:25:27.948058  5077 solver.cpp:337] Iteration 3600, Testing net (#0)
I0507 20:26:07.976744  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 20:26:08.987185  5077 solver.cpp:404]     Test net output #0: loss = 0.227636 (* 1 = 0.227636 loss)
I0507 20:26:09.855849  5077 solver.cpp:228] Iteration 3600, loss = 0.116335
I0507 20:26:09.855916  5077 solver.cpp:244]     Train net output #0: loss = 0.116335 (* 1 = 0.116335 loss)
I0507 20:26:09.855938  5077 sgd_solver.cpp:106] Iteration 3600, lr = 1e-05
I0507 20:28:20.366068  5077 solver.cpp:228] Iteration 3650, loss = 0.0767947
I0507 20:28:20.366219  5077 solver.cpp:244]     Train net output #0: loss = 0.0767947 (* 1 = 0.0767947 loss)
I0507 20:28:20.366235  5077 sgd_solver.cpp:106] Iteration 3650, lr = 1e-05
I0507 20:30:30.423506  5077 solver.cpp:228] Iteration 3700, loss = 0.120817
I0507 20:30:30.423684  5077 solver.cpp:244]     Train net output #0: loss = 0.120817 (* 1 = 0.120817 loss)
I0507 20:30:30.423701  5077 sgd_solver.cpp:106] Iteration 3700, lr = 1e-05
I0507 20:32:40.016932  5077 solver.cpp:228] Iteration 3750, loss = 0.116113
I0507 20:32:40.019842  5077 solver.cpp:244]     Train net output #0: loss = 0.116113 (* 1 = 0.116113 loss)
I0507 20:32:40.019862  5077 sgd_solver.cpp:106] Iteration 3750, lr = 1e-05
I0507 20:34:47.411839  5077 solver.cpp:337] Iteration 3800, Testing net (#0)
I0507 20:35:37.790392  5077 solver.cpp:404]     Test net output #0: loss = 0.227821 (* 1 = 0.227821 loss)
I0507 20:35:38.652889  5077 solver.cpp:228] Iteration 3800, loss = 0.0442574
I0507 20:35:38.652978  5077 solver.cpp:244]     Train net output #0: loss = 0.0442575 (* 1 = 0.0442575 loss)
I0507 20:35:38.652997  5077 sgd_solver.cpp:106] Iteration 3800, lr = 1e-05
I0507 20:37:49.063410  5077 solver.cpp:228] Iteration 3850, loss = 0.151204
I0507 20:37:49.063608  5077 solver.cpp:244]     Train net output #0: loss = 0.151204 (* 1 = 0.151204 loss)
I0507 20:37:49.063627  5077 sgd_solver.cpp:106] Iteration 3850, lr = 1e-05
I0507 20:39:59.092442  5077 solver.cpp:228] Iteration 3900, loss = 0.197019
I0507 20:39:59.092648  5077 solver.cpp:244]     Train net output #0: loss = 0.197019 (* 1 = 0.197019 loss)
I0507 20:39:59.092664  5077 sgd_solver.cpp:106] Iteration 3900, lr = 1e-05
I0507 20:42:09.041407  5077 solver.cpp:228] Iteration 3950, loss = 0.229131
I0507 20:42:09.043015  5077 solver.cpp:244]     Train net output #0: loss = 0.229131 (* 1 = 0.229131 loss)
I0507 20:42:09.043037  5077 sgd_solver.cpp:106] Iteration 3950, lr = 1e-05
I0507 20:44:16.122877  5077 solver.cpp:454] Snapshotting to binary proto file ../../weights/three_stream_triplet_loss_iter_4000.caffemodel
I0507 20:44:24.701920  5077 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../../weights/three_stream_triplet_loss_iter_4000.solverstate
I0507 20:44:24.760563  5077 solver.cpp:337] Iteration 4000, Testing net (#0)
I0507 20:45:14.984704  5077 solver.cpp:404]     Test net output #0: loss = 0.212145 (* 1 = 0.212145 loss)
I0507 20:45:15.845561  5077 solver.cpp:228] Iteration 4000, loss = 0.0215344
I0507 20:45:15.845623  5077 solver.cpp:244]     Train net output #0: loss = 0.0215345 (* 1 = 0.0215345 loss)
I0507 20:45:15.845638  5077 sgd_solver.cpp:106] Iteration 4000, lr = 1e-05
I0507 20:47:26.516072  5077 solver.cpp:228] Iteration 4050, loss = 0.229922
I0507 20:47:26.528605  5077 solver.cpp:244]     Train net output #0: loss = 0.229922 (* 1 = 0.229922 loss)
I0507 20:47:26.528643  5077 sgd_solver.cpp:106] Iteration 4050, lr = 1e-05
I0507 20:49:36.310642  5077 solver.cpp:228] Iteration 4100, loss = 0.11431
I0507 20:49:36.312507  5077 solver.cpp:244]     Train net output #0: loss = 0.11431 (* 1 = 0.11431 loss)
I0507 20:49:36.312527  5077 sgd_solver.cpp:106] Iteration 4100, lr = 1e-05
I0507 20:51:46.578353  5077 solver.cpp:228] Iteration 4150, loss = 0.185533
I0507 20:51:46.578742  5077 solver.cpp:244]     Train net output #0: loss = 0.185533 (* 1 = 0.185533 loss)
I0507 20:51:46.578836  5077 sgd_solver.cpp:106] Iteration 4150, lr = 1e-05
I0507 20:53:54.173044  5077 solver.cpp:337] Iteration 4200, Testing net (#0)
I0507 20:54:01.852650  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 20:54:25.879850  5077 solver.cpp:404]     Test net output #0: loss = 0.324508 (* 1 = 0.324508 loss)
I0507 20:54:26.748457  5077 solver.cpp:228] Iteration 4200, loss = 0.166814
I0507 20:54:26.748553  5077 solver.cpp:244]     Train net output #0: loss = 0.166814 (* 1 = 0.166814 loss)
I0507 20:54:26.748580  5077 sgd_solver.cpp:106] Iteration 4200, lr = 1e-05
I0507 20:56:37.104306  5077 solver.cpp:228] Iteration 4250, loss = 0.0229218
I0507 20:56:37.104470  5077 solver.cpp:244]     Train net output #0: loss = 0.0229219 (* 1 = 0.0229219 loss)
I0507 20:56:37.104496  5077 sgd_solver.cpp:106] Iteration 4250, lr = 1e-05
I0507 20:58:47.203076  5077 solver.cpp:228] Iteration 4300, loss = 0.0575682
I0507 20:58:47.203254  5077 solver.cpp:244]     Train net output #0: loss = 0.0575683 (* 1 = 0.0575683 loss)
I0507 20:58:47.203276  5077 sgd_solver.cpp:106] Iteration 4300, lr = 1e-05
I0507 21:00:56.533248  5077 solver.cpp:228] Iteration 4350, loss = 0.0852218
I0507 21:00:56.533459  5077 solver.cpp:244]     Train net output #0: loss = 0.0852219 (* 1 = 0.0852219 loss)
I0507 21:00:56.533499  5077 sgd_solver.cpp:106] Iteration 4350, lr = 1e-05
I0507 21:03:04.185045  5077 solver.cpp:337] Iteration 4400, Testing net (#0)
I0507 21:03:56.854843  5077 solver.cpp:404]     Test net output #0: loss = 0.294116 (* 1 = 0.294116 loss)
I0507 21:03:57.719877  5077 solver.cpp:228] Iteration 4400, loss = 0.211022
I0507 21:03:57.719938  5077 solver.cpp:244]     Train net output #0: loss = 0.211022 (* 1 = 0.211022 loss)
I0507 21:03:57.719954  5077 sgd_solver.cpp:106] Iteration 4400, lr = 1e-05
I0507 21:06:08.285737  5077 solver.cpp:228] Iteration 4450, loss = 0.018602
I0507 21:06:08.285923  5077 solver.cpp:244]     Train net output #0: loss = 0.0186021 (* 1 = 0.0186021 loss)
I0507 21:06:08.285939  5077 sgd_solver.cpp:106] Iteration 4450, lr = 1e-05
I0507 21:08:18.273365  5077 solver.cpp:228] Iteration 4500, loss = 0.260621
I0507 21:08:18.273527  5077 solver.cpp:244]     Train net output #0: loss = 0.260621 (* 1 = 0.260621 loss)
I0507 21:08:18.273543  5077 sgd_solver.cpp:106] Iteration 4500, lr = 1e-05
I0507 21:10:28.578261  5077 solver.cpp:228] Iteration 4550, loss = 0.477163
I0507 21:10:28.578408  5077 solver.cpp:244]     Train net output #0: loss = 0.477163 (* 1 = 0.477163 loss)
I0507 21:10:28.578424  5077 sgd_solver.cpp:106] Iteration 4550, lr = 1e-05
I0507 21:12:36.236896  5077 solver.cpp:337] Iteration 4600, Testing net (#0)
I0507 21:13:06.593134  5077 solver.cpp:404]     Test net output #0: loss = 0.257106 (* 1 = 0.257106 loss)
I0507 21:13:07.458999  5077 solver.cpp:228] Iteration 4600, loss = 0.105177
I0507 21:13:07.459067  5077 solver.cpp:244]     Train net output #0: loss = 0.105177 (* 1 = 0.105177 loss)
I0507 21:13:07.459082  5077 sgd_solver.cpp:106] Iteration 4600, lr = 1e-05
I0507 21:15:17.430918  5077 solver.cpp:228] Iteration 4650, loss = 0.0810152
I0507 21:15:17.431524  5077 solver.cpp:244]     Train net output #0: loss = 0.0810154 (* 1 = 0.0810154 loss)
I0507 21:15:17.431578  5077 sgd_solver.cpp:106] Iteration 4650, lr = 1e-05
I0507 21:17:27.424224  5077 solver.cpp:228] Iteration 4700, loss = 0.157304
I0507 21:17:27.424826  5077 solver.cpp:244]     Train net output #0: loss = 0.157305 (* 1 = 0.157305 loss)
I0507 21:17:27.424844  5077 sgd_solver.cpp:106] Iteration 4700, lr = 1e-05
I0507 21:19:37.246773  5077 solver.cpp:228] Iteration 4750, loss = 0.0603208
I0507 21:19:37.259310  5077 solver.cpp:244]     Train net output #0: loss = 0.0603211 (* 1 = 0.0603211 loss)
I0507 21:19:37.259348  5077 sgd_solver.cpp:106] Iteration 4750, lr = 1e-05
I0507 21:21:44.601728  5077 solver.cpp:337] Iteration 4800, Testing net (#0)
I0507 21:22:07.664528  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 21:22:14.932955  5077 solver.cpp:404]     Test net output #0: loss = 0.211255 (* 1 = 0.211255 loss)
I0507 21:22:15.800933  5077 solver.cpp:228] Iteration 4800, loss = 0.167135
I0507 21:22:15.801008  5077 solver.cpp:244]     Train net output #0: loss = 0.167136 (* 1 = 0.167136 loss)
I0507 21:22:15.801021  5077 sgd_solver.cpp:106] Iteration 4800, lr = 1e-05
I0507 21:24:25.897709  5077 solver.cpp:228] Iteration 4850, loss = 0.0417881
I0507 21:24:25.899178  5077 solver.cpp:244]     Train net output #0: loss = 0.0417884 (* 1 = 0.0417884 loss)
I0507 21:24:25.899205  5077 sgd_solver.cpp:106] Iteration 4850, lr = 1e-05
I0507 21:26:35.746673  5077 solver.cpp:228] Iteration 4900, loss = 0.303281
I0507 21:26:35.746884  5077 solver.cpp:244]     Train net output #0: loss = 0.303281 (* 1 = 0.303281 loss)
I0507 21:26:35.746954  5077 sgd_solver.cpp:106] Iteration 4900, lr = 1e-05
I0507 21:28:45.969708  5077 solver.cpp:228] Iteration 4950, loss = 0.26198
I0507 21:28:45.969918  5077 solver.cpp:244]     Train net output #0: loss = 0.261981 (* 1 = 0.261981 loss)
I0507 21:28:45.969954  5077 sgd_solver.cpp:106] Iteration 4950, lr = 1e-05
I0507 21:30:53.560509  5077 solver.cpp:337] Iteration 5000, Testing net (#0)
I0507 21:31:28.582687  5077 solver.cpp:404]     Test net output #0: loss = 0.203574 (* 1 = 0.203574 loss)
I0507 21:31:29.449586  5077 solver.cpp:228] Iteration 5000, loss = 0.0465222
I0507 21:31:29.449667  5077 solver.cpp:244]     Train net output #0: loss = 0.0465224 (* 1 = 0.0465224 loss)
I0507 21:31:29.449682  5077 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I0507 21:33:39.542745  5077 solver.cpp:228] Iteration 5050, loss = 0.14959
I0507 21:33:39.543095  5077 solver.cpp:244]     Train net output #0: loss = 0.14959 (* 1 = 0.14959 loss)
I0507 21:33:39.543118  5077 sgd_solver.cpp:106] Iteration 5050, lr = 1e-05
I0507 21:35:49.426322  5077 solver.cpp:228] Iteration 5100, loss = 0.0906498
I0507 21:35:49.426564  5077 solver.cpp:244]     Train net output #0: loss = 0.09065 (* 1 = 0.09065 loss)
I0507 21:35:49.426592  5077 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I0507 21:37:59.554606  5077 solver.cpp:228] Iteration 5150, loss = 0.294473
I0507 21:37:59.554801  5077 solver.cpp:244]     Train net output #0: loss = 0.294473 (* 1 = 0.294473 loss)
I0507 21:37:59.554817  5077 sgd_solver.cpp:106] Iteration 5150, lr = 1e-05
I0507 21:40:06.810961  5077 solver.cpp:337] Iteration 5200, Testing net (#0)
I0507 21:40:37.816571  5077 solver.cpp:404]     Test net output #0: loss = 0.198665 (* 1 = 0.198665 loss)
I0507 21:40:38.685258  5077 solver.cpp:228] Iteration 5200, loss = 0.0462157
I0507 21:40:38.685322  5077 solver.cpp:244]     Train net output #0: loss = 0.0462158 (* 1 = 0.0462158 loss)
I0507 21:40:38.685351  5077 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I0507 21:42:48.697460  5077 solver.cpp:228] Iteration 5250, loss = 0.0823852
I0507 21:42:48.699254  5077 solver.cpp:244]     Train net output #0: loss = 0.0823853 (* 1 = 0.0823853 loss)
I0507 21:42:48.699309  5077 sgd_solver.cpp:106] Iteration 5250, lr = 1e-05
I0507 21:44:58.583943  5077 solver.cpp:228] Iteration 5300, loss = 0.641734
I0507 21:44:58.584183  5077 solver.cpp:244]     Train net output #0: loss = 0.641734 (* 1 = 0.641734 loss)
I0507 21:44:58.584218  5077 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I0507 21:47:08.643784  5077 solver.cpp:228] Iteration 5350, loss = 0.144182
I0507 21:47:08.643981  5077 solver.cpp:244]     Train net output #0: loss = 0.144182 (* 1 = 0.144182 loss)
I0507 21:47:08.644008  5077 sgd_solver.cpp:106] Iteration 5350, lr = 1e-05
I0507 21:49:16.201576  5077 solver.cpp:337] Iteration 5400, Testing net (#0)
I0507 21:49:25.983450  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 21:49:59.949940  5077 solver.cpp:404]     Test net output #0: loss = 0.275335 (* 1 = 0.275335 loss)
I0507 21:50:00.821050  5077 solver.cpp:228] Iteration 5400, loss = 0.0221332
I0507 21:50:00.821152  5077 solver.cpp:244]     Train net output #0: loss = 0.0221333 (* 1 = 0.0221333 loss)
I0507 21:50:00.821173  5077 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I0507 21:52:11.058969  5077 solver.cpp:228] Iteration 5450, loss = 0.440099
I0507 21:52:11.059123  5077 solver.cpp:244]     Train net output #0: loss = 0.440099 (* 1 = 0.440099 loss)
I0507 21:52:11.059144  5077 sgd_solver.cpp:106] Iteration 5450, lr = 1e-05
I0507 21:54:20.957511  5077 solver.cpp:228] Iteration 5500, loss = 0.177566
I0507 21:54:20.957702  5077 solver.cpp:244]     Train net output #0: loss = 0.177566 (* 1 = 0.177566 loss)
I0507 21:54:20.957718  5077 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I0507 21:56:30.947949  5077 solver.cpp:228] Iteration 5550, loss = 0.0972116
I0507 21:56:30.948125  5077 solver.cpp:244]     Train net output #0: loss = 0.0972117 (* 1 = 0.0972117 loss)
I0507 21:56:30.948143  5077 sgd_solver.cpp:106] Iteration 5550, lr = 1e-05
I0507 21:58:38.268383  5077 solver.cpp:337] Iteration 5600, Testing net (#0)
I0507 21:59:30.999789  5077 solver.cpp:404]     Test net output #0: loss = 0.176924 (* 1 = 0.176924 loss)
I0507 21:59:31.858100  5077 solver.cpp:228] Iteration 5600, loss = 0.0262425
I0507 21:59:31.858180  5077 solver.cpp:244]     Train net output #0: loss = 0.0262426 (* 1 = 0.0262426 loss)
I0507 21:59:31.858192  5077 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I0507 22:01:41.828128  5077 solver.cpp:228] Iteration 5650, loss = 0.00540216
I0507 22:01:41.828312  5077 solver.cpp:244]     Train net output #0: loss = 0.00540218 (* 1 = 0.00540218 loss)
I0507 22:01:41.828338  5077 sgd_solver.cpp:106] Iteration 5650, lr = 1e-05
I0507 22:03:51.961213  5077 solver.cpp:228] Iteration 5700, loss = 0.0284016
I0507 22:03:51.961414  5077 solver.cpp:244]     Train net output #0: loss = 0.0284015 (* 1 = 0.0284015 loss)
I0507 22:03:51.961429  5077 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I0507 22:06:02.050012  5077 solver.cpp:228] Iteration 5750, loss = 0.103769
I0507 22:06:02.050173  5077 solver.cpp:244]     Train net output #0: loss = 0.103769 (* 1 = 0.103769 loss)
I0507 22:06:02.050187  5077 sgd_solver.cpp:106] Iteration 5750, lr = 1e-05
I0507 22:08:09.283521  5077 solver.cpp:337] Iteration 5800, Testing net (#0)
I0507 22:08:21.729282  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 22:08:40.169965  5077 solver.cpp:404]     Test net output #0: loss = 0.310787 (* 1 = 0.310787 loss)
I0507 22:08:41.039274  5077 solver.cpp:228] Iteration 5800, loss = 0.143407
I0507 22:08:41.039342  5077 solver.cpp:244]     Train net output #0: loss = 0.143407 (* 1 = 0.143407 loss)
I0507 22:08:41.039358  5077 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I0507 22:10:51.093336  5077 solver.cpp:228] Iteration 5850, loss = 0.00599277
I0507 22:10:51.093562  5077 solver.cpp:244]     Train net output #0: loss = 0.00599286 (* 1 = 0.00599286 loss)
I0507 22:10:51.093587  5077 sgd_solver.cpp:106] Iteration 5850, lr = 1e-05
I0507 22:13:00.696637  5077 solver.cpp:228] Iteration 5900, loss = 0.0596565
I0507 22:13:00.696799  5077 solver.cpp:244]     Train net output #0: loss = 0.0596567 (* 1 = 0.0596567 loss)
I0507 22:13:00.696820  5077 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I0507 22:15:10.672056  5077 solver.cpp:228] Iteration 5950, loss = 0.197818
I0507 22:15:10.672253  5077 solver.cpp:244]     Train net output #0: loss = 0.197819 (* 1 = 0.197819 loss)
I0507 22:15:10.672272  5077 sgd_solver.cpp:106] Iteration 5950, lr = 1e-05
I0507 22:17:18.147637  5077 solver.cpp:454] Snapshotting to binary proto file ../../weights/three_stream_triplet_loss_iter_6000.caffemodel
I0507 22:17:21.337764  5077 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../../weights/three_stream_triplet_loss_iter_6000.solverstate
I0507 22:17:21.397025  5077 solver.cpp:337] Iteration 6000, Testing net (#0)
I0507 22:18:08.611779  5077 solver.cpp:404]     Test net output #0: loss = 0.231738 (* 1 = 0.231738 loss)
I0507 22:18:09.485055  5077 solver.cpp:228] Iteration 6000, loss = 0.134998
I0507 22:18:09.485129  5077 solver.cpp:244]     Train net output #0: loss = 0.134999 (* 1 = 0.134999 loss)
I0507 22:18:09.485144  5077 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I0507 22:20:19.793560  5077 solver.cpp:228] Iteration 6050, loss = 0.234327
I0507 22:20:19.794096  5077 solver.cpp:244]     Train net output #0: loss = 0.234327 (* 1 = 0.234327 loss)
I0507 22:20:19.794126  5077 sgd_solver.cpp:106] Iteration 6050, lr = 1e-05
I0507 22:22:30.040899  5077 solver.cpp:228] Iteration 6100, loss = 0.0394013
I0507 22:22:30.042115  5077 solver.cpp:244]     Train net output #0: loss = 0.0394014 (* 1 = 0.0394014 loss)
I0507 22:22:30.042134  5077 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I0507 22:24:40.078805  5077 solver.cpp:228] Iteration 6150, loss = 0.0383689
I0507 22:24:40.079777  5077 solver.cpp:244]     Train net output #0: loss = 0.0383691 (* 1 = 0.0383691 loss)
I0507 22:24:40.079795  5077 sgd_solver.cpp:106] Iteration 6150, lr = 1e-05
I0507 22:26:47.554388  5077 solver.cpp:337] Iteration 6200, Testing net (#0)
I0507 22:27:09.220060  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 22:27:37.889533  5077 solver.cpp:404]     Test net output #0: loss = 0.197644 (* 1 = 0.197644 loss)
I0507 22:27:38.753244  5077 solver.cpp:228] Iteration 6200, loss = 0.060051
I0507 22:27:38.753345  5077 solver.cpp:244]     Train net output #0: loss = 0.0600511 (* 1 = 0.0600511 loss)
I0507 22:27:38.753362  5077 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I0507 22:29:49.027082  5077 solver.cpp:228] Iteration 6250, loss = 0.256904
I0507 22:29:49.027277  5077 solver.cpp:244]     Train net output #0: loss = 0.256904 (* 1 = 0.256904 loss)
I0507 22:29:49.027294  5077 sgd_solver.cpp:106] Iteration 6250, lr = 1e-05
I0507 22:31:58.751031  5077 solver.cpp:228] Iteration 6300, loss = 0.0479955
I0507 22:31:58.751216  5077 solver.cpp:244]     Train net output #0: loss = 0.0479957 (* 1 = 0.0479957 loss)
I0507 22:31:58.751256  5077 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I0507 22:34:08.574527  5077 solver.cpp:228] Iteration 6350, loss = 0.315969
I0507 22:34:08.574754  5077 solver.cpp:244]     Train net output #0: loss = 0.31597 (* 1 = 0.31597 loss)
I0507 22:34:08.574784  5077 sgd_solver.cpp:106] Iteration 6350, lr = 1e-05
I0507 22:36:16.048449  5077 solver.cpp:337] Iteration 6400, Testing net (#0)
I0507 22:36:50.607035  5077 solver.cpp:404]     Test net output #0: loss = 0.282724 (* 1 = 0.282724 loss)
I0507 22:36:51.474778  5077 solver.cpp:228] Iteration 6400, loss = 0.0389142
I0507 22:36:51.474850  5077 solver.cpp:244]     Train net output #0: loss = 0.0389143 (* 1 = 0.0389143 loss)
I0507 22:36:51.474879  5077 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I0507 22:39:01.739698  5077 solver.cpp:228] Iteration 6450, loss = 0.227532
I0507 22:39:01.739866  5077 solver.cpp:244]     Train net output #0: loss = 0.227532 (* 1 = 0.227532 loss)
I0507 22:39:01.739884  5077 sgd_solver.cpp:106] Iteration 6450, lr = 1e-05
I0507 22:41:11.719604  5077 solver.cpp:228] Iteration 6500, loss = 0.0663985
I0507 22:41:11.719825  5077 solver.cpp:244]     Train net output #0: loss = 0.0663985 (* 1 = 0.0663985 loss)
I0507 22:41:11.719879  5077 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I0507 22:43:21.629432  5077 solver.cpp:228] Iteration 6550, loss = 0.151777
I0507 22:43:21.629617  5077 solver.cpp:244]     Train net output #0: loss = 0.151777 (* 1 = 0.151777 loss)
I0507 22:43:21.629634  5077 sgd_solver.cpp:106] Iteration 6550, lr = 1e-05
I0507 22:45:29.116269  5077 solver.cpp:337] Iteration 6600, Testing net (#0)
I0507 22:45:51.649677  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 22:46:15.778029  5077 solver.cpp:404]     Test net output #0: loss = 0.236092 (* 1 = 0.236092 loss)
I0507 22:46:16.652802  5077 solver.cpp:228] Iteration 6600, loss = 0.0436826
I0507 22:46:16.652878  5077 solver.cpp:244]     Train net output #0: loss = 0.0436827 (* 1 = 0.0436827 loss)
I0507 22:46:16.652892  5077 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I0507 22:48:27.160606  5077 solver.cpp:228] Iteration 6650, loss = 0.148594
I0507 22:48:27.173440  5077 solver.cpp:244]     Train net output #0: loss = 0.148594 (* 1 = 0.148594 loss)
I0507 22:48:27.173492  5077 sgd_solver.cpp:106] Iteration 6650, lr = 1e-05
I0507 22:50:37.076516  5077 solver.cpp:228] Iteration 6700, loss = 0.156517
I0507 22:50:37.076783  5077 solver.cpp:244]     Train net output #0: loss = 0.156518 (* 1 = 0.156518 loss)
I0507 22:50:37.076843  5077 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I0507 22:52:46.856950  5077 solver.cpp:228] Iteration 6750, loss = 0.0791112
I0507 22:52:46.857120  5077 solver.cpp:244]     Train net output #0: loss = 0.0791112 (* 1 = 0.0791112 loss)
I0507 22:52:46.857141  5077 sgd_solver.cpp:106] Iteration 6750, lr = 1e-05
I0507 22:54:54.306982  5077 solver.cpp:337] Iteration 6800, Testing net (#0)
I0507 22:55:35.488086  5077 solver.cpp:404]     Test net output #0: loss = 0.201308 (* 1 = 0.201308 loss)
I0507 22:55:36.360288  5077 solver.cpp:228] Iteration 6800, loss = 0.0727961
I0507 22:55:36.360409  5077 solver.cpp:244]     Train net output #0: loss = 0.0727961 (* 1 = 0.0727961 loss)
I0507 22:55:36.360433  5077 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I0507 22:57:46.407905  5077 solver.cpp:228] Iteration 6850, loss = 0.0584555
I0507 22:57:46.408107  5077 solver.cpp:244]     Train net output #0: loss = 0.0584556 (* 1 = 0.0584556 loss)
I0507 22:57:46.408126  5077 sgd_solver.cpp:106] Iteration 6850, lr = 1e-05
I0507 22:59:56.635458  5077 solver.cpp:228] Iteration 6900, loss = 0.203364
I0507 22:59:56.635746  5077 solver.cpp:244]     Train net output #0: loss = 0.203364 (* 1 = 0.203364 loss)
I0507 22:59:56.635766  5077 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I0507 23:02:06.740276  5077 solver.cpp:228] Iteration 6950, loss = 0.131876
I0507 23:02:06.740483  5077 solver.cpp:244]     Train net output #0: loss = 0.131876 (* 1 = 0.131876 loss)
I0507 23:02:06.740505  5077 sgd_solver.cpp:106] Iteration 6950, lr = 1e-05
I0507 23:04:14.155557  5077 solver.cpp:337] Iteration 7000, Testing net (#0)
I0507 23:04:39.603624  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 23:05:04.167129  5077 solver.cpp:404]     Test net output #0: loss = 0.306768 (* 1 = 0.306768 loss)
I0507 23:05:05.027097  5077 solver.cpp:228] Iteration 7000, loss = 0.0971895
I0507 23:05:05.027168  5077 solver.cpp:244]     Train net output #0: loss = 0.0971896 (* 1 = 0.0971896 loss)
I0507 23:05:05.027187  5077 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I0507 23:07:15.328088  5077 solver.cpp:228] Iteration 7050, loss = 0.0650307
I0507 23:07:15.328337  5077 solver.cpp:244]     Train net output #0: loss = 0.0650307 (* 1 = 0.0650307 loss)
I0507 23:07:15.328385  5077 sgd_solver.cpp:106] Iteration 7050, lr = 1e-05
I0507 23:09:25.297060  5077 solver.cpp:228] Iteration 7100, loss = 0.0645888
I0507 23:09:25.297229  5077 solver.cpp:244]     Train net output #0: loss = 0.0645889 (* 1 = 0.0645889 loss)
I0507 23:09:25.297255  5077 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I0507 23:11:35.417822  5077 solver.cpp:228] Iteration 7150, loss = 0.198508
I0507 23:11:35.419047  5077 solver.cpp:244]     Train net output #0: loss = 0.198508 (* 1 = 0.198508 loss)
I0507 23:11:35.419086  5077 sgd_solver.cpp:106] Iteration 7150, lr = 1e-05
I0507 23:13:42.754377  5077 solver.cpp:337] Iteration 7200, Testing net (#0)
I0507 23:14:30.482950  5077 solver.cpp:404]     Test net output #0: loss = 0.18502 (* 1 = 0.18502 loss)
I0507 23:14:31.348088  5077 solver.cpp:228] Iteration 7200, loss = 0.144028
I0507 23:14:31.348145  5077 solver.cpp:244]     Train net output #0: loss = 0.144028 (* 1 = 0.144028 loss)
I0507 23:14:31.348158  5077 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I0507 23:16:41.930557  5077 solver.cpp:228] Iteration 7250, loss = 0.17089
I0507 23:16:41.931484  5077 solver.cpp:244]     Train net output #0: loss = 0.17089 (* 1 = 0.17089 loss)
I0507 23:16:41.931515  5077 sgd_solver.cpp:106] Iteration 7250, lr = 1e-05
I0507 23:18:51.989624  5077 solver.cpp:228] Iteration 7300, loss = 0.00674756
I0507 23:18:52.002115  5077 solver.cpp:244]     Train net output #0: loss = 0.00674774 (* 1 = 0.00674774 loss)
I0507 23:18:52.002147  5077 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I0507 23:21:02.156441  5077 solver.cpp:228] Iteration 7350, loss = 0.034241
I0507 23:21:02.156599  5077 solver.cpp:244]     Train net output #0: loss = 0.0342412 (* 1 = 0.0342412 loss)
I0507 23:21:02.156620  5077 sgd_solver.cpp:106] Iteration 7350, lr = 1e-05
I0507 23:23:09.581526  5077 solver.cpp:337] Iteration 7400, Testing net (#0)
I0507 23:23:31.320808  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0507 23:23:43.068945  5077 solver.cpp:404]     Test net output #0: loss = 0.364179 (* 1 = 0.364179 loss)
I0507 23:23:43.942440  5077 solver.cpp:228] Iteration 7400, loss = 0.163482
I0507 23:23:43.942515  5077 solver.cpp:244]     Train net output #0: loss = 0.163483 (* 1 = 0.163483 loss)
I0507 23:23:43.942530  5077 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I0507 23:25:54.083012  5077 solver.cpp:228] Iteration 7450, loss = 0.0954672
I0507 23:25:54.084616  5077 solver.cpp:244]     Train net output #0: loss = 0.0954674 (* 1 = 0.0954674 loss)
I0507 23:25:54.084646  5077 sgd_solver.cpp:106] Iteration 7450, lr = 1e-05
I0507 23:28:04.141180  5077 solver.cpp:228] Iteration 7500, loss = -1.60187e-07
I0507 23:28:04.141929  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0507 23:28:04.141962  5077 sgd_solver.cpp:106] Iteration 7500, lr = 1e-05
I0507 23:30:14.147281  5077 solver.cpp:228] Iteration 7550, loss = 0.0175838
I0507 23:30:14.159307  5077 solver.cpp:244]     Train net output #0: loss = 0.017584 (* 1 = 0.017584 loss)
I0507 23:30:14.159375  5077 sgd_solver.cpp:106] Iteration 7550, lr = 1e-05
I0507 23:32:21.420800  5077 solver.cpp:337] Iteration 7600, Testing net (#0)
I0507 23:33:13.235647  5077 solver.cpp:404]     Test net output #0: loss = 0.231178 (* 1 = 0.231178 loss)
I0507 23:33:14.095455  5077 solver.cpp:228] Iteration 7600, loss = 0.225852
I0507 23:33:14.095535  5077 solver.cpp:244]     Train net output #0: loss = 0.225853 (* 1 = 0.225853 loss)
I0507 23:33:14.095549  5077 sgd_solver.cpp:106] Iteration 7600, lr = 1e-05
I0507 23:35:24.523188  5077 solver.cpp:228] Iteration 7650, loss = 0.143627
I0507 23:35:24.523805  5077 solver.cpp:244]     Train net output #0: loss = 0.143627 (* 1 = 0.143627 loss)
I0507 23:35:24.523824  5077 sgd_solver.cpp:106] Iteration 7650, lr = 1e-05
I0507 23:37:34.619055  5077 solver.cpp:228] Iteration 7700, loss = 0.00272304
I0507 23:37:34.619786  5077 solver.cpp:244]     Train net output #0: loss = 0.00272319 (* 1 = 0.00272319 loss)
I0507 23:37:34.619810  5077 sgd_solver.cpp:106] Iteration 7700, lr = 1e-05
I0507 23:39:44.619945  5077 solver.cpp:228] Iteration 7750, loss = 0.0375368
I0507 23:39:44.620160  5077 solver.cpp:244]     Train net output #0: loss = 0.0375369 (* 1 = 0.0375369 loss)
I0507 23:39:44.620180  5077 sgd_solver.cpp:106] Iteration 7750, lr = 1e-05
I0507 23:41:52.128334  5077 solver.cpp:337] Iteration 7800, Testing net (#0)
I0507 23:42:22.569844  5077 solver.cpp:404]     Test net output #0: loss = 0.275921 (* 1 = 0.275921 loss)
I0507 23:42:23.433315  5077 solver.cpp:228] Iteration 7800, loss = 0.154374
I0507 23:42:23.433392  5077 solver.cpp:244]     Train net output #0: loss = 0.154374 (* 1 = 0.154374 loss)
I0507 23:42:23.433406  5077 sgd_solver.cpp:106] Iteration 7800, lr = 1e-05
I0507 23:44:33.247668  5077 solver.cpp:228] Iteration 7850, loss = 0.256604
I0507 23:44:33.247874  5077 solver.cpp:244]     Train net output #0: loss = 0.256604 (* 1 = 0.256604 loss)
I0507 23:44:33.247896  5077 sgd_solver.cpp:106] Iteration 7850, lr = 1e-05
I0507 23:46:43.200837  5077 solver.cpp:228] Iteration 7900, loss = 0.0855573
I0507 23:46:43.201094  5077 solver.cpp:244]     Train net output #0: loss = 0.0855573 (* 1 = 0.0855573 loss)
I0507 23:46:43.201146  5077 sgd_solver.cpp:106] Iteration 7900, lr = 1e-05
I0507 23:48:53.277308  5077 solver.cpp:228] Iteration 7950, loss = 0.0817658
I0507 23:48:53.289774  5077 solver.cpp:244]     Train net output #0: loss = 0.0817657 (* 1 = 0.0817657 loss)
I0507 23:48:53.289801  5077 sgd_solver.cpp:106] Iteration 7950, lr = 1e-05
I0507 23:51:00.581547  5077 solver.cpp:454] Snapshotting to binary proto file ../../weights/three_stream_triplet_loss_iter_8000.caffemodel
I0507 23:51:05.891798  5077 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../../weights/three_stream_triplet_loss_iter_8000.solverstate
I0507 23:51:05.946575  5077 solver.cpp:337] Iteration 8000, Testing net (#0)
I0507 23:51:34.434053  5077 solver.cpp:404]     Test net output #0: loss = 0.201459 (* 1 = 0.201459 loss)
I0507 23:51:35.299353  5077 solver.cpp:228] Iteration 8000, loss = 0.304149
I0507 23:51:35.299451  5077 solver.cpp:244]     Train net output #0: loss = 0.304149 (* 1 = 0.304149 loss)
I0507 23:51:35.299469  5077 sgd_solver.cpp:106] Iteration 8000, lr = 1e-05
I0507 23:53:45.388092  5077 solver.cpp:228] Iteration 8050, loss = 0.112123
I0507 23:53:45.388321  5077 solver.cpp:244]     Train net output #0: loss = 0.112124 (* 1 = 0.112124 loss)
I0507 23:53:45.388345  5077 sgd_solver.cpp:106] Iteration 8050, lr = 1e-05
I0507 23:55:55.214092  5077 solver.cpp:228] Iteration 8100, loss = 0.120632
I0507 23:55:55.214292  5077 solver.cpp:244]     Train net output #0: loss = 0.120632 (* 1 = 0.120632 loss)
I0507 23:55:55.214323  5077 sgd_solver.cpp:106] Iteration 8100, lr = 1e-05
I0507 23:58:05.177743  5077 solver.cpp:228] Iteration 8150, loss = 0.220794
I0507 23:58:05.177892  5077 solver.cpp:244]     Train net output #0: loss = 0.220794 (* 1 = 0.220794 loss)
I0507 23:58:05.177906  5077 sgd_solver.cpp:106] Iteration 8150, lr = 1e-05
I0508 00:00:12.617888  5077 solver.cpp:337] Iteration 8200, Testing net (#0)
I0508 00:00:24.414454  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 00:00:43.107684  5077 solver.cpp:404]     Test net output #0: loss = 0.241935 (* 1 = 0.241935 loss)
I0508 00:00:43.972945  5077 solver.cpp:228] Iteration 8200, loss = 0.0779605
I0508 00:00:43.973027  5077 solver.cpp:244]     Train net output #0: loss = 0.0779607 (* 1 = 0.0779607 loss)
I0508 00:00:43.973040  5077 sgd_solver.cpp:106] Iteration 8200, lr = 1e-05
I0508 00:02:53.790321  5077 solver.cpp:228] Iteration 8250, loss = 0.170643
I0508 00:02:53.790555  5077 solver.cpp:244]     Train net output #0: loss = 0.170643 (* 1 = 0.170643 loss)
I0508 00:02:53.790621  5077 sgd_solver.cpp:106] Iteration 8250, lr = 1e-05
I0508 00:05:03.919479  5077 solver.cpp:228] Iteration 8300, loss = -1.44355e-07
I0508 00:05:03.919706  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 00:05:03.919742  5077 sgd_solver.cpp:106] Iteration 8300, lr = 1e-05
I0508 00:07:13.797701  5077 solver.cpp:228] Iteration 8350, loss = 0.0473906
I0508 00:07:13.797941  5077 solver.cpp:244]     Train net output #0: loss = 0.0473907 (* 1 = 0.0473907 loss)
I0508 00:07:13.797979  5077 sgd_solver.cpp:106] Iteration 8350, lr = 1e-05
I0508 00:09:21.165087  5077 solver.cpp:337] Iteration 8400, Testing net (#0)
I0508 00:09:52.007938  5077 solver.cpp:404]     Test net output #0: loss = 0.24112 (* 1 = 0.24112 loss)
I0508 00:09:52.876855  5077 solver.cpp:228] Iteration 8400, loss = -1.2666e-07
I0508 00:09:52.876929  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 00:09:52.876948  5077 sgd_solver.cpp:106] Iteration 8400, lr = 1e-05
I0508 00:12:02.906705  5077 solver.cpp:228] Iteration 8450, loss = 0.0544465
I0508 00:12:02.907312  5077 solver.cpp:244]     Train net output #0: loss = 0.0544467 (* 1 = 0.0544467 loss)
I0508 00:12:02.907330  5077 sgd_solver.cpp:106] Iteration 8450, lr = 1e-05
I0508 00:14:12.806494  5077 solver.cpp:228] Iteration 8500, loss = 0.120561
I0508 00:14:12.807945  5077 solver.cpp:244]     Train net output #0: loss = 0.120561 (* 1 = 0.120561 loss)
I0508 00:14:12.807972  5077 sgd_solver.cpp:106] Iteration 8500, lr = 1e-05
I0508 00:16:22.930876  5077 solver.cpp:228] Iteration 8550, loss = 0.0880736
I0508 00:16:22.934079  5077 solver.cpp:244]     Train net output #0: loss = 0.0880737 (* 1 = 0.0880737 loss)
I0508 00:16:22.934096  5077 sgd_solver.cpp:106] Iteration 8550, lr = 1e-05
I0508 00:18:30.069677  5077 solver.cpp:337] Iteration 8600, Testing net (#0)
I0508 00:19:12.570180  5077 solver.cpp:404]     Test net output #0: loss = 0.311094 (* 1 = 0.311094 loss)
I0508 00:19:13.433068  5077 solver.cpp:228] Iteration 8600, loss = 0.200439
I0508 00:19:13.433138  5077 solver.cpp:244]     Train net output #0: loss = 0.20044 (* 1 = 0.20044 loss)
I0508 00:19:13.433151  5077 sgd_solver.cpp:106] Iteration 8600, lr = 1e-05
I0508 00:21:23.566519  5077 solver.cpp:228] Iteration 8650, loss = 0.164157
I0508 00:21:23.567054  5077 solver.cpp:244]     Train net output #0: loss = 0.164157 (* 1 = 0.164157 loss)
I0508 00:21:23.567093  5077 sgd_solver.cpp:106] Iteration 8650, lr = 1e-05
I0508 00:23:33.521385  5077 solver.cpp:228] Iteration 8700, loss = 0.0668692
I0508 00:23:33.521550  5077 solver.cpp:244]     Train net output #0: loss = 0.0668694 (* 1 = 0.0668694 loss)
I0508 00:23:33.521569  5077 sgd_solver.cpp:106] Iteration 8700, lr = 1e-05
I0508 00:25:43.373504  5077 solver.cpp:228] Iteration 8750, loss = 0.552205
I0508 00:25:43.373687  5077 solver.cpp:244]     Train net output #0: loss = 0.552206 (* 1 = 0.552206 loss)
I0508 00:25:43.373708  5077 sgd_solver.cpp:106] Iteration 8750, lr = 1e-05
I0508 00:27:50.963749  5077 solver.cpp:337] Iteration 8800, Testing net (#0)
I0508 00:27:54.116097  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 00:28:41.030462  5077 solver.cpp:404]     Test net output #0: loss = 0.18803 (* 1 = 0.18803 loss)
I0508 00:28:41.892357  5077 solver.cpp:228] Iteration 8800, loss = 0.227741
I0508 00:28:41.892432  5077 solver.cpp:244]     Train net output #0: loss = 0.227742 (* 1 = 0.227742 loss)
I0508 00:28:41.892446  5077 sgd_solver.cpp:106] Iteration 8800, lr = 1e-05
I0508 00:30:52.321593  5077 solver.cpp:228] Iteration 8850, loss = 0.138866
I0508 00:30:52.322163  5077 solver.cpp:244]     Train net output #0: loss = 0.138866 (* 1 = 0.138866 loss)
I0508 00:30:52.322180  5077 sgd_solver.cpp:106] Iteration 8850, lr = 1e-05
I0508 00:33:02.199688  5077 solver.cpp:228] Iteration 8900, loss = 0.273315
I0508 00:33:02.201643  5077 solver.cpp:244]     Train net output #0: loss = 0.273315 (* 1 = 0.273315 loss)
I0508 00:33:02.201683  5077 sgd_solver.cpp:106] Iteration 8900, lr = 1e-05
I0508 00:35:12.205348  5077 solver.cpp:228] Iteration 8950, loss = 0.0462746
I0508 00:35:12.205798  5077 solver.cpp:244]     Train net output #0: loss = 0.0462748 (* 1 = 0.0462748 loss)
I0508 00:35:12.205873  5077 sgd_solver.cpp:106] Iteration 8950, lr = 1e-05
I0508 00:37:19.416860  5077 solver.cpp:337] Iteration 9000, Testing net (#0)
I0508 00:37:49.956601  5077 solver.cpp:404]     Test net output #0: loss = 0.162008 (* 1 = 0.162008 loss)
I0508 00:37:50.822649  5077 solver.cpp:228] Iteration 9000, loss = 0.0259343
I0508 00:37:50.822753  5077 solver.cpp:244]     Train net output #0: loss = 0.0259346 (* 1 = 0.0259346 loss)
I0508 00:37:50.822780  5077 sgd_solver.cpp:106] Iteration 9000, lr = 1e-05
I0508 00:40:00.804680  5077 solver.cpp:228] Iteration 9050, loss = 0.741884
I0508 00:40:00.804924  5077 solver.cpp:244]     Train net output #0: loss = 0.741885 (* 1 = 0.741885 loss)
I0508 00:40:00.804958  5077 sgd_solver.cpp:106] Iteration 9050, lr = 1e-05
I0508 00:42:10.789966  5077 solver.cpp:228] Iteration 9100, loss = 0.067653
I0508 00:42:10.790436  5077 solver.cpp:244]     Train net output #0: loss = 0.0676532 (* 1 = 0.0676532 loss)
I0508 00:42:10.790467  5077 sgd_solver.cpp:106] Iteration 9100, lr = 1e-05
I0508 00:44:20.802295  5077 solver.cpp:228] Iteration 9150, loss = 0.0317475
I0508 00:44:20.802525  5077 solver.cpp:244]     Train net output #0: loss = 0.0317477 (* 1 = 0.0317477 loss)
I0508 00:44:20.802561  5077 sgd_solver.cpp:106] Iteration 9150, lr = 1e-05
I0508 00:46:28.388726  5077 solver.cpp:337] Iteration 9200, Testing net (#0)
I0508 00:46:40.318810  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 00:47:15.137440  5077 solver.cpp:404]     Test net output #0: loss = 0.315032 (* 1 = 0.315032 loss)
I0508 00:47:16.015022  5077 solver.cpp:228] Iteration 9200, loss = 0.192079
I0508 00:47:16.015127  5077 solver.cpp:244]     Train net output #0: loss = 0.192079 (* 1 = 0.192079 loss)
I0508 00:47:16.015146  5077 sgd_solver.cpp:106] Iteration 9200, lr = 1e-05
I0508 00:49:26.610787  5077 solver.cpp:228] Iteration 9250, loss = 0.329952
I0508 00:49:26.611048  5077 solver.cpp:244]     Train net output #0: loss = 0.329952 (* 1 = 0.329952 loss)
I0508 00:49:26.611105  5077 sgd_solver.cpp:106] Iteration 9250, lr = 1e-05
I0508 00:51:36.540978  5077 solver.cpp:228] Iteration 9300, loss = 0.191973
I0508 00:51:36.541199  5077 solver.cpp:244]     Train net output #0: loss = 0.191973 (* 1 = 0.191973 loss)
I0508 00:51:36.541237  5077 sgd_solver.cpp:106] Iteration 9300, lr = 1e-05
I0508 00:53:46.252354  5077 solver.cpp:228] Iteration 9350, loss = 0.060683
I0508 00:53:46.253427  5077 solver.cpp:244]     Train net output #0: loss = 0.0606833 (* 1 = 0.0606833 loss)
I0508 00:53:46.253465  5077 sgd_solver.cpp:106] Iteration 9350, lr = 1e-05
I0508 00:55:53.774847  5077 solver.cpp:337] Iteration 9400, Testing net (#0)
I0508 00:56:48.743367  5077 solver.cpp:404]     Test net output #0: loss = 0.223363 (* 1 = 0.223363 loss)
I0508 00:56:49.602533  5077 solver.cpp:228] Iteration 9400, loss = 0.0300298
I0508 00:56:49.602609  5077 solver.cpp:244]     Train net output #0: loss = 0.03003 (* 1 = 0.03003 loss)
I0508 00:56:49.602624  5077 sgd_solver.cpp:106] Iteration 9400, lr = 1e-05
I0508 00:58:59.836025  5077 solver.cpp:228] Iteration 9450, loss = 0.000457361
I0508 00:58:59.836268  5077 solver.cpp:244]     Train net output #0: loss = 0.000457605 (* 1 = 0.000457605 loss)
I0508 00:58:59.836308  5077 sgd_solver.cpp:106] Iteration 9450, lr = 1e-05
I0508 01:01:09.916211  5077 solver.cpp:228] Iteration 9500, loss = -2.38419e-07
I0508 01:01:09.916384  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 01:01:09.916404  5077 sgd_solver.cpp:106] Iteration 9500, lr = 1e-05
I0508 01:03:19.712195  5077 solver.cpp:228] Iteration 9550, loss = 0.0441083
I0508 01:03:19.712440  5077 solver.cpp:244]     Train net output #0: loss = 0.0441085 (* 1 = 0.0441085 loss)
I0508 01:03:19.712479  5077 sgd_solver.cpp:106] Iteration 9550, lr = 1e-05
I0508 01:05:27.254050  5077 solver.cpp:337] Iteration 9600, Testing net (#0)
I0508 01:05:43.476049  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 01:06:22.030297  5077 solver.cpp:404]     Test net output #0: loss = 0.168338 (* 1 = 0.168338 loss)
I0508 01:06:22.892025  5077 solver.cpp:228] Iteration 9600, loss = 0.0504503
I0508 01:06:22.892113  5077 solver.cpp:244]     Train net output #0: loss = 0.0504505 (* 1 = 0.0504505 loss)
I0508 01:06:22.892141  5077 sgd_solver.cpp:106] Iteration 9600, lr = 1e-05
I0508 01:08:33.218135  5077 solver.cpp:228] Iteration 9650, loss = 0.0825342
I0508 01:08:33.218385  5077 solver.cpp:244]     Train net output #0: loss = 0.0825345 (* 1 = 0.0825345 loss)
I0508 01:08:33.218412  5077 sgd_solver.cpp:106] Iteration 9650, lr = 1e-05
I0508 01:10:43.453163  5077 solver.cpp:228] Iteration 9700, loss = 0.0261757
I0508 01:10:43.453336  5077 solver.cpp:244]     Train net output #0: loss = 0.0261759 (* 1 = 0.0261759 loss)
I0508 01:10:43.453364  5077 sgd_solver.cpp:106] Iteration 9700, lr = 1e-05
I0508 01:12:53.434200  5077 solver.cpp:228] Iteration 9750, loss = 0.0990008
I0508 01:12:53.434413  5077 solver.cpp:244]     Train net output #0: loss = 0.099001 (* 1 = 0.099001 loss)
I0508 01:12:53.434440  5077 sgd_solver.cpp:106] Iteration 9750, lr = 1e-05
I0508 01:15:00.805447  5077 solver.cpp:337] Iteration 9800, Testing net (#0)
I0508 01:15:50.885978  5077 solver.cpp:404]     Test net output #0: loss = 0.148972 (* 1 = 0.148972 loss)
I0508 01:15:51.755539  5077 solver.cpp:228] Iteration 9800, loss = 0.0849934
I0508 01:15:51.755622  5077 solver.cpp:244]     Train net output #0: loss = 0.0849936 (* 1 = 0.0849936 loss)
I0508 01:15:51.755637  5077 sgd_solver.cpp:106] Iteration 9800, lr = 1e-05
I0508 01:18:02.078644  5077 solver.cpp:228] Iteration 9850, loss = 0.0023897
I0508 01:18:02.091188  5077 solver.cpp:244]     Train net output #0: loss = 0.00238995 (* 1 = 0.00238995 loss)
I0508 01:18:02.091228  5077 sgd_solver.cpp:106] Iteration 9850, lr = 1e-05
I0508 01:20:12.092967  5077 solver.cpp:228] Iteration 9900, loss = 0.0384903
I0508 01:20:12.094876  5077 solver.cpp:244]     Train net output #0: loss = 0.0384906 (* 1 = 0.0384906 loss)
I0508 01:20:12.094928  5077 sgd_solver.cpp:106] Iteration 9900, lr = 1e-05
I0508 01:22:22.314513  5077 solver.cpp:228] Iteration 9950, loss = 0.0468159
I0508 01:22:22.315080  5077 solver.cpp:244]     Train net output #0: loss = 0.0468162 (* 1 = 0.0468162 loss)
I0508 01:22:22.315140  5077 sgd_solver.cpp:106] Iteration 9950, lr = 1e-05
I0508 01:24:29.844960  5077 solver.cpp:454] Snapshotting to binary proto file ../../weights/three_stream_triplet_loss_iter_10000.caffemodel
I0508 01:24:38.495628  5077 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../../weights/three_stream_triplet_loss_iter_10000.solverstate
I0508 01:24:38.547477  5077 solver.cpp:337] Iteration 10000, Testing net (#0)
I0508 01:24:53.766898  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 01:25:27.483791  5077 solver.cpp:404]     Test net output #0: loss = 0.155415 (* 1 = 0.155415 loss)
I0508 01:25:28.344305  5077 solver.cpp:228] Iteration 10000, loss = 0.196704
I0508 01:25:28.344379  5077 solver.cpp:244]     Train net output #0: loss = 0.196704 (* 1 = 0.196704 loss)
I0508 01:25:28.344393  5077 sgd_solver.cpp:106] Iteration 10000, lr = 1e-05
I0508 01:27:38.528934  5077 solver.cpp:228] Iteration 10050, loss = 0.0735258
I0508 01:27:38.529201  5077 solver.cpp:244]     Train net output #0: loss = 0.0735261 (* 1 = 0.0735261 loss)
I0508 01:27:38.529227  5077 sgd_solver.cpp:106] Iteration 10050, lr = 1e-05
I0508 01:29:48.587716  5077 solver.cpp:228] Iteration 10100, loss = 0.0715373
I0508 01:29:48.587978  5077 solver.cpp:244]     Train net output #0: loss = 0.0715375 (* 1 = 0.0715375 loss)
I0508 01:29:48.588029  5077 sgd_solver.cpp:106] Iteration 10100, lr = 1e-05
I0508 01:31:58.509707  5077 solver.cpp:228] Iteration 10150, loss = 0.115204
I0508 01:31:58.509940  5077 solver.cpp:244]     Train net output #0: loss = 0.115204 (* 1 = 0.115204 loss)
I0508 01:31:58.509981  5077 sgd_solver.cpp:106] Iteration 10150, lr = 1e-05
I0508 01:34:05.844787  5077 solver.cpp:337] Iteration 10200, Testing net (#0)
I0508 01:34:56.362951  5077 solver.cpp:404]     Test net output #0: loss = 0.230422 (* 1 = 0.230422 loss)
I0508 01:34:57.231071  5077 solver.cpp:228] Iteration 10200, loss = 0.0573696
I0508 01:34:57.231129  5077 solver.cpp:244]     Train net output #0: loss = 0.0573699 (* 1 = 0.0573699 loss)
I0508 01:34:57.231144  5077 sgd_solver.cpp:106] Iteration 10200, lr = 1e-05
I0508 01:37:07.668409  5077 solver.cpp:228] Iteration 10250, loss = 0.0648063
I0508 01:37:07.668642  5077 solver.cpp:244]     Train net output #0: loss = 0.0648067 (* 1 = 0.0648067 loss)
I0508 01:37:07.668689  5077 sgd_solver.cpp:106] Iteration 10250, lr = 1e-05
I0508 01:39:17.712586  5077 solver.cpp:228] Iteration 10300, loss = 0.105962
I0508 01:39:17.717221  5077 solver.cpp:244]     Train net output #0: loss = 0.105962 (* 1 = 0.105962 loss)
I0508 01:39:17.717236  5077 sgd_solver.cpp:106] Iteration 10300, lr = 1e-05
I0508 01:41:27.889796  5077 solver.cpp:228] Iteration 10350, loss = 0.11921
I0508 01:41:27.891613  5077 solver.cpp:244]     Train net output #0: loss = 0.11921 (* 1 = 0.11921 loss)
I0508 01:41:27.891634  5077 sgd_solver.cpp:106] Iteration 10350, lr = 1e-05
I0508 01:43:35.413378  5077 solver.cpp:337] Iteration 10400, Testing net (#0)
I0508 01:43:52.070657  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 01:44:11.166285  5077 solver.cpp:404]     Test net output #0: loss = 0.151294 (* 1 = 0.151294 loss)
I0508 01:44:12.031222  5077 solver.cpp:228] Iteration 10400, loss = 0.0636104
I0508 01:44:12.031286  5077 solver.cpp:244]     Train net output #0: loss = 0.0636108 (* 1 = 0.0636108 loss)
I0508 01:44:12.031302  5077 sgd_solver.cpp:106] Iteration 10400, lr = 1e-05
I0508 01:46:22.116955  5077 solver.cpp:228] Iteration 10450, loss = 0.105532
I0508 01:46:22.129452  5077 solver.cpp:244]     Train net output #0: loss = 0.105532 (* 1 = 0.105532 loss)
I0508 01:46:22.129479  5077 sgd_solver.cpp:106] Iteration 10450, lr = 1e-05
I0508 01:48:32.181032  5077 solver.cpp:228] Iteration 10500, loss = 0.0219008
I0508 01:48:32.182261  5077 solver.cpp:244]     Train net output #0: loss = 0.0219012 (* 1 = 0.0219012 loss)
I0508 01:48:32.182298  5077 sgd_solver.cpp:106] Iteration 10500, lr = 1e-05
I0508 01:50:42.162781  5077 solver.cpp:228] Iteration 10550, loss = 0.0300283
I0508 01:50:42.162979  5077 solver.cpp:244]     Train net output #0: loss = 0.0300287 (* 1 = 0.0300287 loss)
I0508 01:50:42.163007  5077 sgd_solver.cpp:106] Iteration 10550, lr = 1e-05
I0508 01:52:49.614307  5077 solver.cpp:337] Iteration 10600, Testing net (#0)
I0508 01:53:19.850126  5077 solver.cpp:404]     Test net output #0: loss = 0.150538 (* 1 = 0.150538 loss)
I0508 01:53:20.720685  5077 solver.cpp:228] Iteration 10600, loss = 0.0591996
I0508 01:53:20.720758  5077 solver.cpp:244]     Train net output #0: loss = 0.0592 (* 1 = 0.0592 loss)
I0508 01:53:20.720784  5077 sgd_solver.cpp:106] Iteration 10600, lr = 1e-05
I0508 01:55:30.651662  5077 solver.cpp:228] Iteration 10650, loss = 0.0222402
I0508 01:55:30.651899  5077 solver.cpp:244]     Train net output #0: loss = 0.0222405 (* 1 = 0.0222405 loss)
I0508 01:55:30.651928  5077 sgd_solver.cpp:106] Iteration 10650, lr = 1e-05
I0508 01:57:40.836606  5077 solver.cpp:228] Iteration 10700, loss = 0.019474
I0508 01:57:40.836799  5077 solver.cpp:244]     Train net output #0: loss = 0.0194743 (* 1 = 0.0194743 loss)
I0508 01:57:40.836825  5077 sgd_solver.cpp:106] Iteration 10700, lr = 1e-05
I0508 01:59:51.150110  5077 solver.cpp:228] Iteration 10750, loss = 0.0430116
I0508 01:59:51.151806  5077 solver.cpp:244]     Train net output #0: loss = 0.043012 (* 1 = 0.043012 loss)
I0508 01:59:51.151824  5077 sgd_solver.cpp:106] Iteration 10750, lr = 1e-05
I0508 02:01:58.434322  5077 solver.cpp:337] Iteration 10800, Testing net (#0)
I0508 02:02:28.789767  5077 solver.cpp:404]     Test net output #0: loss = 0.174811 (* 1 = 0.174811 loss)
I0508 02:02:29.662922  5077 solver.cpp:228] Iteration 10800, loss = 0.0610801
I0508 02:02:29.663125  5077 solver.cpp:244]     Train net output #0: loss = 0.0610805 (* 1 = 0.0610805 loss)
I0508 02:02:29.663172  5077 sgd_solver.cpp:106] Iteration 10800, lr = 1e-05
I0508 02:04:39.737658  5077 solver.cpp:228] Iteration 10850, loss = 0.173105
I0508 02:04:39.737895  5077 solver.cpp:244]     Train net output #0: loss = 0.173106 (* 1 = 0.173106 loss)
I0508 02:04:39.737913  5077 sgd_solver.cpp:106] Iteration 10850, lr = 1e-05
I0508 02:06:49.693859  5077 solver.cpp:228] Iteration 10900, loss = 0.0210472
I0508 02:06:49.696597  5077 solver.cpp:244]     Train net output #0: loss = 0.0210476 (* 1 = 0.0210476 loss)
I0508 02:06:49.696614  5077 sgd_solver.cpp:106] Iteration 10900, lr = 1e-05
I0508 02:08:59.706811  5077 solver.cpp:228] Iteration 10950, loss = 0.138072
I0508 02:08:59.707080  5077 solver.cpp:244]     Train net output #0: loss = 0.138072 (* 1 = 0.138072 loss)
I0508 02:08:59.707130  5077 sgd_solver.cpp:106] Iteration 10950, lr = 1e-05
I0508 02:11:07.251104  5077 solver.cpp:337] Iteration 11000, Testing net (#0)
I0508 02:11:37.528093  5077 solver.cpp:404]     Test net output #0: loss = 0.165848 (* 1 = 0.165848 loss)
I0508 02:11:38.397347  5077 solver.cpp:228] Iteration 11000, loss = 0.0535047
I0508 02:11:38.397424  5077 solver.cpp:244]     Train net output #0: loss = 0.053505 (* 1 = 0.053505 loss)
I0508 02:11:38.397438  5077 sgd_solver.cpp:106] Iteration 11000, lr = 1e-05
I0508 02:13:48.211813  5077 solver.cpp:228] Iteration 11050, loss = 0.0277413
I0508 02:13:48.212072  5077 solver.cpp:244]     Train net output #0: loss = 0.0277417 (* 1 = 0.0277417 loss)
I0508 02:13:48.212116  5077 sgd_solver.cpp:106] Iteration 11050, lr = 1e-05
I0508 02:15:58.076902  5077 solver.cpp:228] Iteration 11100, loss = 0.0872603
I0508 02:15:58.089473  5077 solver.cpp:244]     Train net output #0: loss = 0.0872607 (* 1 = 0.0872607 loss)
I0508 02:15:58.089521  5077 sgd_solver.cpp:106] Iteration 11100, lr = 1e-05
I0508 02:18:08.210378  5077 solver.cpp:228] Iteration 11150, loss = 0.056702
I0508 02:18:08.213240  5077 solver.cpp:244]     Train net output #0: loss = 0.0567022 (* 1 = 0.0567022 loss)
I0508 02:18:08.213281  5077 sgd_solver.cpp:106] Iteration 11150, lr = 1e-05
I0508 02:20:15.676765  5077 solver.cpp:337] Iteration 11200, Testing net (#0)
I0508 02:20:46.619420  5077 solver.cpp:404]     Test net output #0: loss = 0.182476 (* 1 = 0.182476 loss)
I0508 02:20:47.488057  5077 solver.cpp:228] Iteration 11200, loss = 0.128605
I0508 02:20:47.488144  5077 solver.cpp:244]     Train net output #0: loss = 0.128605 (* 1 = 0.128605 loss)
I0508 02:20:47.488158  5077 sgd_solver.cpp:106] Iteration 11200, lr = 1e-05
I0508 02:22:57.663095  5077 solver.cpp:228] Iteration 11250, loss = 0.0349109
I0508 02:22:57.664106  5077 solver.cpp:244]     Train net output #0: loss = 0.0349112 (* 1 = 0.0349112 loss)
I0508 02:22:57.664131  5077 sgd_solver.cpp:106] Iteration 11250, lr = 1e-05
I0508 02:25:07.745743  5077 solver.cpp:228] Iteration 11300, loss = 0.0193345
I0508 02:25:07.749579  5077 solver.cpp:244]     Train net output #0: loss = 0.0193347 (* 1 = 0.0193347 loss)
I0508 02:25:07.749598  5077 sgd_solver.cpp:106] Iteration 11300, lr = 1e-05
I0508 02:27:17.632936  5077 solver.cpp:228] Iteration 11350, loss = 0.198096
I0508 02:27:17.634850  5077 solver.cpp:244]     Train net output #0: loss = 0.198096 (* 1 = 0.198096 loss)
I0508 02:27:17.634869  5077 sgd_solver.cpp:106] Iteration 11350, lr = 1e-05
I0508 02:29:25.109844  5077 solver.cpp:337] Iteration 11400, Testing net (#0)
I0508 02:29:33.506116  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 02:30:00.121219  5077 solver.cpp:404]     Test net output #0: loss = 0.172365 (* 1 = 0.172365 loss)
I0508 02:30:00.991325  5077 solver.cpp:228] Iteration 11400, loss = 0.0259534
I0508 02:30:00.991400  5077 solver.cpp:244]     Train net output #0: loss = 0.0259537 (* 1 = 0.0259537 loss)
I0508 02:30:00.991422  5077 sgd_solver.cpp:106] Iteration 11400, lr = 1e-05
I0508 02:32:11.133544  5077 solver.cpp:228] Iteration 11450, loss = 0.124357
I0508 02:32:11.133713  5077 solver.cpp:244]     Train net output #0: loss = 0.124357 (* 1 = 0.124357 loss)
I0508 02:32:11.133728  5077 sgd_solver.cpp:106] Iteration 11450, lr = 1e-05
I0508 02:34:21.423228  5077 solver.cpp:228] Iteration 11500, loss = 0.450856
I0508 02:34:21.423602  5077 solver.cpp:244]     Train net output #0: loss = 0.450856 (* 1 = 0.450856 loss)
I0508 02:34:21.423637  5077 sgd_solver.cpp:106] Iteration 11500, lr = 1e-05
I0508 02:36:31.460860  5077 solver.cpp:228] Iteration 11550, loss = 0.0449118
I0508 02:36:31.461153  5077 solver.cpp:244]     Train net output #0: loss = 0.0449121 (* 1 = 0.0449121 loss)
I0508 02:36:31.461196  5077 sgd_solver.cpp:106] Iteration 11550, lr = 1e-05
I0508 02:38:38.912855  5077 solver.cpp:337] Iteration 11600, Testing net (#0)
I0508 02:39:26.158069  5077 solver.cpp:404]     Test net output #0: loss = 0.24254 (* 1 = 0.24254 loss)
I0508 02:39:27.026129  5077 solver.cpp:228] Iteration 11600, loss = 0.129344
I0508 02:39:27.026196  5077 solver.cpp:244]     Train net output #0: loss = 0.129344 (* 1 = 0.129344 loss)
I0508 02:39:27.026209  5077 sgd_solver.cpp:106] Iteration 11600, lr = 1e-05
I0508 02:41:37.650328  5077 solver.cpp:228] Iteration 11650, loss = 0.102069
I0508 02:41:37.650591  5077 solver.cpp:244]     Train net output #0: loss = 0.102069 (* 1 = 0.102069 loss)
I0508 02:41:37.650617  5077 sgd_solver.cpp:106] Iteration 11650, lr = 1e-05
I0508 02:43:47.745329  5077 solver.cpp:228] Iteration 11700, loss = 0.308326
I0508 02:43:47.745508  5077 solver.cpp:244]     Train net output #0: loss = 0.308326 (* 1 = 0.308326 loss)
I0508 02:43:47.745524  5077 sgd_solver.cpp:106] Iteration 11700, lr = 1e-05
I0508 02:45:57.375211  5077 solver.cpp:228] Iteration 11750, loss = 0.00124954
I0508 02:45:57.375439  5077 solver.cpp:244]     Train net output #0: loss = 0.00124977 (* 1 = 0.00124977 loss)
I0508 02:45:57.375465  5077 sgd_solver.cpp:106] Iteration 11750, lr = 1e-05
I0508 02:47:40.755200  5091 blocking_queue.cpp:50] Waiting for data
I0508 02:48:05.012115  5077 solver.cpp:337] Iteration 11800, Testing net (#0)
I0508 02:48:15.200692  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 02:48:38.181967  5077 solver.cpp:404]     Test net output #0: loss = 0.193067 (* 1 = 0.193067 loss)
I0508 02:48:39.047768  5077 solver.cpp:228] Iteration 11800, loss = 0.0244919
I0508 02:48:39.047844  5077 solver.cpp:244]     Train net output #0: loss = 0.0244921 (* 1 = 0.0244921 loss)
I0508 02:48:39.047858  5077 sgd_solver.cpp:106] Iteration 11800, lr = 1e-05
I0508 02:50:49.209532  5077 solver.cpp:228] Iteration 11850, loss = 0.114297
I0508 02:50:49.209731  5077 solver.cpp:244]     Train net output #0: loss = 0.114297 (* 1 = 0.114297 loss)
I0508 02:50:49.209756  5077 sgd_solver.cpp:106] Iteration 11850, lr = 1e-05
I0508 02:52:59.601593  5077 solver.cpp:228] Iteration 11900, loss = 0.030469
I0508 02:52:59.602085  5077 solver.cpp:244]     Train net output #0: loss = 0.0304691 (* 1 = 0.0304691 loss)
I0508 02:52:59.602103  5077 sgd_solver.cpp:106] Iteration 11900, lr = 1e-05
I0508 02:55:09.728971  5077 solver.cpp:228] Iteration 11950, loss = 0.086071
I0508 02:55:09.742111  5077 solver.cpp:244]     Train net output #0: loss = 0.0860712 (* 1 = 0.0860712 loss)
I0508 02:55:09.742147  5077 sgd_solver.cpp:106] Iteration 11950, lr = 1e-05
I0508 02:57:17.167956  5077 solver.cpp:454] Snapshotting to binary proto file ../../weights/three_stream_triplet_loss_iter_12000.caffemodel
I0508 02:57:25.711115  5077 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../../weights/three_stream_triplet_loss_iter_12000.solverstate
I0508 02:57:25.773465  5077 solver.cpp:337] Iteration 12000, Testing net (#0)
I0508 02:58:03.310822  5077 solver.cpp:404]     Test net output #0: loss = 0.175405 (* 1 = 0.175405 loss)
I0508 02:58:04.178447  5077 solver.cpp:228] Iteration 12000, loss = 0.145829
I0508 02:58:04.178511  5077 solver.cpp:244]     Train net output #0: loss = 0.145829 (* 1 = 0.145829 loss)
I0508 02:58:04.178530  5077 sgd_solver.cpp:106] Iteration 12000, lr = 1e-05
I0508 03:00:14.395781  5077 solver.cpp:228] Iteration 12050, loss = 0.00125922
I0508 03:00:14.397058  5077 solver.cpp:244]     Train net output #0: loss = 0.00125941 (* 1 = 0.00125941 loss)
I0508 03:00:14.397096  5077 sgd_solver.cpp:106] Iteration 12050, lr = 1e-05
I0508 03:02:24.040212  5077 solver.cpp:228] Iteration 12100, loss = 0.0297616
I0508 03:02:24.040398  5077 solver.cpp:244]     Train net output #0: loss = 0.0297618 (* 1 = 0.0297618 loss)
I0508 03:02:24.040415  5077 sgd_solver.cpp:106] Iteration 12100, lr = 1e-05
I0508 03:04:34.322458  5077 solver.cpp:228] Iteration 12150, loss = 0.0387724
I0508 03:04:34.322614  5077 solver.cpp:244]     Train net output #0: loss = 0.0387726 (* 1 = 0.0387726 loss)
I0508 03:04:34.322638  5077 sgd_solver.cpp:106] Iteration 12150, lr = 1e-05
I0508 03:06:41.556991  5077 solver.cpp:337] Iteration 12200, Testing net (#0)
I0508 03:07:11.761658  5077 solver.cpp:404]     Test net output #0: loss = 0.166823 (* 1 = 0.166823 loss)
I0508 03:07:12.626551  5077 solver.cpp:228] Iteration 12200, loss = 0.130829
I0508 03:07:12.626662  5077 solver.cpp:244]     Train net output #0: loss = 0.130829 (* 1 = 0.130829 loss)
I0508 03:07:12.626682  5077 sgd_solver.cpp:106] Iteration 12200, lr = 1e-05
I0508 03:09:22.505683  5077 solver.cpp:228] Iteration 12250, loss = 0.0319385
I0508 03:09:22.506726  5077 solver.cpp:244]     Train net output #0: loss = 0.0319388 (* 1 = 0.0319388 loss)
I0508 03:09:22.506762  5077 sgd_solver.cpp:106] Iteration 12250, lr = 1e-05
I0508 03:11:32.494107  5077 solver.cpp:228] Iteration 12300, loss = 0.0497252
I0508 03:11:32.494374  5077 solver.cpp:244]     Train net output #0: loss = 0.0497254 (* 1 = 0.0497254 loss)
I0508 03:11:32.494426  5077 sgd_solver.cpp:106] Iteration 12300, lr = 1e-05
I0508 03:13:42.087637  5077 solver.cpp:228] Iteration 12350, loss = 0.324345
I0508 03:13:42.087860  5077 solver.cpp:244]     Train net output #0: loss = 0.324345 (* 1 = 0.324345 loss)
I0508 03:13:42.087898  5077 sgd_solver.cpp:106] Iteration 12350, lr = 1e-05
I0508 03:15:49.753460  5077 solver.cpp:337] Iteration 12400, Testing net (#0)
I0508 03:16:04.199620  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 03:16:23.152102  5077 solver.cpp:404]     Test net output #0: loss = 0.136614 (* 1 = 0.136614 loss)
I0508 03:16:24.018950  5077 solver.cpp:228] Iteration 12400, loss = -2.38419e-07
I0508 03:16:24.019021  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 03:16:24.019042  5077 sgd_solver.cpp:106] Iteration 12400, lr = 1e-05
I0508 03:18:33.535095  5077 solver.cpp:228] Iteration 12450, loss = 0.123943
I0508 03:18:33.535279  5077 solver.cpp:244]     Train net output #0: loss = 0.123943 (* 1 = 0.123943 loss)
I0508 03:18:33.535313  5077 sgd_solver.cpp:106] Iteration 12450, lr = 1e-05
I0508 03:20:43.480722  5077 solver.cpp:228] Iteration 12500, loss = 0.014834
I0508 03:20:43.480893  5077 solver.cpp:244]     Train net output #0: loss = 0.0148342 (* 1 = 0.0148342 loss)
I0508 03:20:43.480911  5077 sgd_solver.cpp:106] Iteration 12500, lr = 1e-05
I0508 03:22:53.398444  5077 solver.cpp:228] Iteration 12550, loss = 0.270945
I0508 03:22:53.398634  5077 solver.cpp:244]     Train net output #0: loss = 0.270946 (* 1 = 0.270946 loss)
I0508 03:22:53.398650  5077 sgd_solver.cpp:106] Iteration 12550, lr = 1e-05
I0508 03:25:00.558082  5077 solver.cpp:337] Iteration 12600, Testing net (#0)
I0508 03:25:31.536926  5077 solver.cpp:404]     Test net output #0: loss = 0.182472 (* 1 = 0.182472 loss)
I0508 03:25:32.405654  5077 solver.cpp:228] Iteration 12600, loss = 0.110859
I0508 03:25:32.405731  5077 solver.cpp:244]     Train net output #0: loss = 0.11086 (* 1 = 0.11086 loss)
I0508 03:25:32.405746  5077 sgd_solver.cpp:106] Iteration 12600, lr = 1e-05
I0508 03:27:42.314133  5077 solver.cpp:228] Iteration 12650, loss = 0.0101683
I0508 03:27:42.314406  5077 solver.cpp:244]     Train net output #0: loss = 0.0101686 (* 1 = 0.0101686 loss)
I0508 03:27:42.314445  5077 sgd_solver.cpp:106] Iteration 12650, lr = 1e-05
I0508 03:29:51.805233  5077 solver.cpp:228] Iteration 12700, loss = 0.00682962
I0508 03:29:51.805405  5077 solver.cpp:244]     Train net output #0: loss = 0.00682974 (* 1 = 0.00682974 loss)
I0508 03:29:51.805420  5077 sgd_solver.cpp:106] Iteration 12700, lr = 1e-05
I0508 03:32:01.731724  5077 solver.cpp:228] Iteration 12750, loss = 0.079615
I0508 03:32:01.734616  5077 solver.cpp:244]     Train net output #0: loss = 0.0796151 (* 1 = 0.0796151 loss)
I0508 03:32:01.734633  5077 sgd_solver.cpp:106] Iteration 12750, lr = 1e-05
I0508 03:34:08.897431  5077 solver.cpp:337] Iteration 12800, Testing net (#0)
I0508 03:34:38.806665  5077 solver.cpp:404]     Test net output #0: loss = 0.139127 (* 1 = 0.139127 loss)
I0508 03:34:39.675686  5077 solver.cpp:228] Iteration 12800, loss = 0.0366803
I0508 03:34:39.675918  5077 solver.cpp:244]     Train net output #0: loss = 0.0366804 (* 1 = 0.0366804 loss)
I0508 03:34:39.675947  5077 sgd_solver.cpp:106] Iteration 12800, lr = 1e-05
I0508 03:36:49.119879  5077 solver.cpp:228] Iteration 12850, loss = -1.19209e-07
I0508 03:36:49.120055  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 03:36:49.120074  5077 sgd_solver.cpp:106] Iteration 12850, lr = 1e-05
I0508 03:38:58.948688  5077 solver.cpp:228] Iteration 12900, loss = -1.21072e-07
I0508 03:38:58.952090  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 03:38:58.952133  5077 sgd_solver.cpp:106] Iteration 12900, lr = 1e-05
I0508 03:41:08.739935  5077 solver.cpp:228] Iteration 12950, loss = 0.0590565
I0508 03:41:08.740103  5077 solver.cpp:244]     Train net output #0: loss = 0.0590566 (* 1 = 0.0590566 loss)
I0508 03:41:08.740118  5077 sgd_solver.cpp:106] Iteration 12950, lr = 1e-05
I0508 03:43:16.063571  5077 solver.cpp:337] Iteration 13000, Testing net (#0)
I0508 03:43:46.630031  5077 solver.cpp:404]     Test net output #0: loss = 0.14589 (* 1 = 0.14589 loss)
I0508 03:43:47.512567  5077 solver.cpp:228] Iteration 13000, loss = -1.04308e-07
I0508 03:43:47.512641  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 03:43:47.512655  5077 sgd_solver.cpp:106] Iteration 13000, lr = 1e-05
I0508 03:45:57.321777  5077 solver.cpp:228] Iteration 13050, loss = 0.144573
I0508 03:45:57.321936  5077 solver.cpp:244]     Train net output #0: loss = 0.144573 (* 1 = 0.144573 loss)
I0508 03:45:57.321954  5077 sgd_solver.cpp:106] Iteration 13050, lr = 1e-05
I0508 03:48:06.758885  5077 solver.cpp:228] Iteration 13100, loss = 0.00096333
I0508 03:48:06.759068  5077 solver.cpp:244]     Train net output #0: loss = 0.000963483 (* 1 = 0.000963483 loss)
I0508 03:48:06.759089  5077 sgd_solver.cpp:106] Iteration 13100, lr = 1e-05
I0508 03:50:16.314314  5077 solver.cpp:228] Iteration 13150, loss = 0.0336653
I0508 03:50:16.314497  5077 solver.cpp:244]     Train net output #0: loss = 0.0336654 (* 1 = 0.0336654 loss)
I0508 03:50:16.314517  5077 sgd_solver.cpp:106] Iteration 13150, lr = 1e-05
I0508 03:52:23.493294  5077 solver.cpp:337] Iteration 13200, Testing net (#0)
I0508 03:52:53.142523  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 03:52:53.486680  5077 solver.cpp:404]     Test net output #0: loss = 0.152063 (* 1 = 0.152063 loss)
I0508 03:52:54.350342  5077 solver.cpp:228] Iteration 13200, loss = -1.15484e-07
I0508 03:52:54.350499  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 03:52:54.350527  5077 sgd_solver.cpp:106] Iteration 13200, lr = 1e-05
I0508 03:55:04.245713  5077 solver.cpp:228] Iteration 13250, loss = 0.0839281
I0508 03:55:04.245880  5077 solver.cpp:244]     Train net output #0: loss = 0.0839281 (* 1 = 0.0839281 loss)
I0508 03:55:04.245901  5077 sgd_solver.cpp:106] Iteration 13250, lr = 1e-05
I0508 03:57:14.080747  5077 solver.cpp:228] Iteration 13300, loss = -5.68107e-08
I0508 03:57:14.080934  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 03:57:14.080956  5077 sgd_solver.cpp:106] Iteration 13300, lr = 1e-05
I0508 03:59:23.754428  5077 solver.cpp:228] Iteration 13350, loss = 0.297768
I0508 03:59:23.754613  5077 solver.cpp:244]     Train net output #0: loss = 0.297768 (* 1 = 0.297768 loss)
I0508 03:59:23.754631  5077 sgd_solver.cpp:106] Iteration 13350, lr = 1e-05
I0508 04:01:31.102152  5077 solver.cpp:337] Iteration 13400, Testing net (#0)
I0508 04:02:01.157788  5077 solver.cpp:404]     Test net output #0: loss = 0.218445 (* 1 = 0.218445 loss)
I0508 04:02:02.026525  5077 solver.cpp:228] Iteration 13400, loss = 0.331311
I0508 04:02:02.026608  5077 solver.cpp:244]     Train net output #0: loss = 0.331311 (* 1 = 0.331311 loss)
I0508 04:02:02.026621  5077 sgd_solver.cpp:106] Iteration 13400, lr = 1e-05
I0508 04:04:11.847198  5077 solver.cpp:228] Iteration 13450, loss = 0.0488814
I0508 04:04:11.847376  5077 solver.cpp:244]     Train net output #0: loss = 0.0488814 (* 1 = 0.0488814 loss)
I0508 04:04:11.847393  5077 sgd_solver.cpp:106] Iteration 13450, lr = 1e-05
I0508 04:06:21.471304  5077 solver.cpp:228] Iteration 13500, loss = 0.115555
I0508 04:06:21.471555  5077 solver.cpp:244]     Train net output #0: loss = 0.115555 (* 1 = 0.115555 loss)
I0508 04:06:21.471592  5077 sgd_solver.cpp:106] Iteration 13500, lr = 1e-05
I0508 04:08:30.959161  5077 solver.cpp:228] Iteration 13550, loss = 0.192972
I0508 04:08:30.960430  5077 solver.cpp:244]     Train net output #0: loss = 0.192972 (* 1 = 0.192972 loss)
I0508 04:08:30.960448  5077 sgd_solver.cpp:106] Iteration 13550, lr = 1e-05
I0508 04:10:38.033625  5077 solver.cpp:337] Iteration 13600, Testing net (#0)
I0508 04:11:08.165680  5077 solver.cpp:404]     Test net output #0: loss = 0.189662 (* 1 = 0.189662 loss)
I0508 04:11:09.036617  5077 solver.cpp:228] Iteration 13600, loss = 0.0683243
I0508 04:11:09.036695  5077 solver.cpp:244]     Train net output #0: loss = 0.0683244 (* 1 = 0.0683244 loss)
I0508 04:11:09.036710  5077 sgd_solver.cpp:106] Iteration 13600, lr = 1e-05
I0508 04:13:18.923796  5077 solver.cpp:228] Iteration 13650, loss = -8.56817e-08
I0508 04:13:18.924021  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 04:13:18.924055  5077 sgd_solver.cpp:106] Iteration 13650, lr = 1e-05
I0508 04:15:28.823729  5077 solver.cpp:228] Iteration 13700, loss = 0.0779241
I0508 04:15:28.824038  5077 solver.cpp:244]     Train net output #0: loss = 0.0779242 (* 1 = 0.0779242 loss)
I0508 04:15:28.824082  5077 sgd_solver.cpp:106] Iteration 13700, lr = 1e-05
I0508 04:17:38.505903  5077 solver.cpp:228] Iteration 13750, loss = 0.074865
I0508 04:17:38.506098  5077 solver.cpp:244]     Train net output #0: loss = 0.0748652 (* 1 = 0.0748652 loss)
I0508 04:17:38.506114  5077 sgd_solver.cpp:106] Iteration 13750, lr = 1e-05
I0508 04:19:45.754488  5077 solver.cpp:337] Iteration 13800, Testing net (#0)
I0508 04:20:16.838815  5077 solver.cpp:404]     Test net output #0: loss = 0.243104 (* 1 = 0.243104 loss)
I0508 04:20:17.707557  5077 solver.cpp:228] Iteration 13800, loss = 0.0455018
I0508 04:20:17.707620  5077 solver.cpp:244]     Train net output #0: loss = 0.045502 (* 1 = 0.045502 loss)
I0508 04:20:17.707643  5077 sgd_solver.cpp:106] Iteration 13800, lr = 1e-05
I0508 04:22:27.421221  5077 solver.cpp:228] Iteration 13850, loss = 0.0427437
I0508 04:22:27.423929  5077 solver.cpp:244]     Train net output #0: loss = 0.0427438 (* 1 = 0.0427438 loss)
I0508 04:22:27.423959  5077 sgd_solver.cpp:106] Iteration 13850, lr = 1e-05
I0508 04:24:36.993075  5077 solver.cpp:228] Iteration 13900, loss = 0.172067
I0508 04:24:36.993281  5077 solver.cpp:244]     Train net output #0: loss = 0.172067 (* 1 = 0.172067 loss)
I0508 04:24:36.993299  5077 sgd_solver.cpp:106] Iteration 13900, lr = 1e-05
I0508 04:26:46.575043  5077 solver.cpp:228] Iteration 13950, loss = 0.0480482
I0508 04:26:46.575206  5077 solver.cpp:244]     Train net output #0: loss = 0.0480483 (* 1 = 0.0480483 loss)
I0508 04:26:46.575230  5077 sgd_solver.cpp:106] Iteration 13950, lr = 1e-05
I0508 04:28:53.538745  5077 solver.cpp:454] Snapshotting to binary proto file ../../weights/three_stream_triplet_loss_iter_14000.caffemodel
I0508 04:28:56.635251  5077 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../../weights/three_stream_triplet_loss_iter_14000.solverstate
I0508 04:28:56.709197  5077 solver.cpp:337] Iteration 14000, Testing net (#0)
I0508 04:29:19.152338  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 04:29:25.018818  5077 solver.cpp:404]     Test net output #0: loss = 0.13813 (* 1 = 0.13813 loss)
I0508 04:29:25.888448  5077 solver.cpp:228] Iteration 14000, loss = -1.78814e-07
I0508 04:29:25.888542  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 04:29:25.888556  5077 sgd_solver.cpp:106] Iteration 14000, lr = 1e-05
I0508 04:31:35.839869  5077 solver.cpp:228] Iteration 14050, loss = 0.0585296
I0508 04:31:35.842061  5077 solver.cpp:244]     Train net output #0: loss = 0.0585298 (* 1 = 0.0585298 loss)
I0508 04:31:35.842092  5077 sgd_solver.cpp:106] Iteration 14050, lr = 1e-05
I0508 04:33:45.488955  5077 solver.cpp:228] Iteration 14100, loss = 0.0216659
I0508 04:33:45.489182  5077 solver.cpp:244]     Train net output #0: loss = 0.0216661 (* 1 = 0.0216661 loss)
I0508 04:33:45.489226  5077 sgd_solver.cpp:106] Iteration 14100, lr = 1e-05
I0508 04:35:55.130648  5077 solver.cpp:228] Iteration 14150, loss = -2.08616e-07
I0508 04:35:55.134384  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 04:35:55.134402  5077 sgd_solver.cpp:106] Iteration 14150, lr = 1e-05
I0508 04:38:02.129911  5077 solver.cpp:337] Iteration 14200, Testing net (#0)
I0508 04:38:32.527629  5077 solver.cpp:404]     Test net output #0: loss = 0.164581 (* 1 = 0.164581 loss)
I0508 04:38:33.393187  5077 solver.cpp:228] Iteration 14200, loss = 0.0349943
I0508 04:38:33.393254  5077 solver.cpp:244]     Train net output #0: loss = 0.0349945 (* 1 = 0.0349945 loss)
I0508 04:38:33.393267  5077 sgd_solver.cpp:106] Iteration 14200, lr = 1e-05
I0508 04:40:43.033720  5077 solver.cpp:228] Iteration 14250, loss = 0.00919586
I0508 04:40:43.033910  5077 solver.cpp:244]     Train net output #0: loss = 0.00919605 (* 1 = 0.00919605 loss)
I0508 04:40:43.033926  5077 sgd_solver.cpp:106] Iteration 14250, lr = 1e-05
I0508 04:42:52.754032  5077 solver.cpp:228] Iteration 14300, loss = 0.0104945
I0508 04:42:52.767897  5077 solver.cpp:244]     Train net output #0: loss = 0.0104947 (* 1 = 0.0104947 loss)
I0508 04:42:52.767937  5077 sgd_solver.cpp:106] Iteration 14300, lr = 1e-05
I0508 04:45:02.416394  5077 solver.cpp:228] Iteration 14350, loss = -1.78814e-07
I0508 04:45:02.416589  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 04:45:02.416609  5077 sgd_solver.cpp:106] Iteration 14350, lr = 1e-05
I0508 04:47:09.532754  5077 solver.cpp:337] Iteration 14400, Testing net (#0)
I0508 04:47:39.684198  5077 solver.cpp:404]     Test net output #0: loss = 0.145576 (* 1 = 0.145576 loss)
I0508 04:47:40.552598  5077 solver.cpp:228] Iteration 14400, loss = 0.0198604
I0508 04:47:40.552673  5077 solver.cpp:244]     Train net output #0: loss = 0.0198606 (* 1 = 0.0198606 loss)
I0508 04:47:40.552688  5077 sgd_solver.cpp:106] Iteration 14400, lr = 1e-05
I0508 04:49:50.239816  5077 solver.cpp:228] Iteration 14450, loss = 0.073376
I0508 04:49:50.240002  5077 solver.cpp:244]     Train net output #0: loss = 0.0733762 (* 1 = 0.0733762 loss)
I0508 04:49:50.240020  5077 sgd_solver.cpp:106] Iteration 14450, lr = 1e-05
I0508 04:51:59.806368  5077 solver.cpp:228] Iteration 14500, loss = 0.223915
I0508 04:51:59.806623  5077 solver.cpp:244]     Train net output #0: loss = 0.223915 (* 1 = 0.223915 loss)
I0508 04:51:59.806663  5077 sgd_solver.cpp:106] Iteration 14500, lr = 1e-05
I0508 04:54:09.508190  5077 solver.cpp:228] Iteration 14550, loss = 0.00853808
I0508 04:54:09.508455  5077 solver.cpp:244]     Train net output #0: loss = 0.00853826 (* 1 = 0.00853826 loss)
I0508 04:54:09.508492  5077 sgd_solver.cpp:106] Iteration 14550, lr = 1e-05
I0508 04:56:16.281698  5077 solver.cpp:337] Iteration 14600, Testing net (#0)
I0508 04:56:34.465833  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 04:56:47.065636  5077 solver.cpp:404]     Test net output #0: loss = 0.215489 (* 1 = 0.215489 loss)
I0508 04:56:47.931720  5077 solver.cpp:228] Iteration 14600, loss = 0.0281864
I0508 04:56:47.931793  5077 solver.cpp:244]     Train net output #0: loss = 0.0281865 (* 1 = 0.0281865 loss)
I0508 04:56:47.931809  5077 sgd_solver.cpp:106] Iteration 14600, lr = 1e-05
I0508 04:58:57.673301  5077 solver.cpp:228] Iteration 14650, loss = 0.0324011
I0508 04:58:57.673913  5077 solver.cpp:244]     Train net output #0: loss = 0.0324012 (* 1 = 0.0324012 loss)
I0508 04:58:57.673945  5077 sgd_solver.cpp:106] Iteration 14650, lr = 1e-05
I0508 05:01:07.346797  5077 solver.cpp:228] Iteration 14700, loss = 0.0116536
I0508 05:01:07.346973  5077 solver.cpp:244]     Train net output #0: loss = 0.0116537 (* 1 = 0.0116537 loss)
I0508 05:01:07.346989  5077 sgd_solver.cpp:106] Iteration 14700, lr = 1e-05
I0508 05:03:16.937295  5077 solver.cpp:228] Iteration 14750, loss = -1.3411e-07
I0508 05:03:16.937727  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 05:03:16.937757  5077 sgd_solver.cpp:106] Iteration 14750, lr = 1e-05
I0508 05:05:24.116653  5077 solver.cpp:337] Iteration 14800, Testing net (#0)
I0508 05:05:54.922260  5077 solver.cpp:404]     Test net output #0: loss = 0.121838 (* 1 = 0.121838 loss)
I0508 05:05:55.786450  5077 solver.cpp:228] Iteration 14800, loss = 0.0831338
I0508 05:05:55.786551  5077 solver.cpp:244]     Train net output #0: loss = 0.083134 (* 1 = 0.083134 loss)
I0508 05:05:55.786576  5077 sgd_solver.cpp:106] Iteration 14800, lr = 1e-05
I0508 05:08:05.189435  5077 solver.cpp:228] Iteration 14850, loss = -1.3411e-07
I0508 05:08:05.189643  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 05:08:05.189668  5077 sgd_solver.cpp:106] Iteration 14850, lr = 1e-05
I0508 05:10:14.718528  5077 solver.cpp:228] Iteration 14900, loss = 0.276228
I0508 05:10:14.718711  5077 solver.cpp:244]     Train net output #0: loss = 0.276228 (* 1 = 0.276228 loss)
I0508 05:10:14.718729  5077 sgd_solver.cpp:106] Iteration 14900, lr = 1e-05
I0508 05:12:24.437873  5077 solver.cpp:228] Iteration 14950, loss = -8.9407e-08
I0508 05:12:24.450629  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 05:12:24.450677  5077 sgd_solver.cpp:106] Iteration 14950, lr = 1e-05
I0508 05:14:31.177862  5077 solver.cpp:337] Iteration 15000, Testing net (#0)
I0508 05:15:01.245527  5077 solver.cpp:404]     Test net output #0: loss = 0.155311 (* 1 = 0.155311 loss)
I0508 05:15:02.112891  5077 solver.cpp:228] Iteration 15000, loss = 0.00758435
I0508 05:15:02.112967  5077 solver.cpp:244]     Train net output #0: loss = 0.00758444 (* 1 = 0.00758444 loss)
I0508 05:15:02.112982  5077 sgd_solver.cpp:106] Iteration 15000, lr = 1e-05
I0508 05:17:11.840296  5077 solver.cpp:228] Iteration 15050, loss = -5.96046e-08
I0508 05:17:11.840487  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 05:17:11.840512  5077 sgd_solver.cpp:106] Iteration 15050, lr = 1e-05
I0508 05:19:21.486178  5077 solver.cpp:228] Iteration 15100, loss = 0.0799065
I0508 05:19:21.486414  5077 solver.cpp:244]     Train net output #0: loss = 0.0799066 (* 1 = 0.0799066 loss)
I0508 05:19:21.486449  5077 sgd_solver.cpp:106] Iteration 15100, lr = 1e-05
I0508 05:21:31.045464  5077 solver.cpp:228] Iteration 15150, loss = -5.58794e-08
I0508 05:21:31.047118  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 05:21:31.047137  5077 sgd_solver.cpp:106] Iteration 15150, lr = 1e-05
I0508 05:23:38.283805  5077 solver.cpp:337] Iteration 15200, Testing net (#0)
I0508 05:24:01.355347  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 05:24:08.483606  5077 solver.cpp:404]     Test net output #0: loss = 0.19443 (* 1 = 0.19443 loss)
I0508 05:24:09.349820  5077 solver.cpp:228] Iteration 15200, loss = 0.0877534
I0508 05:24:09.349887  5077 solver.cpp:244]     Train net output #0: loss = 0.0877535 (* 1 = 0.0877535 loss)
I0508 05:24:09.349901  5077 sgd_solver.cpp:106] Iteration 15200, lr = 1e-05
I0508 05:26:18.610972  5077 solver.cpp:228] Iteration 15250, loss = 0.067353
I0508 05:26:18.611232  5077 solver.cpp:244]     Train net output #0: loss = 0.0673531 (* 1 = 0.0673531 loss)
I0508 05:26:18.611263  5077 sgd_solver.cpp:106] Iteration 15250, lr = 1e-05
I0508 05:28:28.028549  5077 solver.cpp:228] Iteration 15300, loss = 0.0766547
I0508 05:28:28.029527  5077 solver.cpp:244]     Train net output #0: loss = 0.0766547 (* 1 = 0.0766547 loss)
I0508 05:28:28.029554  5077 sgd_solver.cpp:106] Iteration 15300, lr = 1e-05
I0508 05:30:37.805923  5077 solver.cpp:228] Iteration 15350, loss = 0.138148
I0508 05:30:37.806172  5077 solver.cpp:244]     Train net output #0: loss = 0.138148 (* 1 = 0.138148 loss)
I0508 05:30:37.806203  5077 sgd_solver.cpp:106] Iteration 15350, lr = 1e-05
I0508 05:32:44.561373  5077 solver.cpp:337] Iteration 15400, Testing net (#0)
I0508 05:33:14.978279  5077 solver.cpp:404]     Test net output #0: loss = 0.174388 (* 1 = 0.174388 loss)
I0508 05:33:15.845721  5077 solver.cpp:228] Iteration 15400, loss = 0.0944398
I0508 05:33:15.845823  5077 solver.cpp:244]     Train net output #0: loss = 0.0944398 (* 1 = 0.0944398 loss)
I0508 05:33:15.845854  5077 sgd_solver.cpp:106] Iteration 15400, lr = 1e-05
I0508 05:35:25.477962  5077 solver.cpp:228] Iteration 15450, loss = 0.0965924
I0508 05:35:25.478155  5077 solver.cpp:244]     Train net output #0: loss = 0.0965925 (* 1 = 0.0965925 loss)
I0508 05:35:25.478171  5077 sgd_solver.cpp:106] Iteration 15450, lr = 1e-05
I0508 05:37:35.162477  5077 solver.cpp:228] Iteration 15500, loss = 0.262912
I0508 05:37:35.162719  5077 solver.cpp:244]     Train net output #0: loss = 0.262912 (* 1 = 0.262912 loss)
I0508 05:37:35.162765  5077 sgd_solver.cpp:106] Iteration 15500, lr = 1e-05
I0508 05:39:44.774118  5077 solver.cpp:228] Iteration 15550, loss = 0.0939191
I0508 05:39:44.774518  5077 solver.cpp:244]     Train net output #0: loss = 0.0939193 (* 1 = 0.0939193 loss)
I0508 05:39:44.774546  5077 sgd_solver.cpp:106] Iteration 15550, lr = 1e-05
I0508 05:41:51.730607  5077 solver.cpp:337] Iteration 15600, Testing net (#0)
I0508 05:42:22.464216  5077 solver.cpp:404]     Test net output #0: loss = 0.132949 (* 1 = 0.132949 loss)
I0508 05:42:23.334758  5077 solver.cpp:228] Iteration 15600, loss = 0.107624
I0508 05:42:23.334807  5077 solver.cpp:244]     Train net output #0: loss = 0.107624 (* 1 = 0.107624 loss)
I0508 05:42:23.334821  5077 sgd_solver.cpp:106] Iteration 15600, lr = 1e-05
I0508 05:44:32.424151  5077 solver.cpp:228] Iteration 15650, loss = 0.0527794
I0508 05:44:32.424335  5077 solver.cpp:244]     Train net output #0: loss = 0.0527795 (* 1 = 0.0527795 loss)
I0508 05:44:32.424360  5077 sgd_solver.cpp:106] Iteration 15650, lr = 1e-05
I0508 05:46:42.108919  5077 solver.cpp:228] Iteration 15700, loss = 0.0133832
I0508 05:46:42.110304  5077 solver.cpp:244]     Train net output #0: loss = 0.0133834 (* 1 = 0.0133834 loss)
I0508 05:46:42.110327  5077 sgd_solver.cpp:106] Iteration 15700, lr = 1e-05
I0508 05:48:51.712587  5077 solver.cpp:228] Iteration 15750, loss = 0.166052
I0508 05:48:51.712785  5077 solver.cpp:244]     Train net output #0: loss = 0.166052 (* 1 = 0.166052 loss)
I0508 05:48:51.712801  5077 sgd_solver.cpp:106] Iteration 15750, lr = 1e-05
I0508 05:50:58.567468  5077 solver.cpp:337] Iteration 15800, Testing net (#0)
I0508 05:51:18.677779  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 05:51:28.679829  5077 solver.cpp:404]     Test net output #0: loss = 0.154836 (* 1 = 0.154836 loss)
I0508 05:51:29.548321  5077 solver.cpp:228] Iteration 15800, loss = 0.119897
I0508 05:51:29.548398  5077 solver.cpp:244]     Train net output #0: loss = 0.119897 (* 1 = 0.119897 loss)
I0508 05:51:29.548411  5077 sgd_solver.cpp:106] Iteration 15800, lr = 1e-05
I0508 05:53:39.309702  5077 solver.cpp:228] Iteration 15850, loss = 0.173645
I0508 05:53:39.309888  5077 solver.cpp:244]     Train net output #0: loss = 0.173645 (* 1 = 0.173645 loss)
I0508 05:53:39.309909  5077 sgd_solver.cpp:106] Iteration 15850, lr = 1e-05
I0508 05:55:48.649693  5077 solver.cpp:228] Iteration 15900, loss = -1.04308e-07
I0508 05:55:48.649886  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 05:55:48.649907  5077 sgd_solver.cpp:106] Iteration 15900, lr = 1e-05
I0508 05:57:58.147558  5077 solver.cpp:228] Iteration 15950, loss = -1.49012e-07
I0508 05:57:58.147789  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 05:57:58.147821  5077 sgd_solver.cpp:106] Iteration 15950, lr = 1e-05
I0508 06:00:05.279613  5077 solver.cpp:454] Snapshotting to binary proto file ../../weights/three_stream_triplet_loss_iter_16000.caffemodel
I0508 06:00:08.286432  5077 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../../weights/three_stream_triplet_loss_iter_16000.solverstate
I0508 06:00:08.350781  5077 solver.cpp:337] Iteration 16000, Testing net (#0)
I0508 06:00:37.202471  5077 solver.cpp:404]     Test net output #0: loss = 0.127104 (* 1 = 0.127104 loss)
I0508 06:00:38.071306  5077 solver.cpp:228] Iteration 16000, loss = 0.250355
I0508 06:00:38.071482  5077 solver.cpp:244]     Train net output #0: loss = 0.250355 (* 1 = 0.250355 loss)
I0508 06:00:38.071503  5077 sgd_solver.cpp:106] Iteration 16000, lr = 1e-05
I0508 06:02:47.504910  5077 solver.cpp:228] Iteration 16050, loss = 0.0772367
I0508 06:02:47.505388  5077 solver.cpp:244]     Train net output #0: loss = 0.0772368 (* 1 = 0.0772368 loss)
I0508 06:02:47.505424  5077 sgd_solver.cpp:106] Iteration 16050, lr = 1e-05
I0508 06:04:57.404294  5077 solver.cpp:228] Iteration 16100, loss = 0.0344733
I0508 06:04:57.404901  5077 solver.cpp:244]     Train net output #0: loss = 0.0344734 (* 1 = 0.0344734 loss)
I0508 06:04:57.404935  5077 sgd_solver.cpp:106] Iteration 16100, lr = 1e-05
I0508 06:07:06.735909  5077 solver.cpp:228] Iteration 16150, loss = 0.0427542
I0508 06:07:06.736084  5077 solver.cpp:244]     Train net output #0: loss = 0.0427544 (* 1 = 0.0427544 loss)
I0508 06:07:06.736104  5077 sgd_solver.cpp:106] Iteration 16150, lr = 1e-05
I0508 06:09:13.723381  5077 solver.cpp:337] Iteration 16200, Testing net (#0)
I0508 06:09:44.512732  5077 solver.cpp:404]     Test net output #0: loss = 0.150822 (* 1 = 0.150822 loss)
I0508 06:09:45.390988  5077 solver.cpp:228] Iteration 16200, loss = 0.0193778
I0508 06:09:45.391062  5077 solver.cpp:244]     Train net output #0: loss = 0.019378 (* 1 = 0.019378 loss)
I0508 06:09:45.391078  5077 sgd_solver.cpp:106] Iteration 16200, lr = 1e-05
I0508 06:11:54.851851  5077 solver.cpp:228] Iteration 16250, loss = 0.261047
I0508 06:11:54.852900  5077 solver.cpp:244]     Train net output #0: loss = 0.261047 (* 1 = 0.261047 loss)
I0508 06:11:54.852928  5077 sgd_solver.cpp:106] Iteration 16250, lr = 1e-05
I0508 06:14:04.121847  5077 solver.cpp:228] Iteration 16300, loss = 0.0161773
I0508 06:14:04.122103  5077 solver.cpp:244]     Train net output #0: loss = 0.0161775 (* 1 = 0.0161775 loss)
I0508 06:14:04.122131  5077 sgd_solver.cpp:106] Iteration 16300, lr = 1e-05
I0508 06:16:13.870352  5077 solver.cpp:228] Iteration 16350, loss = 0.0536517
I0508 06:16:13.870607  5077 solver.cpp:244]     Train net output #0: loss = 0.0536519 (* 1 = 0.0536519 loss)
I0508 06:16:13.870645  5077 sgd_solver.cpp:106] Iteration 16350, lr = 1e-05
I0508 06:18:20.723263  5077 solver.cpp:337] Iteration 16400, Testing net (#0)
I0508 06:18:39.450793  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 06:18:51.443024  5077 solver.cpp:404]     Test net output #0: loss = 0.142781 (* 1 = 0.142781 loss)
I0508 06:18:52.311286  5077 solver.cpp:228] Iteration 16400, loss = 0.132425
I0508 06:18:52.311374  5077 solver.cpp:244]     Train net output #0: loss = 0.132425 (* 1 = 0.132425 loss)
I0508 06:18:52.311388  5077 sgd_solver.cpp:106] Iteration 16400, lr = 1e-05
I0508 06:21:02.157920  5077 solver.cpp:228] Iteration 16450, loss = 0.000620276
I0508 06:21:02.158630  5077 solver.cpp:244]     Train net output #0: loss = 0.000620455 (* 1 = 0.000620455 loss)
I0508 06:21:02.158674  5077 sgd_solver.cpp:106] Iteration 16450, lr = 1e-05
I0508 06:23:11.789887  5077 solver.cpp:228] Iteration 16500, loss = 0.141909
I0508 06:23:11.790066  5077 solver.cpp:244]     Train net output #0: loss = 0.141909 (* 1 = 0.141909 loss)
I0508 06:23:11.790082  5077 sgd_solver.cpp:106] Iteration 16500, lr = 1e-05
I0508 06:25:21.157981  5077 solver.cpp:228] Iteration 16550, loss = 0.00299995
I0508 06:25:21.158231  5077 solver.cpp:244]     Train net output #0: loss = 0.00300019 (* 1 = 0.00300019 loss)
I0508 06:25:21.158257  5077 sgd_solver.cpp:106] Iteration 16550, lr = 1e-05
I0508 06:27:28.497495  5077 solver.cpp:337] Iteration 16600, Testing net (#0)
I0508 06:27:58.765743  5077 solver.cpp:404]     Test net output #0: loss = 0.13911 (* 1 = 0.13911 loss)
I0508 06:27:59.633661  5077 solver.cpp:228] Iteration 16600, loss = 0.0277295
I0508 06:27:59.633720  5077 solver.cpp:244]     Train net output #0: loss = 0.0277297 (* 1 = 0.0277297 loss)
I0508 06:27:59.633733  5077 sgd_solver.cpp:106] Iteration 16600, lr = 1e-05
I0508 06:30:09.077872  5077 solver.cpp:228] Iteration 16650, loss = -2.5332e-07
I0508 06:30:09.079840  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 06:30:09.079859  5077 sgd_solver.cpp:106] Iteration 16650, lr = 1e-05
I0508 06:32:18.367748  5077 solver.cpp:228] Iteration 16700, loss = 0.404097
I0508 06:32:18.367960  5077 solver.cpp:244]     Train net output #0: loss = 0.404097 (* 1 = 0.404097 loss)
I0508 06:32:18.368006  5077 sgd_solver.cpp:106] Iteration 16700, lr = 1e-05
I0508 06:34:27.909373  5077 solver.cpp:228] Iteration 16750, loss = 0.0789123
I0508 06:34:27.910130  5077 solver.cpp:244]     Train net output #0: loss = 0.0789125 (* 1 = 0.0789125 loss)
I0508 06:34:27.910150  5077 sgd_solver.cpp:106] Iteration 16750, lr = 1e-05
I0508 06:36:34.835644  5077 solver.cpp:337] Iteration 16800, Testing net (#0)
I0508 06:37:03.599140  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 06:37:05.191321  5077 solver.cpp:404]     Test net output #0: loss = 0.178621 (* 1 = 0.178621 loss)
I0508 06:37:06.061894  5077 solver.cpp:228] Iteration 16800, loss = 0.114406
I0508 06:37:06.062036  5077 solver.cpp:244]     Train net output #0: loss = 0.114406 (* 1 = 0.114406 loss)
I0508 06:37:06.062059  5077 sgd_solver.cpp:106] Iteration 16800, lr = 1e-05
I0508 06:39:15.973907  5077 solver.cpp:228] Iteration 16850, loss = 0.0593812
I0508 06:39:15.986315  5077 solver.cpp:244]     Train net output #0: loss = 0.0593814 (* 1 = 0.0593814 loss)
I0508 06:39:15.986333  5077 sgd_solver.cpp:106] Iteration 16850, lr = 1e-05
I0508 06:41:25.615273  5077 solver.cpp:228] Iteration 16900, loss = 0.0917463
I0508 06:41:25.618072  5077 solver.cpp:244]     Train net output #0: loss = 0.0917465 (* 1 = 0.0917465 loss)
I0508 06:41:25.618109  5077 sgd_solver.cpp:106] Iteration 16900, lr = 1e-05
I0508 06:43:35.108649  5077 solver.cpp:228] Iteration 16950, loss = 0.0717791
I0508 06:43:35.109174  5077 solver.cpp:244]     Train net output #0: loss = 0.0717794 (* 1 = 0.0717794 loss)
I0508 06:43:35.109191  5077 sgd_solver.cpp:106] Iteration 16950, lr = 1e-05
I0508 06:45:42.259640  5077 solver.cpp:337] Iteration 17000, Testing net (#0)
I0508 06:46:12.356326  5077 solver.cpp:404]     Test net output #0: loss = 0.153066 (* 1 = 0.153066 loss)
I0508 06:46:13.223839  5077 solver.cpp:228] Iteration 17000, loss = 0.0761423
I0508 06:46:13.223934  5077 solver.cpp:244]     Train net output #0: loss = 0.0761426 (* 1 = 0.0761426 loss)
I0508 06:46:13.223955  5077 sgd_solver.cpp:106] Iteration 17000, lr = 1e-05
I0508 06:48:22.636339  5077 solver.cpp:228] Iteration 17050, loss = 0.0530917
I0508 06:48:22.636747  5077 solver.cpp:244]     Train net output #0: loss = 0.053092 (* 1 = 0.053092 loss)
I0508 06:48:22.636765  5077 sgd_solver.cpp:106] Iteration 17050, lr = 1e-05
I0508 06:50:32.135958  5077 solver.cpp:228] Iteration 17100, loss = 0.0705199
I0508 06:50:32.136255  5077 solver.cpp:244]     Train net output #0: loss = 0.0705202 (* 1 = 0.0705202 loss)
I0508 06:50:32.136296  5077 sgd_solver.cpp:106] Iteration 17100, lr = 1e-05
I0508 06:52:41.723338  5077 solver.cpp:228] Iteration 17150, loss = 0.0422311
I0508 06:52:41.723513  5077 solver.cpp:244]     Train net output #0: loss = 0.0422314 (* 1 = 0.0422314 loss)
I0508 06:52:41.723532  5077 sgd_solver.cpp:106] Iteration 17150, lr = 1e-05
I0508 06:54:48.758448  5077 solver.cpp:337] Iteration 17200, Testing net (#0)
I0508 06:55:18.991502  5077 solver.cpp:404]     Test net output #0: loss = 0.142689 (* 1 = 0.142689 loss)
I0508 06:55:19.858114  5077 solver.cpp:228] Iteration 17200, loss = -3.8743e-07
I0508 06:55:19.858194  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 06:55:19.858211  5077 sgd_solver.cpp:106] Iteration 17200, lr = 1e-05
I0508 06:57:29.841359  5077 solver.cpp:228] Iteration 17250, loss = -3.8743e-07
I0508 06:57:29.841616  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 06:57:29.841644  5077 sgd_solver.cpp:106] Iteration 17250, lr = 1e-05
I0508 06:59:39.433169  5077 solver.cpp:228] Iteration 17300, loss = 0.0214528
I0508 06:59:39.433365  5077 solver.cpp:244]     Train net output #0: loss = 0.0214532 (* 1 = 0.0214532 loss)
I0508 06:59:39.433387  5077 sgd_solver.cpp:106] Iteration 17300, lr = 1e-05
I0508 07:01:49.035549  5077 solver.cpp:228] Iteration 17350, loss = -4.02331e-07
I0508 07:01:49.036717  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 07:01:49.036736  5077 sgd_solver.cpp:106] Iteration 17350, lr = 1e-05
I0508 07:03:56.138226  5077 solver.cpp:337] Iteration 17400, Testing net (#0)
I0508 07:04:26.625555  5077 solver.cpp:404]     Test net output #0: loss = 0.121238 (* 1 = 0.121238 loss)
I0508 07:04:27.488555  5077 solver.cpp:228] Iteration 17400, loss = 0.0967165
I0508 07:04:27.488627  5077 solver.cpp:244]     Train net output #0: loss = 0.0967169 (* 1 = 0.0967169 loss)
I0508 07:04:27.488641  5077 sgd_solver.cpp:106] Iteration 17400, lr = 1e-05
I0508 07:06:36.997817  5077 solver.cpp:228] Iteration 17450, loss = -4.47035e-07
I0508 07:06:36.998059  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 07:06:36.998090  5077 sgd_solver.cpp:106] Iteration 17450, lr = 1e-05
I0508 07:08:46.710304  5077 solver.cpp:228] Iteration 17500, loss = 0.0435278
I0508 07:08:46.722924  5077 solver.cpp:244]     Train net output #0: loss = 0.0435282 (* 1 = 0.0435282 loss)
I0508 07:08:46.722959  5077 sgd_solver.cpp:106] Iteration 17500, lr = 1e-05
I0508 07:10:56.409782  5077 solver.cpp:228] Iteration 17550, loss = 0.31888
I0508 07:10:56.410115  5077 solver.cpp:244]     Train net output #0: loss = 0.318881 (* 1 = 0.318881 loss)
I0508 07:10:56.410151  5077 sgd_solver.cpp:106] Iteration 17550, lr = 1e-05
I0508 07:13:03.513607  5077 solver.cpp:337] Iteration 17600, Testing net (#0)
I0508 07:13:20.212435  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 07:13:33.915381  5077 solver.cpp:404]     Test net output #0: loss = 0.1339 (* 1 = 0.1339 loss)
I0508 07:13:34.788336  5077 solver.cpp:228] Iteration 17600, loss = 0.00604117
I0508 07:13:34.788434  5077 solver.cpp:244]     Train net output #0: loss = 0.00604155 (* 1 = 0.00604155 loss)
I0508 07:13:34.788455  5077 sgd_solver.cpp:106] Iteration 17600, lr = 1e-05
I0508 07:15:44.669047  5077 solver.cpp:228] Iteration 17650, loss = 0.245698
I0508 07:15:44.669221  5077 solver.cpp:244]     Train net output #0: loss = 0.245698 (* 1 = 0.245698 loss)
I0508 07:15:44.669236  5077 sgd_solver.cpp:106] Iteration 17650, lr = 1e-05
I0508 07:17:54.102221  5077 solver.cpp:228] Iteration 17700, loss = 0.0736436
I0508 07:17:54.102779  5077 solver.cpp:244]     Train net output #0: loss = 0.073644 (* 1 = 0.073644 loss)
I0508 07:17:54.102826  5077 sgd_solver.cpp:106] Iteration 17700, lr = 1e-05
I0508 07:20:03.797035  5077 solver.cpp:228] Iteration 17750, loss = 0.133136
I0508 07:20:03.797338  5077 solver.cpp:244]     Train net output #0: loss = 0.133136 (* 1 = 0.133136 loss)
I0508 07:20:03.797391  5077 sgd_solver.cpp:106] Iteration 17750, lr = 1e-05
I0508 07:22:10.993314  5077 solver.cpp:337] Iteration 17800, Testing net (#0)
I0508 07:22:40.988431  5077 solver.cpp:404]     Test net output #0: loss = 0.174668 (* 1 = 0.174668 loss)
I0508 07:22:41.855589  5077 solver.cpp:228] Iteration 17800, loss = 0.0751492
I0508 07:22:41.855845  5077 solver.cpp:244]     Train net output #0: loss = 0.0751496 (* 1 = 0.0751496 loss)
I0508 07:22:41.855872  5077 sgd_solver.cpp:106] Iteration 17800, lr = 1e-05
I0508 07:24:51.370573  5077 solver.cpp:228] Iteration 17850, loss = 0.039734
I0508 07:24:51.372886  5077 solver.cpp:244]     Train net output #0: loss = 0.0397344 (* 1 = 0.0397344 loss)
I0508 07:24:51.372903  5077 sgd_solver.cpp:106] Iteration 17850, lr = 1e-05
I0508 07:27:01.357261  5077 solver.cpp:228] Iteration 17900, loss = -3.42727e-07
I0508 07:27:01.357477  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 07:27:01.357518  5077 sgd_solver.cpp:106] Iteration 17900, lr = 1e-05
I0508 07:29:11.083750  5077 solver.cpp:228] Iteration 17950, loss = -3.65078e-07
I0508 07:29:11.083902  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 07:29:11.083917  5077 sgd_solver.cpp:106] Iteration 17950, lr = 1e-05
I0508 07:31:18.200347  5077 solver.cpp:454] Snapshotting to binary proto file ../../weights/three_stream_triplet_loss_iter_18000.caffemodel
I0508 07:31:22.737867  5077 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../../weights/three_stream_triplet_loss_iter_18000.solverstate
I0508 07:31:22.794116  5077 solver.cpp:337] Iteration 18000, Testing net (#0)
I0508 07:31:51.238390  5077 solver.cpp:404]     Test net output #0: loss = 0.13117 (* 1 = 0.13117 loss)
I0508 07:31:52.099807  5077 solver.cpp:228] Iteration 18000, loss = 0.0932321
I0508 07:31:52.099879  5077 solver.cpp:244]     Train net output #0: loss = 0.0932325 (* 1 = 0.0932325 loss)
I0508 07:31:52.099895  5077 sgd_solver.cpp:106] Iteration 18000, lr = 1e-05
I0508 07:34:01.424154  5077 solver.cpp:228] Iteration 18050, loss = 0.0594999
I0508 07:34:01.424337  5077 solver.cpp:244]     Train net output #0: loss = 0.0595003 (* 1 = 0.0595003 loss)
I0508 07:34:01.424353  5077 sgd_solver.cpp:106] Iteration 18050, lr = 1e-05
I0508 07:36:11.057073  5077 solver.cpp:228] Iteration 18100, loss = 0.0296178
I0508 07:36:11.057265  5077 solver.cpp:244]     Train net output #0: loss = 0.0296182 (* 1 = 0.0296182 loss)
I0508 07:36:11.057284  5077 sgd_solver.cpp:106] Iteration 18100, lr = 1e-05
I0508 07:38:20.782459  5077 solver.cpp:228] Iteration 18150, loss = 0.148174
I0508 07:38:20.782673  5077 solver.cpp:244]     Train net output #0: loss = 0.148175 (* 1 = 0.148175 loss)
I0508 07:38:20.782698  5077 sgd_solver.cpp:106] Iteration 18150, lr = 1e-05
I0508 07:40:27.802398  5077 solver.cpp:337] Iteration 18200, Testing net (#0)
I0508 07:40:57.656162  5077 solver.cpp:404]     Test net output #0: loss = 0.118563 (* 1 = 0.118563 loss)
I0508 07:40:58.525255  5077 solver.cpp:228] Iteration 18200, loss = 0.173711
I0508 07:40:58.525445  5077 solver.cpp:244]     Train net output #0: loss = 0.173711 (* 1 = 0.173711 loss)
I0508 07:40:58.525463  5077 sgd_solver.cpp:106] Iteration 18200, lr = 1e-05
I0508 07:43:08.634177  5077 solver.cpp:228] Iteration 18250, loss = 0.0462049
I0508 07:43:08.634342  5077 solver.cpp:244]     Train net output #0: loss = 0.0462053 (* 1 = 0.0462053 loss)
I0508 07:43:08.634363  5077 sgd_solver.cpp:106] Iteration 18250, lr = 1e-05
I0508 07:45:18.347913  5077 solver.cpp:228] Iteration 18300, loss = 0.121018
I0508 07:45:18.348119  5077 solver.cpp:244]     Train net output #0: loss = 0.121018 (* 1 = 0.121018 loss)
I0508 07:45:18.348155  5077 sgd_solver.cpp:106] Iteration 18300, lr = 1e-05
I0508 07:47:27.903785  5077 solver.cpp:228] Iteration 18350, loss = 0.144701
I0508 07:47:27.904009  5077 solver.cpp:244]     Train net output #0: loss = 0.144702 (* 1 = 0.144702 loss)
I0508 07:47:27.904036  5077 sgd_solver.cpp:106] Iteration 18350, lr = 1e-05
I0508 07:49:35.102890  5077 solver.cpp:337] Iteration 18400, Testing net (#0)
I0508 07:49:52.130679  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 07:50:05.276139  5077 solver.cpp:404]     Test net output #0: loss = 0.185369 (* 1 = 0.185369 loss)
I0508 07:50:06.139539  5077 solver.cpp:228] Iteration 18400, loss = 0.220812
I0508 07:50:06.139633  5077 solver.cpp:244]     Train net output #0: loss = 0.220813 (* 1 = 0.220813 loss)
I0508 07:50:06.139668  5077 sgd_solver.cpp:106] Iteration 18400, lr = 1e-05
I0508 07:52:15.564424  5077 solver.cpp:228] Iteration 18450, loss = 0.0428876
I0508 07:52:15.564579  5077 solver.cpp:244]     Train net output #0: loss = 0.0428881 (* 1 = 0.0428881 loss)
I0508 07:52:15.564599  5077 sgd_solver.cpp:106] Iteration 18450, lr = 1e-05
I0508 07:54:25.362576  5077 solver.cpp:228] Iteration 18500, loss = 0.110313
I0508 07:54:25.362826  5077 solver.cpp:244]     Train net output #0: loss = 0.110313 (* 1 = 0.110313 loss)
I0508 07:54:25.362851  5077 sgd_solver.cpp:106] Iteration 18500, lr = 1e-05
I0508 07:56:35.094250  5077 solver.cpp:228] Iteration 18550, loss = 0.0507418
I0508 07:56:35.094426  5077 solver.cpp:244]     Train net output #0: loss = 0.0507422 (* 1 = 0.0507422 loss)
I0508 07:56:35.094444  5077 sgd_solver.cpp:106] Iteration 18550, lr = 1e-05
I0508 07:58:41.956706  5077 solver.cpp:337] Iteration 18600, Testing net (#0)
I0508 07:59:12.054276  5077 solver.cpp:404]     Test net output #0: loss = 0.143377 (* 1 = 0.143377 loss)
I0508 07:59:12.921833  5077 solver.cpp:228] Iteration 18600, loss = -3.7998e-07
I0508 07:59:12.921921  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 07:59:12.921946  5077 sgd_solver.cpp:106] Iteration 18600, lr = 1e-05
I0508 08:01:22.883402  5077 solver.cpp:228] Iteration 18650, loss = 0.243132
I0508 08:01:22.885613  5077 solver.cpp:244]     Train net output #0: loss = 0.243132 (* 1 = 0.243132 loss)
I0508 08:01:22.885640  5077 sgd_solver.cpp:106] Iteration 18650, lr = 1e-05
I0508 08:03:32.657939  5077 solver.cpp:228] Iteration 18700, loss = -3.57628e-07
I0508 08:03:32.661092  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 08:03:32.661139  5077 sgd_solver.cpp:106] Iteration 18700, lr = 1e-05
I0508 08:05:42.118121  5077 solver.cpp:228] Iteration 18750, loss = -3.50177e-07
I0508 08:05:42.118386  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 08:05:42.118418  5077 sgd_solver.cpp:106] Iteration 18750, lr = 1e-05
I0508 08:07:49.449262  5077 solver.cpp:337] Iteration 18800, Testing net (#0)
I0508 08:08:19.389494  5077 solver.cpp:404]     Test net output #0: loss = 0.252507 (* 1 = 0.252507 loss)
I0508 08:08:20.255749  5077 solver.cpp:228] Iteration 18800, loss = 0.0907605
I0508 08:08:20.255945  5077 solver.cpp:244]     Train net output #0: loss = 0.0907608 (* 1 = 0.0907608 loss)
I0508 08:08:20.255962  5077 sgd_solver.cpp:106] Iteration 18800, lr = 1e-05
I0508 08:10:29.890468  5077 solver.cpp:228] Iteration 18850, loss = -3.12924e-07
I0508 08:10:29.890758  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 08:10:29.890789  5077 sgd_solver.cpp:106] Iteration 18850, lr = 1e-05
I0508 08:12:39.626798  5077 solver.cpp:228] Iteration 18900, loss = -2.83122e-07
I0508 08:12:39.627650  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 08:12:39.627679  5077 sgd_solver.cpp:106] Iteration 18900, lr = 1e-05
I0508 08:14:49.388770  5077 solver.cpp:228] Iteration 18950, loss = 0.0412963
I0508 08:14:49.389037  5077 solver.cpp:244]     Train net output #0: loss = 0.0412965 (* 1 = 0.0412965 loss)
I0508 08:14:49.389078  5077 sgd_solver.cpp:106] Iteration 18950, lr = 1e-05
I0508 08:16:56.256314  5077 solver.cpp:337] Iteration 19000, Testing net (#0)
I0508 08:17:27.080796  5077 solver.cpp:404]     Test net output #0: loss = 0.122279 (* 1 = 0.122279 loss)
I0508 08:17:27.947973  5077 solver.cpp:228] Iteration 19000, loss = 0.112429
I0508 08:17:27.948072  5077 solver.cpp:244]     Train net output #0: loss = 0.112429 (* 1 = 0.112429 loss)
I0508 08:17:27.948099  5077 sgd_solver.cpp:106] Iteration 19000, lr = 1e-05
I0508 08:19:37.881574  5077 solver.cpp:228] Iteration 19050, loss = 0.0555097
I0508 08:19:37.881816  5077 solver.cpp:244]     Train net output #0: loss = 0.05551 (* 1 = 0.05551 loss)
I0508 08:19:37.881844  5077 sgd_solver.cpp:106] Iteration 19050, lr = 1e-05
I0508 08:21:47.558665  5077 solver.cpp:228] Iteration 19100, loss = -2.08616e-07
I0508 08:21:47.559504  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 08:21:47.559522  5077 sgd_solver.cpp:106] Iteration 19100, lr = 1e-05
I0508 08:23:56.961825  5077 solver.cpp:228] Iteration 19150, loss = -2.40281e-07
I0508 08:23:56.962699  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 08:23:56.962733  5077 sgd_solver.cpp:106] Iteration 19150, lr = 1e-05
I0508 08:26:04.294936  5077 solver.cpp:337] Iteration 19200, Testing net (#0)
I0508 08:26:27.652333  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 08:26:34.796304  5077 solver.cpp:404]     Test net output #0: loss = 0.170278 (* 1 = 0.170278 loss)
I0508 08:26:35.668421  5077 solver.cpp:228] Iteration 19200, loss = 0.040207
I0508 08:26:35.668488  5077 solver.cpp:244]     Train net output #0: loss = 0.0402072 (* 1 = 0.0402072 loss)
I0508 08:26:35.668503  5077 sgd_solver.cpp:106] Iteration 19200, lr = 1e-05
I0508 08:28:45.407737  5077 solver.cpp:228] Iteration 19250, loss = 0.00247344
I0508 08:28:45.409265  5077 solver.cpp:244]     Train net output #0: loss = 0.00247369 (* 1 = 0.00247369 loss)
I0508 08:28:45.409317  5077 sgd_solver.cpp:106] Iteration 19250, lr = 1e-05
I0508 08:30:55.137051  5077 solver.cpp:228] Iteration 19300, loss = -2.38419e-07
I0508 08:30:55.137248  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 08:30:55.137264  5077 sgd_solver.cpp:106] Iteration 19300, lr = 1e-05
I0508 08:33:04.876505  5077 solver.cpp:228] Iteration 19350, loss = -1.93715e-07
I0508 08:33:04.878012  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 08:33:04.878051  5077 sgd_solver.cpp:106] Iteration 19350, lr = 1e-05
I0508 08:35:11.911715  5077 solver.cpp:337] Iteration 19400, Testing net (#0)
I0508 08:35:42.453272  5077 solver.cpp:404]     Test net output #0: loss = 0.146648 (* 1 = 0.146648 loss)
I0508 08:35:43.318982  5077 solver.cpp:228] Iteration 19400, loss = 0.0717022
I0508 08:35:43.319063  5077 solver.cpp:244]     Train net output #0: loss = 0.0717025 (* 1 = 0.0717025 loss)
I0508 08:35:43.319082  5077 sgd_solver.cpp:106] Iteration 19400, lr = 1e-05
I0508 08:37:52.966032  5077 solver.cpp:228] Iteration 19450, loss = 0.0222842
I0508 08:37:52.968307  5077 solver.cpp:244]     Train net output #0: loss = 0.0222844 (* 1 = 0.0222844 loss)
I0508 08:37:52.968353  5077 sgd_solver.cpp:106] Iteration 19450, lr = 1e-05
I0508 08:40:02.613045  5077 solver.cpp:228] Iteration 19500, loss = -2.68221e-07
I0508 08:40:02.619603  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 08:40:02.619624  5077 sgd_solver.cpp:106] Iteration 19500, lr = 1e-05
I0508 08:42:12.403609  5077 solver.cpp:228] Iteration 19550, loss = 0.0648075
I0508 08:42:12.403833  5077 solver.cpp:244]     Train net output #0: loss = 0.0648077 (* 1 = 0.0648077 loss)
I0508 08:42:12.403856  5077 sgd_solver.cpp:106] Iteration 19550, lr = 1e-05
I0508 08:44:19.693244  5077 solver.cpp:337] Iteration 19600, Testing net (#0)
I0508 08:44:48.423468  5077 blocking_queue.cpp:50] Data layer prefetch queue empty
I0508 08:44:50.127908  5077 solver.cpp:404]     Test net output #0: loss = 0.110059 (* 1 = 0.110059 loss)
I0508 08:44:50.999994  5077 solver.cpp:228] Iteration 19600, loss = 0.078224
I0508 08:44:51.000072  5077 solver.cpp:244]     Train net output #0: loss = 0.0782243 (* 1 = 0.0782243 loss)
I0508 08:44:51.000088  5077 sgd_solver.cpp:106] Iteration 19600, lr = 1e-05
I0508 08:47:00.559417  5077 solver.cpp:228] Iteration 19650, loss = 0.0813596
I0508 08:47:00.559952  5077 solver.cpp:244]     Train net output #0: loss = 0.0813598 (* 1 = 0.0813598 loss)
I0508 08:47:00.560011  5077 sgd_solver.cpp:106] Iteration 19650, lr = 1e-05
I0508 08:49:10.175248  5077 solver.cpp:228] Iteration 19700, loss = 0.0153348
I0508 08:49:10.177610  5077 solver.cpp:244]     Train net output #0: loss = 0.015335 (* 1 = 0.015335 loss)
I0508 08:49:10.177661  5077 sgd_solver.cpp:106] Iteration 19700, lr = 1e-05
I0508 08:51:19.729187  5077 solver.cpp:228] Iteration 19750, loss = -2.38419e-07
I0508 08:51:19.729432  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 08:51:19.729461  5077 sgd_solver.cpp:106] Iteration 19750, lr = 1e-05
I0508 08:53:26.682118  5077 solver.cpp:337] Iteration 19800, Testing net (#0)
I0508 08:53:57.047619  5077 solver.cpp:404]     Test net output #0: loss = 0.141389 (* 1 = 0.141389 loss)
I0508 08:53:57.911262  5077 solver.cpp:228] Iteration 19800, loss = 0.0339446
I0508 08:53:57.911334  5077 solver.cpp:244]     Train net output #0: loss = 0.0339448 (* 1 = 0.0339448 loss)
I0508 08:53:57.911348  5077 sgd_solver.cpp:106] Iteration 19800, lr = 1e-05
I0508 08:56:07.473624  5077 solver.cpp:228] Iteration 19850, loss = 0.0617732
I0508 08:56:07.473856  5077 solver.cpp:244]     Train net output #0: loss = 0.0617734 (* 1 = 0.0617734 loss)
I0508 08:56:07.473899  5077 sgd_solver.cpp:106] Iteration 19850, lr = 1e-05
I0508 08:58:17.310943  5077 solver.cpp:228] Iteration 19900, loss = 0.0285326
I0508 08:58:17.311110  5077 solver.cpp:244]     Train net output #0: loss = 0.0285328 (* 1 = 0.0285328 loss)
I0508 08:58:17.311126  5077 sgd_solver.cpp:106] Iteration 19900, lr = 1e-05
I0508 09:00:27.046386  5077 solver.cpp:228] Iteration 19950, loss = 0.0829779
I0508 09:00:27.046581  5077 solver.cpp:244]     Train net output #0: loss = 0.0829781 (* 1 = 0.0829781 loss)
I0508 09:00:27.046612  5077 sgd_solver.cpp:106] Iteration 19950, lr = 1e-05
I0508 09:02:34.185807  5077 solver.cpp:454] Snapshotting to binary proto file ../../weights/three_stream_triplet_loss_iter_20000.caffemodel
I0508 09:02:39.081037  5077 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../../weights/three_stream_triplet_loss_iter_20000.solverstate
I0508 09:02:39.134821  5077 solver.cpp:337] Iteration 20000, Testing net (#0)
I0508 09:03:07.708158  5077 solver.cpp:404]     Test net output #0: loss = 0.137932 (* 1 = 0.137932 loss)
I0508 09:03:08.575408  5077 solver.cpp:228] Iteration 20000, loss = 0.00316482
I0508 09:03:08.575492  5077 solver.cpp:244]     Train net output #0: loss = 0.00316506 (* 1 = 0.00316506 loss)
I0508 09:03:08.575515  5077 sgd_solver.cpp:106] Iteration 20000, lr = 1e-06
I0508 09:05:18.435111  5077 solver.cpp:228] Iteration 20050, loss = -2.26777e-07
I0508 09:05:18.447595  5077 solver.cpp:244]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0508 09:05:18.447630  5077 sgd_solver.cpp:106] Iteration 20050, lr = 1e-06
I0508 09:07:28.469207  5077 solver.cpp:228] Iteration 20100, loss = 0.0483707
I0508 09:07:28.469518  5077 solver.cpp:244]     Train net output #0: loss = 0.0483709 (* 1 = 0.0483709 loss)
I0508 09:07:28.469561  5077 sgd_solver.cpp:106] Iteration 20100, lr = 1e-06
I0508 09:09:38.422117  5077 solver.cpp:228] Iteration 20150, loss = 0.13328
I0508 09:09:38.422369  5077 solver.cpp:244]     Train net output #0: loss = 0.13328 (* 1 = 0.13328 loss)
I0508 09:09:38.422404  5077 sgd_solver.cpp:106] Iteration 20150, lr = 1e-06
