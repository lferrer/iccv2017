I0524 09:18:36.021412   849 caffe.cpp:217] Using GPUs 3
I0524 09:18:36.109696   849 caffe.cpp:222] GPU 3: TITAN X (Pascal)
I0524 09:18:36.869568   849 solver.cpp:48] Initializing solver from parameters: 
test_iter: 500
test_interval: 200
base_lr: 0.001
display: 1
max_iter: 60000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
stepsize: 20000
snapshot: 2000
snapshot_prefix: "../../weights/third_person_scene"
solver_mode: GPU
device_id: 3
net: "../../models/third_person_classifier.prototxt"
train_state {
  level: 0
  stage: ""
}
test_state {
  stage: "test-on-val"
}
I0524 09:18:36.869964   849 solver.cpp:91] Creating training net from net file: ../../models/third_person_classifier.prototxt
I0524 09:18:36.872263   849 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0524 09:18:36.872333   849 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer prob
I0524 09:18:36.872339   849 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0524 09:18:36.872344   849 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0524 09:18:36.872932   849 net.cpp:58] Initializing net from parameters: 
name: "C3D-TP-Classifier"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "triplet"
  top: "raw_label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 112
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
  }
  data_param {
    source: "/data/leo-data/Synthetic/LMDB/Triplets/train"
    batch_size: 24
    backend: LMDB
  }
}
layer {
  name: "slicer"
  type: "Slice"
  bottom: "triplet"
  top: "anchor_stacked"
  top: "positive_stacked"
  top: "negative_stacked"
  slice_param {
    slice_dim: 1
    slice_point: 48
    slice_point: 96
  }
}
layer {
  name: "reshape_anchor"
  type: "Reshape"
  bottom: "anchor_stacked"
  top: "anchor"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "reshape_positive"
  type: "Reshape"
  bottom: "positive_stacked"
  top: "positive"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "silence_negative"
  type: "Silence"
  bottom: "negative_stacked"
}
layer {
  name: "data_switch"
  type: "Python"
  bottom: "anchor"
  bottom: "positive"
  top: "first_person"
  top: "third_person"
  python_param {
    module: "data_switch"
    layer: "DataSwitchLayer"
  }
}
layer {
  name: "label_split"
  type: "Python"
  bottom: "raw_label"
  top: "fp_label"
  top: "tp_label"
  python_param {
    module: "label_split"
    layer: "LabelSplitLayer"
  }
}
layer {
  name: "label_parse"
  type: "Python"
  bottom: "tp_label"
  top: "label"
  python_param {
    module: "label_parse"
    layer: "LabelParseLayer"
    param_str: "Scene"
  }
}
layer {
  name: "silence_first_person"
  type: "Silence"
  bottom: "first_person"
  bottom: "fp_label"
}
layer {
  name: "conv1a"
  type: "NdConvolution"
  bottom: "third_person"
  top: "conv1a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "pool1"
  type: "NdPooling"
  bottom: "conv1a"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a"
  type: "NdConvolution"
  bottom: "pool1"
  top: "conv2a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a"
  type: "ReLU"
  bottom: "conv2a"
  top: "conv2a"
}
layer {
  name: "pool2"
  type: "NdPooling"
  bottom: "conv2a"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a"
  type: "NdConvolution"
  bottom: "pool2"
  top: "conv3a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a"
  type: "ReLU"
  bottom: "conv3a"
  top: "conv3a"
}
layer {
  name: "pool3"
  type: "NdPooling"
  bottom: "conv3a"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a"
  type: "NdConvolution"
  bottom: "pool3"
  top: "conv4a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a"
  type: "ReLU"
  bottom: "conv4a"
  top: "conv4a"
}
layer {
  name: "pool4"
  type: "NdPooling"
  bottom: "conv4a"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a"
  type: "NdConvolution"
  bottom: "pool4"
  top: "conv5a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a"
  type: "ReLU"
  bottom: "conv5a"
  top: "conv5a"
}
layer {
  name: "pool5"
  type: "NdPooling"
  bottom: "conv5a"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0524 09:18:36.873221   849 layer_factory.hpp:77] Creating layer data
I0524 09:18:36.874080   849 net.cpp:100] Creating Layer data
I0524 09:18:36.874100   849 net.cpp:408] data -> triplet
I0524 09:18:36.874128   849 net.cpp:408] data -> raw_label
I0524 09:18:36.876282   856 db_lmdb.cpp:35] Opened lmdb /data/leo-data/Synthetic/LMDB/Triplets/train
I0524 09:18:36.940888   849 data_layer.cpp:41] output data size: 24,144,112,112
I0524 09:18:37.401365   849 net.cpp:150] Setting up data
I0524 09:18:37.401484   849 net.cpp:157] Top shape: 24 144 112 112 (43352064)
I0524 09:18:37.401506   849 net.cpp:157] Top shape: 24 (24)
I0524 09:18:37.401515   849 net.cpp:165] Memory required for data: 173408352
I0524 09:18:37.401571   849 layer_factory.hpp:77] Creating layer slicer
I0524 09:18:37.414767   849 net.cpp:100] Creating Layer slicer
I0524 09:18:37.414813   849 net.cpp:434] slicer <- triplet
I0524 09:18:37.414872   849 net.cpp:408] slicer -> anchor_stacked
I0524 09:18:37.414922   849 net.cpp:408] slicer -> positive_stacked
I0524 09:18:37.414937   849 net.cpp:408] slicer -> negative_stacked
I0524 09:18:37.457221   849 net.cpp:150] Setting up slicer
I0524 09:18:37.457262   849 net.cpp:157] Top shape: 24 48 112 112 (14450688)
I0524 09:18:37.457267   849 net.cpp:157] Top shape: 24 48 112 112 (14450688)
I0524 09:18:37.457271   849 net.cpp:157] Top shape: 24 48 112 112 (14450688)
I0524 09:18:37.457274   849 net.cpp:165] Memory required for data: 346816608
I0524 09:18:37.457286   849 layer_factory.hpp:77] Creating layer reshape_anchor
I0524 09:18:37.457317   849 net.cpp:100] Creating Layer reshape_anchor
I0524 09:18:37.457324   849 net.cpp:434] reshape_anchor <- anchor_stacked
I0524 09:18:37.457332   849 net.cpp:408] reshape_anchor -> anchor
I0524 09:18:37.457388   849 net.cpp:150] Setting up reshape_anchor
I0524 09:18:37.457397   849 net.cpp:157] Top shape: 24 3 16 112 112 (14450688)
I0524 09:18:37.457401   849 net.cpp:165] Memory required for data: 404619360
I0524 09:18:37.457403   849 layer_factory.hpp:77] Creating layer reshape_positive
I0524 09:18:37.457412   849 net.cpp:100] Creating Layer reshape_positive
I0524 09:18:37.457414   849 net.cpp:434] reshape_positive <- positive_stacked
I0524 09:18:37.457422   849 net.cpp:408] reshape_positive -> positive
I0524 09:18:37.457450   849 net.cpp:150] Setting up reshape_positive
I0524 09:18:37.457458   849 net.cpp:157] Top shape: 24 3 16 112 112 (14450688)
I0524 09:18:37.457460   849 net.cpp:165] Memory required for data: 462422112
I0524 09:18:37.457464   849 layer_factory.hpp:77] Creating layer silence_negative
I0524 09:18:37.457473   849 net.cpp:100] Creating Layer silence_negative
I0524 09:18:37.457476   849 net.cpp:434] silence_negative <- negative_stacked
I0524 09:18:37.457481   849 net.cpp:150] Setting up silence_negative
I0524 09:18:37.457484   849 net.cpp:165] Memory required for data: 462422112
I0524 09:18:37.457487   849 layer_factory.hpp:77] Creating layer data_switch
I0524 09:18:38.422475   849 net.cpp:100] Creating Layer data_switch
I0524 09:18:38.422519   849 net.cpp:434] data_switch <- anchor
I0524 09:18:38.422528   849 net.cpp:434] data_switch <- positive
I0524 09:18:38.422534   849 net.cpp:408] data_switch -> first_person
I0524 09:18:38.422543   849 net.cpp:408] data_switch -> third_person
I0524 09:18:38.581482   849 net.cpp:150] Setting up data_switch
I0524 09:18:38.581539   849 net.cpp:157] Top shape: 24 3 16 112 112 (14450688)
I0524 09:18:38.581547   849 net.cpp:157] Top shape: 24 3 16 112 112 (14450688)
I0524 09:18:38.581552   849 net.cpp:165] Memory required for data: 578027616
I0524 09:18:38.581573   849 layer_factory.hpp:77] Creating layer label_split
I0524 09:18:38.581826   849 net.cpp:100] Creating Layer label_split
I0524 09:18:38.581840   849 net.cpp:434] label_split <- raw_label
I0524 09:18:38.581848   849 net.cpp:408] label_split -> fp_label
I0524 09:18:38.581857   849 net.cpp:408] label_split -> tp_label
I0524 09:18:38.581959   849 net.cpp:150] Setting up label_split
I0524 09:18:38.581969   849 net.cpp:157] Top shape: 24 1 (24)
I0524 09:18:38.581974   849 net.cpp:157] Top shape: 24 1 (24)
I0524 09:18:38.581977   849 net.cpp:165] Memory required for data: 578027808
I0524 09:18:38.581985   849 layer_factory.hpp:77] Creating layer label_parse
I0524 09:18:38.582129   849 net.cpp:100] Creating Layer label_parse
I0524 09:18:38.582139   849 net.cpp:434] label_parse <- tp_label
I0524 09:18:38.582144   849 net.cpp:408] label_parse -> label
I0524 09:18:38.582221   849 net.cpp:150] Setting up label_parse
I0524 09:18:38.582231   849 net.cpp:157] Top shape: 24 1 (24)
I0524 09:18:38.582234   849 net.cpp:165] Memory required for data: 578027904
I0524 09:18:38.582237   849 layer_factory.hpp:77] Creating layer silence_first_person
I0524 09:18:38.582247   849 net.cpp:100] Creating Layer silence_first_person
I0524 09:18:38.582270   849 net.cpp:434] silence_first_person <- first_person
I0524 09:18:38.582275   849 net.cpp:434] silence_first_person <- fp_label
I0524 09:18:38.582279   849 net.cpp:150] Setting up silence_first_person
I0524 09:18:38.582283   849 net.cpp:165] Memory required for data: 578027904
I0524 09:18:38.582285   849 layer_factory.hpp:77] Creating layer conv1a
I0524 09:18:38.582310   849 net.cpp:100] Creating Layer conv1a
I0524 09:18:38.582316   849 net.cpp:434] conv1a <- third_person
I0524 09:18:38.582322   849 net.cpp:408] conv1a -> conv1a
I0524 09:18:39.230020   849 net.cpp:150] Setting up conv1a
I0524 09:18:39.230063   849 net.cpp:157] Top shape: 24 64 16 112 112 (308281344)
I0524 09:18:39.230070   849 net.cpp:165] Memory required for data: 1811153280
I0524 09:18:39.230093   849 layer_factory.hpp:77] Creating layer relu1a
I0524 09:18:39.230119   849 net.cpp:100] Creating Layer relu1a
I0524 09:18:39.230137   849 net.cpp:434] relu1a <- conv1a
I0524 09:18:39.230154   849 net.cpp:395] relu1a -> conv1a (in-place)
I0524 09:18:39.231709   849 net.cpp:150] Setting up relu1a
I0524 09:18:39.231732   849 net.cpp:157] Top shape: 24 64 16 112 112 (308281344)
I0524 09:18:39.231739   849 net.cpp:165] Memory required for data: 3044278656
I0524 09:18:39.231745   849 layer_factory.hpp:77] Creating layer pool1
I0524 09:18:39.231760   849 net.cpp:100] Creating Layer pool1
I0524 09:18:39.231767   849 net.cpp:434] pool1 <- conv1a
I0524 09:18:39.231776   849 net.cpp:408] pool1 -> pool1
I0524 09:18:39.232209   849 net.cpp:150] Setting up pool1
I0524 09:18:39.232229   849 net.cpp:157] Top shape: 24 64 16 56 56 (77070336)
I0524 09:18:39.232237   849 net.cpp:165] Memory required for data: 3352560000
I0524 09:18:39.232249   849 layer_factory.hpp:77] Creating layer conv2a
I0524 09:18:39.232273   849 net.cpp:100] Creating Layer conv2a
I0524 09:18:39.232286   849 net.cpp:434] conv2a <- pool1
I0524 09:18:39.232306   849 net.cpp:408] conv2a -> conv2a
I0524 09:18:39.245908   849 net.cpp:150] Setting up conv2a
I0524 09:18:39.245930   849 net.cpp:157] Top shape: 24 128 16 56 56 (154140672)
I0524 09:18:39.245935   849 net.cpp:165] Memory required for data: 3969122688
I0524 09:18:39.245949   849 layer_factory.hpp:77] Creating layer relu2a
I0524 09:18:39.245961   849 net.cpp:100] Creating Layer relu2a
I0524 09:18:39.245966   849 net.cpp:434] relu2a <- conv2a
I0524 09:18:39.245978   849 net.cpp:395] relu2a -> conv2a (in-place)
I0524 09:18:39.246345   849 net.cpp:150] Setting up relu2a
I0524 09:18:39.246364   849 net.cpp:157] Top shape: 24 128 16 56 56 (154140672)
I0524 09:18:39.246373   849 net.cpp:165] Memory required for data: 4585685376
I0524 09:18:39.246384   849 layer_factory.hpp:77] Creating layer pool2
I0524 09:18:39.246404   849 net.cpp:100] Creating Layer pool2
I0524 09:18:39.246417   849 net.cpp:434] pool2 <- conv2a
I0524 09:18:39.246433   849 net.cpp:408] pool2 -> pool2
I0524 09:18:39.246821   849 net.cpp:150] Setting up pool2
I0524 09:18:39.246840   849 net.cpp:157] Top shape: 24 128 8 28 28 (19267584)
I0524 09:18:39.246850   849 net.cpp:165] Memory required for data: 4662755712
I0524 09:18:39.246863   849 layer_factory.hpp:77] Creating layer conv3a
I0524 09:18:39.246886   849 net.cpp:100] Creating Layer conv3a
I0524 09:18:39.246898   849 net.cpp:434] conv3a <- pool2
I0524 09:18:39.246940   849 net.cpp:408] conv3a -> conv3a
I0524 09:18:39.296634   849 net.cpp:150] Setting up conv3a
I0524 09:18:39.296676   849 net.cpp:157] Top shape: 24 256 8 28 28 (38535168)
I0524 09:18:39.296691   849 net.cpp:165] Memory required for data: 4816896384
I0524 09:18:39.296722   849 layer_factory.hpp:77] Creating layer relu3a
I0524 09:18:39.296752   849 net.cpp:100] Creating Layer relu3a
I0524 09:18:39.296762   849 net.cpp:434] relu3a <- conv3a
I0524 09:18:39.296774   849 net.cpp:395] relu3a -> conv3a (in-place)
I0524 09:18:39.297168   849 net.cpp:150] Setting up relu3a
I0524 09:18:39.297195   849 net.cpp:157] Top shape: 24 256 8 28 28 (38535168)
I0524 09:18:39.297201   849 net.cpp:165] Memory required for data: 4971037056
I0524 09:18:39.297209   849 layer_factory.hpp:77] Creating layer pool3
I0524 09:18:39.297261   849 net.cpp:100] Creating Layer pool3
I0524 09:18:39.297271   849 net.cpp:434] pool3 <- conv3a
I0524 09:18:39.297283   849 net.cpp:408] pool3 -> pool3
I0524 09:18:39.299183   849 net.cpp:150] Setting up pool3
I0524 09:18:39.299209   849 net.cpp:157] Top shape: 24 256 4 14 14 (4816896)
I0524 09:18:39.299223   849 net.cpp:165] Memory required for data: 4990304640
I0524 09:18:39.299242   849 layer_factory.hpp:77] Creating layer conv4a
I0524 09:18:39.299260   849 net.cpp:100] Creating Layer conv4a
I0524 09:18:39.299269   849 net.cpp:434] conv4a <- pool3
I0524 09:18:39.299290   849 net.cpp:408] conv4a -> conv4a
I0524 09:18:39.395941   849 net.cpp:150] Setting up conv4a
I0524 09:18:39.395972   849 net.cpp:157] Top shape: 24 256 4 14 14 (4816896)
I0524 09:18:39.395977   849 net.cpp:165] Memory required for data: 5009572224
I0524 09:18:39.395987   849 layer_factory.hpp:77] Creating layer relu4a
I0524 09:18:39.396009   849 net.cpp:100] Creating Layer relu4a
I0524 09:18:39.396014   849 net.cpp:434] relu4a <- conv4a
I0524 09:18:39.396021   849 net.cpp:395] relu4a -> conv4a (in-place)
I0524 09:18:39.396265   849 net.cpp:150] Setting up relu4a
I0524 09:18:39.396277   849 net.cpp:157] Top shape: 24 256 4 14 14 (4816896)
I0524 09:18:39.396281   849 net.cpp:165] Memory required for data: 5028839808
I0524 09:18:39.396286   849 layer_factory.hpp:77] Creating layer pool4
I0524 09:18:39.396299   849 net.cpp:100] Creating Layer pool4
I0524 09:18:39.396303   849 net.cpp:434] pool4 <- conv4a
I0524 09:18:39.396311   849 net.cpp:408] pool4 -> pool4
I0524 09:18:39.396582   849 net.cpp:150] Setting up pool4
I0524 09:18:39.396595   849 net.cpp:157] Top shape: 24 256 2 7 7 (602112)
I0524 09:18:39.396600   849 net.cpp:165] Memory required for data: 5031248256
I0524 09:18:39.396602   849 layer_factory.hpp:77] Creating layer conv5a
I0524 09:18:39.396615   849 net.cpp:100] Creating Layer conv5a
I0524 09:18:39.396620   849 net.cpp:434] conv5a <- pool4
I0524 09:18:39.396630   849 net.cpp:408] conv5a -> conv5a
I0524 09:18:39.459352   849 net.cpp:150] Setting up conv5a
I0524 09:18:39.459391   849 net.cpp:157] Top shape: 24 256 2 7 7 (602112)
I0524 09:18:39.459395   849 net.cpp:165] Memory required for data: 5033656704
I0524 09:18:39.459410   849 layer_factory.hpp:77] Creating layer relu5a
I0524 09:18:39.459425   849 net.cpp:100] Creating Layer relu5a
I0524 09:18:39.459430   849 net.cpp:434] relu5a <- conv5a
I0524 09:18:39.459439   849 net.cpp:395] relu5a -> conv5a (in-place)
I0524 09:18:39.459652   849 net.cpp:150] Setting up relu5a
I0524 09:18:39.459663   849 net.cpp:157] Top shape: 24 256 2 7 7 (602112)
I0524 09:18:39.459666   849 net.cpp:165] Memory required for data: 5036065152
I0524 09:18:39.459671   849 layer_factory.hpp:77] Creating layer pool5
I0524 09:18:39.459679   849 net.cpp:100] Creating Layer pool5
I0524 09:18:39.459684   849 net.cpp:434] pool5 <- conv5a
I0524 09:18:39.459691   849 net.cpp:408] pool5 -> pool5
I0524 09:18:39.460824   849 net.cpp:150] Setting up pool5
I0524 09:18:39.460839   849 net.cpp:157] Top shape: 24 256 1 4 4 (98304)
I0524 09:18:39.460842   849 net.cpp:165] Memory required for data: 5036458368
I0524 09:18:39.460846   849 layer_factory.hpp:77] Creating layer fc6
I0524 09:18:39.460868   849 net.cpp:100] Creating Layer fc6
I0524 09:18:39.460873   849 net.cpp:434] fc6 <- pool5
I0524 09:18:39.460880   849 net.cpp:408] fc6 -> fc6
I0524 09:18:39.743044   849 net.cpp:150] Setting up fc6
I0524 09:18:39.743082   849 net.cpp:157] Top shape: 24 2048 (49152)
I0524 09:18:39.743085   849 net.cpp:165] Memory required for data: 5036654976
I0524 09:18:39.743094   849 layer_factory.hpp:77] Creating layer relu6
I0524 09:18:39.743113   849 net.cpp:100] Creating Layer relu6
I0524 09:18:39.743118   849 net.cpp:434] relu6 <- fc6
I0524 09:18:39.743124   849 net.cpp:395] relu6 -> fc6 (in-place)
I0524 09:18:39.743402   849 net.cpp:150] Setting up relu6
I0524 09:18:39.743413   849 net.cpp:157] Top shape: 24 2048 (49152)
I0524 09:18:39.743417   849 net.cpp:165] Memory required for data: 5036851584
I0524 09:18:39.743440   849 layer_factory.hpp:77] Creating layer drop6
I0524 09:18:39.743454   849 net.cpp:100] Creating Layer drop6
I0524 09:18:39.743458   849 net.cpp:434] drop6 <- fc6
I0524 09:18:39.743464   849 net.cpp:395] drop6 -> fc6 (in-place)
I0524 09:18:39.743501   849 net.cpp:150] Setting up drop6
I0524 09:18:39.743535   849 net.cpp:157] Top shape: 24 2048 (49152)
I0524 09:18:39.743540   849 net.cpp:165] Memory required for data: 5037048192
I0524 09:18:39.743541   849 layer_factory.hpp:77] Creating layer fc7
I0524 09:18:39.743552   849 net.cpp:100] Creating Layer fc7
I0524 09:18:39.743556   849 net.cpp:434] fc7 <- fc6
I0524 09:18:39.743566   849 net.cpp:408] fc7 -> fc7
I0524 09:18:39.887246   849 net.cpp:150] Setting up fc7
I0524 09:18:39.887295   849 net.cpp:157] Top shape: 24 2048 (49152)
I0524 09:18:39.887298   849 net.cpp:165] Memory required for data: 5037244800
I0524 09:18:39.887315   849 layer_factory.hpp:77] Creating layer relu7
I0524 09:18:39.887331   849 net.cpp:100] Creating Layer relu7
I0524 09:18:39.887346   849 net.cpp:434] relu7 <- fc7
I0524 09:18:39.887358   849 net.cpp:395] relu7 -> fc7 (in-place)
I0524 09:18:39.887693   849 net.cpp:150] Setting up relu7
I0524 09:18:39.887706   849 net.cpp:157] Top shape: 24 2048 (49152)
I0524 09:18:39.887708   849 net.cpp:165] Memory required for data: 5037441408
I0524 09:18:39.887712   849 layer_factory.hpp:77] Creating layer drop7
I0524 09:18:39.887725   849 net.cpp:100] Creating Layer drop7
I0524 09:18:39.887729   849 net.cpp:434] drop7 <- fc7
I0524 09:18:39.887734   849 net.cpp:395] drop7 -> fc7 (in-place)
I0524 09:18:39.887774   849 net.cpp:150] Setting up drop7
I0524 09:18:39.887784   849 net.cpp:157] Top shape: 24 2048 (49152)
I0524 09:18:39.887786   849 net.cpp:165] Memory required for data: 5037638016
I0524 09:18:39.887789   849 layer_factory.hpp:77] Creating layer fc8
I0524 09:18:39.887800   849 net.cpp:100] Creating Layer fc8
I0524 09:18:39.887804   849 net.cpp:434] fc8 <- fc7
I0524 09:18:39.887810   849 net.cpp:408] fc8 -> fc8
I0524 09:18:39.888607   849 net.cpp:150] Setting up fc8
I0524 09:18:39.888617   849 net.cpp:157] Top shape: 24 10 (240)
I0524 09:18:39.888619   849 net.cpp:165] Memory required for data: 5037638976
I0524 09:18:39.888633   849 layer_factory.hpp:77] Creating layer loss
I0524 09:18:39.888648   849 net.cpp:100] Creating Layer loss
I0524 09:18:39.888653   849 net.cpp:434] loss <- fc8
I0524 09:18:39.888658   849 net.cpp:434] loss <- label
I0524 09:18:39.888665   849 net.cpp:408] loss -> loss
I0524 09:18:39.888680   849 layer_factory.hpp:77] Creating layer loss
I0524 09:18:39.889011   849 net.cpp:150] Setting up loss
I0524 09:18:39.889022   849 net.cpp:157] Top shape: (1)
I0524 09:18:39.889026   849 net.cpp:160]     with loss weight 1
I0524 09:18:39.889052   849 net.cpp:165] Memory required for data: 5037638980
I0524 09:18:39.889055   849 net.cpp:226] loss needs backward computation.
I0524 09:18:39.889060   849 net.cpp:226] fc8 needs backward computation.
I0524 09:18:39.889063   849 net.cpp:228] drop7 does not need backward computation.
I0524 09:18:39.889066   849 net.cpp:228] relu7 does not need backward computation.
I0524 09:18:39.889075   849 net.cpp:228] fc7 does not need backward computation.
I0524 09:18:39.889078   849 net.cpp:228] drop6 does not need backward computation.
I0524 09:18:39.889081   849 net.cpp:228] relu6 does not need backward computation.
I0524 09:18:39.889084   849 net.cpp:228] fc6 does not need backward computation.
I0524 09:18:39.889097   849 net.cpp:228] pool5 does not need backward computation.
I0524 09:18:39.889101   849 net.cpp:228] relu5a does not need backward computation.
I0524 09:18:39.889104   849 net.cpp:228] conv5a does not need backward computation.
I0524 09:18:39.889109   849 net.cpp:228] pool4 does not need backward computation.
I0524 09:18:39.889112   849 net.cpp:228] relu4a does not need backward computation.
I0524 09:18:39.889116   849 net.cpp:228] conv4a does not need backward computation.
I0524 09:18:39.889119   849 net.cpp:228] pool3 does not need backward computation.
I0524 09:18:39.889147   849 net.cpp:228] relu3a does not need backward computation.
I0524 09:18:39.889152   849 net.cpp:228] conv3a does not need backward computation.
I0524 09:18:39.889154   849 net.cpp:228] pool2 does not need backward computation.
I0524 09:18:39.889158   849 net.cpp:228] relu2a does not need backward computation.
I0524 09:18:39.889161   849 net.cpp:228] conv2a does not need backward computation.
I0524 09:18:39.889166   849 net.cpp:228] pool1 does not need backward computation.
I0524 09:18:39.889170   849 net.cpp:228] relu1a does not need backward computation.
I0524 09:18:39.889176   849 net.cpp:228] conv1a does not need backward computation.
I0524 09:18:39.889181   849 net.cpp:228] silence_first_person does not need backward computation.
I0524 09:18:39.889189   849 net.cpp:228] label_parse does not need backward computation.
I0524 09:18:39.889194   849 net.cpp:228] label_split does not need backward computation.
I0524 09:18:39.889204   849 net.cpp:228] data_switch does not need backward computation.
I0524 09:18:39.889209   849 net.cpp:228] silence_negative does not need backward computation.
I0524 09:18:39.889212   849 net.cpp:228] reshape_positive does not need backward computation.
I0524 09:18:39.889216   849 net.cpp:228] reshape_anchor does not need backward computation.
I0524 09:18:39.889221   849 net.cpp:228] slicer does not need backward computation.
I0524 09:18:39.889225   849 net.cpp:228] data does not need backward computation.
I0524 09:18:39.889227   849 net.cpp:270] This network produces output loss
I0524 09:18:39.889256   849 net.cpp:283] Network initialization done.
I0524 09:18:39.890774   849 solver.cpp:181] Creating test net (#0) specified by net file: ../../models/third_person_classifier.prototxt
I0524 09:18:39.890846   849 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0524 09:18:39.891248   849 net.cpp:58] Initializing net from parameters: 
name: "C3D-TP-Classifier"
state {
  phase: TEST
  stage: "test-on-val"
}
layer {
  name: "data"
  type: "Data"
  top: "triplet"
  top: "raw_label"
  include {
    phase: TEST
    stage: "test-on-val"
  }
  transform_param {
    mirror: false
    crop_size: 112
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
    mean_value: 65
    mean_value: 74
    mean_value: 92
  }
  data_param {
    source: "/data/leo-data/Synthetic/LMDB/Triplets/val"
    batch_size: 1
    backend: LMDB
  }
}
layer {
  name: "slicer"
  type: "Slice"
  bottom: "triplet"
  top: "anchor_stacked"
  top: "positive_stacked"
  top: "negative_stacked"
  slice_param {
    slice_dim: 1
    slice_point: 48
    slice_point: 96
  }
}
layer {
  name: "reshape_anchor"
  type: "Reshape"
  bottom: "anchor_stacked"
  top: "anchor"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "reshape_positive"
  type: "Reshape"
  bottom: "positive_stacked"
  top: "positive"
  reshape_param {
    shape {
      dim: 0
      dim: 3
      dim: 16
      dim: 112
      dim: 112
    }
  }
}
layer {
  name: "silence_negative"
  type: "Silence"
  bottom: "negative_stacked"
}
layer {
  name: "data_switch"
  type: "Python"
  bottom: "anchor"
  bottom: "positive"
  top: "first_person"
  top: "third_person"
  python_param {
    module: "data_switch"
    layer: "DataSwitchLayer"
  }
}
layer {
  name: "label_split"
  type: "Python"
  bottom: "raw_label"
  top: "fp_label"
  top: "tp_label"
  python_param {
    module: "label_split"
    layer: "LabelSplitLayer"
  }
}
layer {
  name: "label_parse"
  type: "Python"
  bottom: "tp_label"
  top: "label"
  python_param {
    module: "label_parse"
    layer: "LabelParseLayer"
    param_str: "Scene"
  }
}
layer {
  name: "silence_first_person"
  type: "Silence"
  bottom: "first_person"
  bottom: "fp_label"
}
layer {
  name: "conv1a"
  type: "NdConvolution"
  bottom: "third_person"
  top: "conv1a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu1a"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "pool1"
  type: "NdPooling"
  bottom: "conv1a"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 1
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 1
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv2a"
  type: "NdConvolution"
  bottom: "pool1"
  top: "conv2a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu2a"
  type: "ReLU"
  bottom: "conv2a"
  top: "conv2a"
}
layer {
  name: "pool2"
  type: "NdPooling"
  bottom: "conv2a"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv3a"
  type: "NdConvolution"
  bottom: "pool2"
  top: "conv3a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu3a"
  type: "ReLU"
  bottom: "conv3a"
  top: "conv3a"
}
layer {
  name: "pool3"
  type: "NdPooling"
  bottom: "conv3a"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv4a"
  type: "NdConvolution"
  bottom: "pool3"
  top: "conv4a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu4a"
  type: "ReLU"
  bottom: "conv4a"
  top: "conv4a"
}
layer {
  name: "pool4"
  type: "NdPooling"
  bottom: "conv4a"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "conv5a"
  type: "NdConvolution"
  bottom: "pool4"
  top: "conv5a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    pad_shape {
      dim: 1
      dim: 1
      dim: 1
    }
    kernel_shape {
      dim: 3
      dim: 3
      dim: 3
    }
    stride_shape {
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "relu5a"
  type: "ReLU"
  bottom: "conv5a"
  top: "conv5a"
}
layer {
  name: "pool5"
  type: "NdPooling"
  bottom: "conv5a"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_shape {
      dim: 2
      dim: 2
      dim: 2
    }
    stride_shape {
      dim: 2
      dim: 2
      dim: 2
    }
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "prob"
  type: "Softmax"
  bottom: "fc8"
  top: "prob"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "prob"
  bottom: "label"
  top: "accuracy/top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "prob"
  bottom: "label"
  top: "accuracy/top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0524 09:18:39.891450   849 layer_factory.hpp:77] Creating layer data
I0524 09:18:39.891530   849 net.cpp:100] Creating Layer data
I0524 09:18:39.891540   849 net.cpp:408] data -> triplet
I0524 09:18:39.891547   849 net.cpp:408] data -> raw_label
I0524 09:18:39.895279   859 db_lmdb.cpp:35] Opened lmdb /data/leo-data/Synthetic/LMDB/Triplets/val
I0524 09:18:39.908179   849 data_layer.cpp:41] output data size: 1,144,112,112
I0524 09:18:39.946187   849 net.cpp:150] Setting up data
I0524 09:18:39.946240   849 net.cpp:157] Top shape: 1 144 112 112 (1806336)
I0524 09:18:39.946255   849 net.cpp:157] Top shape: 1 (1)
I0524 09:18:39.946259   849 net.cpp:165] Memory required for data: 7225348
I0524 09:18:39.946265   849 layer_factory.hpp:77] Creating layer slicer
I0524 09:18:39.946280   849 net.cpp:100] Creating Layer slicer
I0524 09:18:39.946290   849 net.cpp:434] slicer <- triplet
I0524 09:18:39.946306   849 net.cpp:408] slicer -> anchor_stacked
I0524 09:18:39.946316   849 net.cpp:408] slicer -> positive_stacked
I0524 09:18:39.946326   849 net.cpp:408] slicer -> negative_stacked
I0524 09:18:39.946542   849 net.cpp:150] Setting up slicer
I0524 09:18:39.946552   849 net.cpp:157] Top shape: 1 48 112 112 (602112)
I0524 09:18:39.946555   849 net.cpp:157] Top shape: 1 48 112 112 (602112)
I0524 09:18:39.946559   849 net.cpp:157] Top shape: 1 48 112 112 (602112)
I0524 09:18:39.946561   849 net.cpp:165] Memory required for data: 14450692
I0524 09:18:39.946565   849 layer_factory.hpp:77] Creating layer reshape_anchor
I0524 09:18:39.946575   849 net.cpp:100] Creating Layer reshape_anchor
I0524 09:18:39.946579   849 net.cpp:434] reshape_anchor <- anchor_stacked
I0524 09:18:39.946586   849 net.cpp:408] reshape_anchor -> anchor
I0524 09:18:39.946640   849 net.cpp:150] Setting up reshape_anchor
I0524 09:18:39.946647   849 net.cpp:157] Top shape: 1 3 16 112 112 (602112)
I0524 09:18:39.946650   849 net.cpp:165] Memory required for data: 16859140
I0524 09:18:39.946653   849 layer_factory.hpp:77] Creating layer reshape_positive
I0524 09:18:39.946661   849 net.cpp:100] Creating Layer reshape_positive
I0524 09:18:39.946665   849 net.cpp:434] reshape_positive <- positive_stacked
I0524 09:18:39.946669   849 net.cpp:408] reshape_positive -> positive
I0524 09:18:39.946704   849 net.cpp:150] Setting up reshape_positive
I0524 09:18:39.946712   849 net.cpp:157] Top shape: 1 3 16 112 112 (602112)
I0524 09:18:39.946714   849 net.cpp:165] Memory required for data: 19267588
I0524 09:18:39.946717   849 layer_factory.hpp:77] Creating layer silence_negative
I0524 09:18:39.946723   849 net.cpp:100] Creating Layer silence_negative
I0524 09:18:39.946727   849 net.cpp:434] silence_negative <- negative_stacked
I0524 09:18:39.946732   849 net.cpp:150] Setting up silence_negative
I0524 09:18:39.946734   849 net.cpp:165] Memory required for data: 19267588
I0524 09:18:39.946738   849 layer_factory.hpp:77] Creating layer data_switch
I0524 09:18:39.946810   849 net.cpp:100] Creating Layer data_switch
I0524 09:18:39.946816   849 net.cpp:434] data_switch <- anchor
I0524 09:18:39.946820   849 net.cpp:434] data_switch <- positive
I0524 09:18:39.946826   849 net.cpp:408] data_switch -> first_person
I0524 09:18:39.946832   849 net.cpp:408] data_switch -> third_person
I0524 09:18:39.951115   849 net.cpp:150] Setting up data_switch
I0524 09:18:39.951130   849 net.cpp:157] Top shape: 1 3 16 112 112 (602112)
I0524 09:18:39.951136   849 net.cpp:157] Top shape: 1 3 16 112 112 (602112)
I0524 09:18:39.951139   849 net.cpp:165] Memory required for data: 24084484
I0524 09:18:39.951143   849 layer_factory.hpp:77] Creating layer label_split
I0524 09:18:39.951176   849 net.cpp:100] Creating Layer label_split
I0524 09:18:39.951202   849 net.cpp:434] label_split <- raw_label
I0524 09:18:39.951212   849 net.cpp:408] label_split -> fp_label
I0524 09:18:39.951223   849 net.cpp:408] label_split -> tp_label
I0524 09:18:39.951336   849 net.cpp:150] Setting up label_split
I0524 09:18:39.951350   849 net.cpp:157] Top shape: 1 1 (1)
I0524 09:18:39.951357   849 net.cpp:157] Top shape: 1 1 (1)
I0524 09:18:39.951362   849 net.cpp:165] Memory required for data: 24084492
I0524 09:18:39.951369   849 layer_factory.hpp:77] Creating layer label_parse
I0524 09:18:39.951406   849 net.cpp:100] Creating Layer label_parse
I0524 09:18:39.951416   849 net.cpp:434] label_parse <- tp_label
I0524 09:18:39.951424   849 net.cpp:408] label_parse -> label
I0524 09:18:39.951506   849 net.cpp:150] Setting up label_parse
I0524 09:18:39.951519   849 net.cpp:157] Top shape: 1 1 (1)
I0524 09:18:39.951524   849 net.cpp:165] Memory required for data: 24084496
I0524 09:18:39.951529   849 layer_factory.hpp:77] Creating layer label_label_parse_0_split
I0524 09:18:39.951550   849 net.cpp:100] Creating Layer label_label_parse_0_split
I0524 09:18:39.951557   849 net.cpp:434] label_label_parse_0_split <- label
I0524 09:18:39.951565   849 net.cpp:408] label_label_parse_0_split -> label_label_parse_0_split_0
I0524 09:18:39.951576   849 net.cpp:408] label_label_parse_0_split -> label_label_parse_0_split_1
I0524 09:18:39.951587   849 net.cpp:408] label_label_parse_0_split -> label_label_parse_0_split_2
I0524 09:18:39.951653   849 net.cpp:150] Setting up label_label_parse_0_split
I0524 09:18:39.951663   849 net.cpp:157] Top shape: 1 1 (1)
I0524 09:18:39.951670   849 net.cpp:157] Top shape: 1 1 (1)
I0524 09:18:39.951678   849 net.cpp:157] Top shape: 1 1 (1)
I0524 09:18:39.951683   849 net.cpp:165] Memory required for data: 24084508
I0524 09:18:39.951688   849 layer_factory.hpp:77] Creating layer silence_first_person
I0524 09:18:39.951695   849 net.cpp:100] Creating Layer silence_first_person
I0524 09:18:39.951702   849 net.cpp:434] silence_first_person <- first_person
I0524 09:18:39.951709   849 net.cpp:434] silence_first_person <- fp_label
I0524 09:18:39.951717   849 net.cpp:150] Setting up silence_first_person
I0524 09:18:39.951725   849 net.cpp:165] Memory required for data: 24084508
I0524 09:18:39.951730   849 layer_factory.hpp:77] Creating layer conv1a
I0524 09:18:39.951746   849 net.cpp:100] Creating Layer conv1a
I0524 09:18:39.951752   849 net.cpp:434] conv1a <- third_person
I0524 09:18:39.951763   849 net.cpp:408] conv1a -> conv1a
I0524 09:18:39.963184   849 net.cpp:150] Setting up conv1a
I0524 09:18:39.963248   849 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0524 09:18:39.963258   849 net.cpp:165] Memory required for data: 75464732
I0524 09:18:39.963300   849 layer_factory.hpp:77] Creating layer relu1a
I0524 09:18:39.963323   849 net.cpp:100] Creating Layer relu1a
I0524 09:18:39.963338   849 net.cpp:434] relu1a <- conv1a
I0524 09:18:39.963359   849 net.cpp:395] relu1a -> conv1a (in-place)
I0524 09:18:39.966569   849 net.cpp:150] Setting up relu1a
I0524 09:18:39.966593   849 net.cpp:157] Top shape: 1 64 16 112 112 (12845056)
I0524 09:18:39.966598   849 net.cpp:165] Memory required for data: 126844956
I0524 09:18:39.966612   849 layer_factory.hpp:77] Creating layer pool1
I0524 09:18:39.966627   849 net.cpp:100] Creating Layer pool1
I0524 09:18:39.966634   849 net.cpp:434] pool1 <- conv1a
I0524 09:18:39.966642   849 net.cpp:408] pool1 -> pool1
I0524 09:18:39.967037   849 net.cpp:150] Setting up pool1
I0524 09:18:39.967054   849 net.cpp:157] Top shape: 1 64 16 56 56 (3211264)
I0524 09:18:39.967061   849 net.cpp:165] Memory required for data: 139690012
I0524 09:18:39.967066   849 layer_factory.hpp:77] Creating layer conv2a
I0524 09:18:39.967079   849 net.cpp:100] Creating Layer conv2a
I0524 09:18:39.967085   849 net.cpp:434] conv2a <- pool1
I0524 09:18:39.967095   849 net.cpp:408] conv2a -> conv2a
I0524 09:18:39.980656   849 net.cpp:150] Setting up conv2a
I0524 09:18:39.980696   849 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0524 09:18:39.980762   849 net.cpp:165] Memory required for data: 165380124
I0524 09:18:39.980787   849 layer_factory.hpp:77] Creating layer relu2a
I0524 09:18:39.980799   849 net.cpp:100] Creating Layer relu2a
I0524 09:18:39.980806   849 net.cpp:434] relu2a <- conv2a
I0524 09:18:39.980816   849 net.cpp:395] relu2a -> conv2a (in-place)
I0524 09:18:39.981079   849 net.cpp:150] Setting up relu2a
I0524 09:18:39.981093   849 net.cpp:157] Top shape: 1 128 16 56 56 (6422528)
I0524 09:18:39.981101   849 net.cpp:165] Memory required for data: 191070236
I0524 09:18:39.981106   849 layer_factory.hpp:77] Creating layer pool2
I0524 09:18:39.981122   849 net.cpp:100] Creating Layer pool2
I0524 09:18:39.981129   849 net.cpp:434] pool2 <- conv2a
I0524 09:18:39.981139   849 net.cpp:408] pool2 -> pool2
I0524 09:18:39.981467   849 net.cpp:150] Setting up pool2
I0524 09:18:39.981480   849 net.cpp:157] Top shape: 1 128 8 28 28 (802816)
I0524 09:18:39.981490   849 net.cpp:165] Memory required for data: 194281500
I0524 09:18:39.981494   849 layer_factory.hpp:77] Creating layer conv3a
I0524 09:18:39.981510   849 net.cpp:100] Creating Layer conv3a
I0524 09:18:39.981519   849 net.cpp:434] conv3a <- pool2
I0524 09:18:39.981530   849 net.cpp:408] conv3a -> conv3a
I0524 09:18:40.018601   849 net.cpp:150] Setting up conv3a
I0524 09:18:40.018636   849 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0524 09:18:40.018641   849 net.cpp:165] Memory required for data: 200704028
I0524 09:18:40.018656   849 layer_factory.hpp:77] Creating layer relu3a
I0524 09:18:40.018676   849 net.cpp:100] Creating Layer relu3a
I0524 09:18:40.018682   849 net.cpp:434] relu3a <- conv3a
I0524 09:18:40.018687   849 net.cpp:395] relu3a -> conv3a (in-place)
I0524 09:18:40.018939   849 net.cpp:150] Setting up relu3a
I0524 09:18:40.018954   849 net.cpp:157] Top shape: 1 256 8 28 28 (1605632)
I0524 09:18:40.018959   849 net.cpp:165] Memory required for data: 207126556
I0524 09:18:40.018963   849 layer_factory.hpp:77] Creating layer pool3
I0524 09:18:40.018975   849 net.cpp:100] Creating Layer pool3
I0524 09:18:40.018980   849 net.cpp:434] pool3 <- conv3a
I0524 09:18:40.018990   849 net.cpp:408] pool3 -> pool3
I0524 09:18:40.020107   849 net.cpp:150] Setting up pool3
I0524 09:18:40.020122   849 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0524 09:18:40.020126   849 net.cpp:165] Memory required for data: 207929372
I0524 09:18:40.020130   849 layer_factory.hpp:77] Creating layer conv4a
I0524 09:18:40.020143   849 net.cpp:100] Creating Layer conv4a
I0524 09:18:40.020148   849 net.cpp:434] conv4a <- pool3
I0524 09:18:40.020159   849 net.cpp:408] conv4a -> conv4a
I0524 09:18:40.084965   849 net.cpp:150] Setting up conv4a
I0524 09:18:40.084987   849 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0524 09:18:40.084991   849 net.cpp:165] Memory required for data: 208732188
I0524 09:18:40.085000   849 layer_factory.hpp:77] Creating layer relu4a
I0524 09:18:40.085007   849 net.cpp:100] Creating Layer relu4a
I0524 09:18:40.085011   849 net.cpp:434] relu4a <- conv4a
I0524 09:18:40.085018   849 net.cpp:395] relu4a -> conv4a (in-place)
I0524 09:18:40.085232   849 net.cpp:150] Setting up relu4a
I0524 09:18:40.085242   849 net.cpp:157] Top shape: 1 256 4 14 14 (200704)
I0524 09:18:40.085247   849 net.cpp:165] Memory required for data: 209535004
I0524 09:18:40.085252   849 layer_factory.hpp:77] Creating layer pool4
I0524 09:18:40.085264   849 net.cpp:100] Creating Layer pool4
I0524 09:18:40.085269   849 net.cpp:434] pool4 <- conv4a
I0524 09:18:40.085278   849 net.cpp:408] pool4 -> pool4
I0524 09:18:40.085536   849 net.cpp:150] Setting up pool4
I0524 09:18:40.085546   849 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0524 09:18:40.085551   849 net.cpp:165] Memory required for data: 209635356
I0524 09:18:40.085556   849 layer_factory.hpp:77] Creating layer conv5a
I0524 09:18:40.085569   849 net.cpp:100] Creating Layer conv5a
I0524 09:18:40.085575   849 net.cpp:434] conv5a <- pool4
I0524 09:18:40.085584   849 net.cpp:408] conv5a -> conv5a
I0524 09:18:40.147151   849 net.cpp:150] Setting up conv5a
I0524 09:18:40.147194   849 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0524 09:18:40.147198   849 net.cpp:165] Memory required for data: 209735708
I0524 09:18:40.147210   849 layer_factory.hpp:77] Creating layer relu5a
I0524 09:18:40.147218   849 net.cpp:100] Creating Layer relu5a
I0524 09:18:40.147224   849 net.cpp:434] relu5a <- conv5a
I0524 09:18:40.147248   849 net.cpp:395] relu5a -> conv5a (in-place)
I0524 09:18:40.147478   849 net.cpp:150] Setting up relu5a
I0524 09:18:40.147490   849 net.cpp:157] Top shape: 1 256 2 7 7 (25088)
I0524 09:18:40.147500   849 net.cpp:165] Memory required for data: 209836060
I0524 09:18:40.147507   849 layer_factory.hpp:77] Creating layer pool5
I0524 09:18:40.147521   849 net.cpp:100] Creating Layer pool5
I0524 09:18:40.147526   849 net.cpp:434] pool5 <- conv5a
I0524 09:18:40.147534   849 net.cpp:408] pool5 -> pool5
I0524 09:18:40.151495   849 net.cpp:150] Setting up pool5
I0524 09:18:40.151520   849 net.cpp:157] Top shape: 1 256 1 4 4 (4096)
I0524 09:18:40.151525   849 net.cpp:165] Memory required for data: 209852444
I0524 09:18:40.151530   849 layer_factory.hpp:77] Creating layer fc6
I0524 09:18:40.151553   849 net.cpp:100] Creating Layer fc6
I0524 09:18:40.151592   849 net.cpp:434] fc6 <- pool5
I0524 09:18:40.151607   849 net.cpp:408] fc6 -> fc6
I0524 09:18:40.445794   849 net.cpp:150] Setting up fc6
I0524 09:18:40.445837   849 net.cpp:157] Top shape: 1 2048 (2048)
I0524 09:18:40.445840   849 net.cpp:165] Memory required for data: 209860636
I0524 09:18:40.445854   849 layer_factory.hpp:77] Creating layer relu6
I0524 09:18:40.445883   849 net.cpp:100] Creating Layer relu6
I0524 09:18:40.445890   849 net.cpp:434] relu6 <- fc6
I0524 09:18:40.445901   849 net.cpp:395] relu6 -> fc6 (in-place)
I0524 09:18:40.446214   849 net.cpp:150] Setting up relu6
I0524 09:18:40.446226   849 net.cpp:157] Top shape: 1 2048 (2048)
I0524 09:18:40.446229   849 net.cpp:165] Memory required for data: 209868828
I0524 09:18:40.446238   849 layer_factory.hpp:77] Creating layer drop6
I0524 09:18:40.446257   849 net.cpp:100] Creating Layer drop6
I0524 09:18:40.446266   849 net.cpp:434] drop6 <- fc6
I0524 09:18:40.446274   849 net.cpp:395] drop6 -> fc6 (in-place)
I0524 09:18:40.446318   849 net.cpp:150] Setting up drop6
I0524 09:18:40.446327   849 net.cpp:157] Top shape: 1 2048 (2048)
I0524 09:18:40.446334   849 net.cpp:165] Memory required for data: 209877020
I0524 09:18:40.446339   849 layer_factory.hpp:77] Creating layer fc7
I0524 09:18:40.446365   849 net.cpp:100] Creating Layer fc7
I0524 09:18:40.446370   849 net.cpp:434] fc7 <- fc6
I0524 09:18:40.446377   849 net.cpp:408] fc7 -> fc7
I0524 09:18:40.588372   849 net.cpp:150] Setting up fc7
I0524 09:18:40.588412   849 net.cpp:157] Top shape: 1 2048 (2048)
I0524 09:18:40.588415   849 net.cpp:165] Memory required for data: 209885212
I0524 09:18:40.588444   849 layer_factory.hpp:77] Creating layer relu7
I0524 09:18:40.588490   849 net.cpp:100] Creating Layer relu7
I0524 09:18:40.588500   849 net.cpp:434] relu7 <- fc7
I0524 09:18:40.588507   849 net.cpp:395] relu7 -> fc7 (in-place)
I0524 09:18:40.588865   849 net.cpp:150] Setting up relu7
I0524 09:18:40.588878   849 net.cpp:157] Top shape: 1 2048 (2048)
I0524 09:18:40.588882   849 net.cpp:165] Memory required for data: 209893404
I0524 09:18:40.588886   849 layer_factory.hpp:77] Creating layer drop7
I0524 09:18:40.588893   849 net.cpp:100] Creating Layer drop7
I0524 09:18:40.588898   849 net.cpp:434] drop7 <- fc7
I0524 09:18:40.588904   849 net.cpp:395] drop7 -> fc7 (in-place)
I0524 09:18:40.588944   849 net.cpp:150] Setting up drop7
I0524 09:18:40.588956   849 net.cpp:157] Top shape: 1 2048 (2048)
I0524 09:18:40.588963   849 net.cpp:165] Memory required for data: 209901596
I0524 09:18:40.588968   849 layer_factory.hpp:77] Creating layer fc8
I0524 09:18:40.588989   849 net.cpp:100] Creating Layer fc8
I0524 09:18:40.588994   849 net.cpp:434] fc8 <- fc7
I0524 09:18:40.589002   849 net.cpp:408] fc8 -> fc8
I0524 09:18:40.590010   849 net.cpp:150] Setting up fc8
I0524 09:18:40.590026   849 net.cpp:157] Top shape: 1 10 (10)
I0524 09:18:40.590072   849 net.cpp:165] Memory required for data: 209901636
I0524 09:18:40.590085   849 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0524 09:18:40.590111   849 net.cpp:100] Creating Layer fc8_fc8_0_split
I0524 09:18:40.590117   849 net.cpp:434] fc8_fc8_0_split <- fc8
I0524 09:18:40.590126   849 net.cpp:408] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0524 09:18:40.590139   849 net.cpp:408] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0524 09:18:40.590201   849 net.cpp:150] Setting up fc8_fc8_0_split
I0524 09:18:40.590212   849 net.cpp:157] Top shape: 1 10 (10)
I0524 09:18:40.590220   849 net.cpp:157] Top shape: 1 10 (10)
I0524 09:18:40.590226   849 net.cpp:165] Memory required for data: 209901716
I0524 09:18:40.590231   849 layer_factory.hpp:77] Creating layer prob
I0524 09:18:40.590250   849 net.cpp:100] Creating Layer prob
I0524 09:18:40.590258   849 net.cpp:434] prob <- fc8_fc8_0_split_0
I0524 09:18:40.590265   849 net.cpp:408] prob -> prob
I0524 09:18:40.590632   849 net.cpp:150] Setting up prob
I0524 09:18:40.590647   849 net.cpp:157] Top shape: 1 10 (10)
I0524 09:18:40.590653   849 net.cpp:165] Memory required for data: 209901756
I0524 09:18:40.590659   849 layer_factory.hpp:77] Creating layer prob_prob_0_split
I0524 09:18:40.590668   849 net.cpp:100] Creating Layer prob_prob_0_split
I0524 09:18:40.590675   849 net.cpp:434] prob_prob_0_split <- prob
I0524 09:18:40.590685   849 net.cpp:408] prob_prob_0_split -> prob_prob_0_split_0
I0524 09:18:40.590697   849 net.cpp:408] prob_prob_0_split -> prob_prob_0_split_1
I0524 09:18:40.590764   849 net.cpp:150] Setting up prob_prob_0_split
I0524 09:18:40.590773   849 net.cpp:157] Top shape: 1 10 (10)
I0524 09:18:40.590782   849 net.cpp:157] Top shape: 1 10 (10)
I0524 09:18:40.590788   849 net.cpp:165] Memory required for data: 209901836
I0524 09:18:40.590795   849 layer_factory.hpp:77] Creating layer accuracy
I0524 09:18:40.590806   849 net.cpp:100] Creating Layer accuracy
I0524 09:18:40.590812   849 net.cpp:434] accuracy <- prob_prob_0_split_0
I0524 09:18:40.590821   849 net.cpp:434] accuracy <- label_label_parse_0_split_0
I0524 09:18:40.590832   849 net.cpp:408] accuracy -> accuracy/top-1
I0524 09:18:40.590844   849 net.cpp:150] Setting up accuracy
I0524 09:18:40.590852   849 net.cpp:157] Top shape: (1)
I0524 09:18:40.590858   849 net.cpp:165] Memory required for data: 209901840
I0524 09:18:40.590863   849 layer_factory.hpp:77] Creating layer accuracy
I0524 09:18:40.590894   849 net.cpp:100] Creating Layer accuracy
I0524 09:18:40.590901   849 net.cpp:434] accuracy <- prob_prob_0_split_1
I0524 09:18:40.590921   849 net.cpp:434] accuracy <- label_label_parse_0_split_1
I0524 09:18:40.590929   849 net.cpp:408] accuracy -> accuracy/top-5
I0524 09:18:40.590939   849 net.cpp:150] Setting up accuracy
I0524 09:18:40.590947   849 net.cpp:157] Top shape: (1)
I0524 09:18:40.590955   849 net.cpp:165] Memory required for data: 209901844
I0524 09:18:40.590960   849 layer_factory.hpp:77] Creating layer loss
I0524 09:18:40.590967   849 net.cpp:100] Creating Layer loss
I0524 09:18:40.590975   849 net.cpp:434] loss <- fc8_fc8_0_split_1
I0524 09:18:40.590981   849 net.cpp:434] loss <- label_label_parse_0_split_2
I0524 09:18:40.590997   849 net.cpp:408] loss -> loss
I0524 09:18:40.591011   849 layer_factory.hpp:77] Creating layer loss
I0524 09:18:40.592283   849 net.cpp:150] Setting up loss
I0524 09:18:40.592298   849 net.cpp:157] Top shape: (1)
I0524 09:18:40.592301   849 net.cpp:160]     with loss weight 1
I0524 09:18:40.592317   849 net.cpp:165] Memory required for data: 209901848
I0524 09:18:40.592320   849 net.cpp:226] loss needs backward computation.
I0524 09:18:40.592324   849 net.cpp:228] accuracy does not need backward computation.
I0524 09:18:40.592329   849 net.cpp:228] accuracy does not need backward computation.
I0524 09:18:40.592331   849 net.cpp:228] prob_prob_0_split does not need backward computation.
I0524 09:18:40.592334   849 net.cpp:228] prob does not need backward computation.
I0524 09:18:40.592337   849 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0524 09:18:40.592352   849 net.cpp:226] fc8 needs backward computation.
I0524 09:18:40.592356   849 net.cpp:228] drop7 does not need backward computation.
I0524 09:18:40.592358   849 net.cpp:228] relu7 does not need backward computation.
I0524 09:18:40.592361   849 net.cpp:228] fc7 does not need backward computation.
I0524 09:18:40.592365   849 net.cpp:228] drop6 does not need backward computation.
I0524 09:18:40.592367   849 net.cpp:228] relu6 does not need backward computation.
I0524 09:18:40.592370   849 net.cpp:228] fc6 does not need backward computation.
I0524 09:18:40.592373   849 net.cpp:228] pool5 does not need backward computation.
I0524 09:18:40.592377   849 net.cpp:228] relu5a does not need backward computation.
I0524 09:18:40.592381   849 net.cpp:228] conv5a does not need backward computation.
I0524 09:18:40.592383   849 net.cpp:228] pool4 does not need backward computation.
I0524 09:18:40.592388   849 net.cpp:228] relu4a does not need backward computation.
I0524 09:18:40.592391   849 net.cpp:228] conv4a does not need backward computation.
I0524 09:18:40.592396   849 net.cpp:228] pool3 does not need backward computation.
I0524 09:18:40.592398   849 net.cpp:228] relu3a does not need backward computation.
I0524 09:18:40.592401   849 net.cpp:228] conv3a does not need backward computation.
I0524 09:18:40.592404   849 net.cpp:228] pool2 does not need backward computation.
I0524 09:18:40.592407   849 net.cpp:228] relu2a does not need backward computation.
I0524 09:18:40.592411   849 net.cpp:228] conv2a does not need backward computation.
I0524 09:18:40.592413   849 net.cpp:228] pool1 does not need backward computation.
I0524 09:18:40.592417   849 net.cpp:228] relu1a does not need backward computation.
I0524 09:18:40.592420   849 net.cpp:228] conv1a does not need backward computation.
I0524 09:18:40.592423   849 net.cpp:228] silence_first_person does not need backward computation.
I0524 09:18:40.592429   849 net.cpp:228] label_label_parse_0_split does not need backward computation.
I0524 09:18:40.592435   849 net.cpp:228] label_parse does not need backward computation.
I0524 09:18:40.592442   849 net.cpp:228] label_split does not need backward computation.
I0524 09:18:40.592447   849 net.cpp:228] data_switch does not need backward computation.
I0524 09:18:40.592456   849 net.cpp:228] silence_negative does not need backward computation.
I0524 09:18:40.592463   849 net.cpp:228] reshape_positive does not need backward computation.
I0524 09:18:40.592470   849 net.cpp:228] reshape_anchor does not need backward computation.
I0524 09:18:40.592479   849 net.cpp:228] slicer does not need backward computation.
I0524 09:18:40.592489   849 net.cpp:228] data does not need backward computation.
I0524 09:18:40.592494   849 net.cpp:270] This network produces output accuracy/top-1
I0524 09:18:40.592500   849 net.cpp:270] This network produces output accuracy/top-5
I0524 09:18:40.592507   849 net.cpp:270] This network produces output loss
I0524 09:18:40.592559   849 net.cpp:283] Network initialization done.
I0524 09:18:40.592736   849 solver.cpp:60] Solver scaffolding done.
I0524 09:18:40.593437   849 caffe.cpp:155] Finetuning from ../../weights/three_stream_triplet_loss_iter_60000.caffemodel
I0524 09:18:41.334151   849 net.cpp:761] Ignoring source layer reshape_negative
I0524 09:18:41.352735   849 net.cpp:761] Ignoring source layer mvn
I0524 09:18:41.352777   849 net.cpp:761] Ignoring source layer fc7_norm_mvn_0_split
I0524 09:18:41.352780   849 net.cpp:761] Ignoring source layer conv1a_pos
I0524 09:18:41.352782   849 net.cpp:761] Ignoring source layer relu1a_pos
I0524 09:18:41.352785   849 net.cpp:761] Ignoring source layer pool1_pos
I0524 09:18:41.352787   849 net.cpp:761] Ignoring source layer conv2a_pos
I0524 09:18:41.352790   849 net.cpp:761] Ignoring source layer relu2a_pos
I0524 09:18:41.352792   849 net.cpp:761] Ignoring source layer pool2_pos
I0524 09:18:41.352795   849 net.cpp:761] Ignoring source layer conv3a_pos
I0524 09:18:41.352799   849 net.cpp:761] Ignoring source layer relu3a_pos
I0524 09:18:41.352802   849 net.cpp:761] Ignoring source layer pool3_pos
I0524 09:18:41.352836   849 net.cpp:761] Ignoring source layer conv4a_pos
I0524 09:18:41.352839   849 net.cpp:761] Ignoring source layer relu4a_pos
I0524 09:18:41.352852   849 net.cpp:761] Ignoring source layer pool4_pos
I0524 09:18:41.352855   849 net.cpp:761] Ignoring source layer conv5a_pos
I0524 09:18:41.352857   849 net.cpp:761] Ignoring source layer relu5a_pos
I0524 09:18:41.352860   849 net.cpp:761] Ignoring source layer pool5_pos
I0524 09:18:41.352862   849 net.cpp:761] Ignoring source layer fc6_pos
I0524 09:18:41.352865   849 net.cpp:761] Ignoring source layer relu6_pos
I0524 09:18:41.352867   849 net.cpp:761] Ignoring source layer drop6_pos
I0524 09:18:41.352870   849 net.cpp:761] Ignoring source layer fc7_pos
I0524 09:18:41.352872   849 net.cpp:761] Ignoring source layer relu7_pos
I0524 09:18:41.352875   849 net.cpp:761] Ignoring source layer drop7_pos
I0524 09:18:41.352877   849 net.cpp:761] Ignoring source layer mvn_pos
I0524 09:18:41.352880   849 net.cpp:761] Ignoring source layer fc7_pos_norm_mvn_pos_0_split
I0524 09:18:41.352882   849 net.cpp:761] Ignoring source layer conv1a_neg
I0524 09:18:41.352885   849 net.cpp:761] Ignoring source layer relu1a_neg
I0524 09:18:41.352887   849 net.cpp:761] Ignoring source layer pool1_neg
I0524 09:18:41.352890   849 net.cpp:761] Ignoring source layer conv2a_neg
I0524 09:18:41.352892   849 net.cpp:761] Ignoring source layer relu2a_neg
I0524 09:18:41.352895   849 net.cpp:761] Ignoring source layer pool2_neg
I0524 09:18:41.352896   849 net.cpp:761] Ignoring source layer conv3a_neg
I0524 09:18:41.352900   849 net.cpp:761] Ignoring source layer relu3a_neg
I0524 09:18:41.352901   849 net.cpp:761] Ignoring source layer pool3_neg
I0524 09:18:41.352905   849 net.cpp:761] Ignoring source layer conv4a_neg
I0524 09:18:41.352907   849 net.cpp:761] Ignoring source layer relu4a_neg
I0524 09:18:41.352910   849 net.cpp:761] Ignoring source layer pool4_neg
I0524 09:18:41.352912   849 net.cpp:761] Ignoring source layer conv5a_neg
I0524 09:18:41.352915   849 net.cpp:761] Ignoring source layer relu5a_neg
I0524 09:18:41.352917   849 net.cpp:761] Ignoring source layer pool5_neg
I0524 09:18:41.352919   849 net.cpp:761] Ignoring source layer fc6_neg
I0524 09:18:41.352922   849 net.cpp:761] Ignoring source layer relu6_neg
I0524 09:18:41.352924   849 net.cpp:761] Ignoring source layer drop6_neg
I0524 09:18:41.352926   849 net.cpp:761] Ignoring source layer fc7_neg
I0524 09:18:41.352929   849 net.cpp:761] Ignoring source layer relu7_neg
I0524 09:18:41.352931   849 net.cpp:761] Ignoring source layer drop7_neg
I0524 09:18:41.352934   849 net.cpp:761] Ignoring source layer mvn_neg
I0524 09:18:41.352936   849 net.cpp:761] Ignoring source layer fc7_neg_norm_mvn_neg_0_split
I0524 09:18:41.352941   849 net.cpp:761] Ignoring source layer triplet_check
I0524 09:18:42.016449   849 net.cpp:761] Ignoring source layer reshape_negative
I0524 09:18:42.027779   849 net.cpp:761] Ignoring source layer mvn
I0524 09:18:42.027799   849 net.cpp:761] Ignoring source layer fc7_norm_mvn_0_split
I0524 09:18:42.027813   849 net.cpp:761] Ignoring source layer conv1a_pos
I0524 09:18:42.027817   849 net.cpp:761] Ignoring source layer relu1a_pos
I0524 09:18:42.027819   849 net.cpp:761] Ignoring source layer pool1_pos
I0524 09:18:42.027822   849 net.cpp:761] Ignoring source layer conv2a_pos
I0524 09:18:42.027824   849 net.cpp:761] Ignoring source layer relu2a_pos
I0524 09:18:42.027827   849 net.cpp:761] Ignoring source layer pool2_pos
I0524 09:18:42.027829   849 net.cpp:761] Ignoring source layer conv3a_pos
I0524 09:18:42.027832   849 net.cpp:761] Ignoring source layer relu3a_pos
I0524 09:18:42.027834   849 net.cpp:761] Ignoring source layer pool3_pos
I0524 09:18:42.027837   849 net.cpp:761] Ignoring source layer conv4a_pos
I0524 09:18:42.027839   849 net.cpp:761] Ignoring source layer relu4a_pos
I0524 09:18:42.027842   849 net.cpp:761] Ignoring source layer pool4_pos
I0524 09:18:42.027844   849 net.cpp:761] Ignoring source layer conv5a_pos
I0524 09:18:42.027846   849 net.cpp:761] Ignoring source layer relu5a_pos
I0524 09:18:42.027894   849 net.cpp:761] Ignoring source layer pool5_pos
I0524 09:18:42.027895   849 net.cpp:761] Ignoring source layer fc6_pos
I0524 09:18:42.027899   849 net.cpp:761] Ignoring source layer relu6_pos
I0524 09:18:42.027900   849 net.cpp:761] Ignoring source layer drop6_pos
I0524 09:18:42.027904   849 net.cpp:761] Ignoring source layer fc7_pos
I0524 09:18:42.027906   849 net.cpp:761] Ignoring source layer relu7_pos
I0524 09:18:42.027909   849 net.cpp:761] Ignoring source layer drop7_pos
I0524 09:18:42.027911   849 net.cpp:761] Ignoring source layer mvn_pos
I0524 09:18:42.027914   849 net.cpp:761] Ignoring source layer fc7_pos_norm_mvn_pos_0_split
I0524 09:18:42.027916   849 net.cpp:761] Ignoring source layer conv1a_neg
I0524 09:18:42.027918   849 net.cpp:761] Ignoring source layer relu1a_neg
I0524 09:18:42.027921   849 net.cpp:761] Ignoring source layer pool1_neg
I0524 09:18:42.027923   849 net.cpp:761] Ignoring source layer conv2a_neg
I0524 09:18:42.027926   849 net.cpp:761] Ignoring source layer relu2a_neg
I0524 09:18:42.027928   849 net.cpp:761] Ignoring source layer pool2_neg
I0524 09:18:42.027930   849 net.cpp:761] Ignoring source layer conv3a_neg
I0524 09:18:42.027933   849 net.cpp:761] Ignoring source layer relu3a_neg
I0524 09:18:42.027935   849 net.cpp:761] Ignoring source layer pool3_neg
I0524 09:18:42.027937   849 net.cpp:761] Ignoring source layer conv4a_neg
I0524 09:18:42.027940   849 net.cpp:761] Ignoring source layer relu4a_neg
I0524 09:18:42.027942   849 net.cpp:761] Ignoring source layer pool4_neg
I0524 09:18:42.027945   849 net.cpp:761] Ignoring source layer conv5a_neg
I0524 09:18:42.027947   849 net.cpp:761] Ignoring source layer relu5a_neg
I0524 09:18:42.027951   849 net.cpp:761] Ignoring source layer pool5_neg
I0524 09:18:42.027953   849 net.cpp:761] Ignoring source layer fc6_neg
I0524 09:18:42.027956   849 net.cpp:761] Ignoring source layer relu6_neg
I0524 09:18:42.027958   849 net.cpp:761] Ignoring source layer drop6_neg
I0524 09:18:42.027961   849 net.cpp:761] Ignoring source layer fc7_neg
I0524 09:18:42.027962   849 net.cpp:761] Ignoring source layer relu7_neg
I0524 09:18:42.027966   849 net.cpp:761] Ignoring source layer drop7_neg
I0524 09:18:42.027967   849 net.cpp:761] Ignoring source layer mvn_neg
I0524 09:18:42.027969   849 net.cpp:761] Ignoring source layer fc7_neg_norm_mvn_neg_0_split
I0524 09:18:42.027972   849 net.cpp:761] Ignoring source layer triplet_check
I0524 09:18:42.038782   849 caffe.cpp:251] Starting Optimization
I0524 09:18:42.038806   849 solver.cpp:279] Solving C3D-TP-Classifier
I0524 09:18:42.038811   849 solver.cpp:280] Learning Rate Policy: step
I0524 09:18:42.044850   849 solver.cpp:337] Iteration 0, Testing net (#0)
I0524 09:18:42.199828   849 blocking_queue.cpp:50] Data layer prefetch queue empty
I0524 09:18:49.434197   849 solver.cpp:404]     Test net output #0: accuracy/top-1 = 0.116
I0524 09:18:49.434243   849 solver.cpp:404]     Test net output #1: accuracy/top-5 = 0.45
I0524 09:18:49.434253   849 solver.cpp:404]     Test net output #2: loss = 11.8422 (* 1 = 11.8422 loss)
I0524 09:18:49.840593   849 solver.cpp:228] Iteration 0, loss = 2.51515
I0524 09:18:49.840641   849 solver.cpp:244]     Train net output #0: loss = 2.51515 (* 1 = 2.51515 loss)
I0524 09:18:49.840662   849 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0524 09:18:50.171612   849 solver.cpp:228] Iteration 1, loss = 2.60179
I0524 09:18:50.171658   849 solver.cpp:244]     Train net output #0: loss = 2.60179 (* 1 = 2.60179 loss)
I0524 09:18:50.171670   849 sgd_solver.cpp:106] Iteration 1, lr = 0.001
I0524 09:18:50.479249   849 solver.cpp:228] Iteration 2, loss = 2.25699
I0524 09:18:50.479310   849 solver.cpp:244]     Train net output #0: loss = 2.25699 (* 1 = 2.25699 loss)
I0524 09:18:50.479326   849 sgd_solver.cpp:106] Iteration 2, lr = 0.001
I0524 09:18:50.808403   849 solver.cpp:228] Iteration 3, loss = 2.36167
I0524 09:18:50.808459   849 solver.cpp:244]     Train net output #0: loss = 2.36167 (* 1 = 2.36167 loss)
I0524 09:18:50.808472   849 sgd_solver.cpp:106] Iteration 3, lr = 0.001
I0524 09:18:51.131206   849 solver.cpp:228] Iteration 4, loss = 2.34795
I0524 09:18:51.131249   849 solver.cpp:244]     Train net output #0: loss = 2.34795 (* 1 = 2.34795 loss)
I0524 09:18:51.131259   849 sgd_solver.cpp:106] Iteration 4, lr = 0.001
I0524 09:18:51.440425   849 solver.cpp:228] Iteration 5, loss = 2.34709
I0524 09:18:51.440464   849 solver.cpp:244]     Train net output #0: loss = 2.34709 (* 1 = 2.34709 loss)
I0524 09:18:51.440474   849 sgd_solver.cpp:106] Iteration 5, lr = 0.001
I0524 09:18:51.756001   849 solver.cpp:228] Iteration 6, loss = 2.43532
I0524 09:18:51.756050   849 solver.cpp:244]     Train net output #0: loss = 2.43532 (* 1 = 2.43532 loss)
I0524 09:18:51.756059   849 sgd_solver.cpp:106] Iteration 6, lr = 0.001
I0524 09:18:52.068200   849 solver.cpp:228] Iteration 7, loss = 2.33352
I0524 09:18:52.068256   849 solver.cpp:244]     Train net output #0: loss = 2.33352 (* 1 = 2.33352 loss)
I0524 09:18:52.068265   849 sgd_solver.cpp:106] Iteration 7, lr = 0.001
I0524 09:18:52.384090   849 solver.cpp:228] Iteration 8, loss = 2.39829
I0524 09:18:52.384155   849 solver.cpp:244]     Train net output #0: loss = 2.39829 (* 1 = 2.39829 loss)
I0524 09:18:52.384173   849 sgd_solver.cpp:106] Iteration 8, lr = 0.001
I0524 09:18:52.702824   849 solver.cpp:228] Iteration 9, loss = 2.52917
I0524 09:18:52.702877   849 solver.cpp:244]     Train net output #0: loss = 2.52917 (* 1 = 2.52917 loss)
I0524 09:18:52.702888   849 sgd_solver.cpp:106] Iteration 9, lr = 0.001
I0524 09:18:53.019315   849 solver.cpp:228] Iteration 10, loss = 2.37747
I0524 09:18:53.019392   849 solver.cpp:244]     Train net output #0: loss = 2.37747 (* 1 = 2.37747 loss)
I0524 09:18:53.019403   849 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I0524 09:18:53.334484   849 solver.cpp:228] Iteration 11, loss = 2.52566
I0524 09:18:53.334533   849 solver.cpp:244]     Train net output #0: loss = 2.52566 (* 1 = 2.52566 loss)
I0524 09:18:53.334543   849 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I0524 09:18:53.661419   849 solver.cpp:228] Iteration 12, loss = 2.4393
I0524 09:18:53.661468   849 solver.cpp:244]     Train net output #0: loss = 2.4393 (* 1 = 2.4393 loss)
I0524 09:18:53.661478   849 sgd_solver.cpp:106] Iteration 12, lr = 0.001
I0524 09:18:53.995653   849 solver.cpp:228] Iteration 13, loss = 2.37423
I0524 09:18:53.995707   849 solver.cpp:244]     Train net output #0: loss = 2.37423 (* 1 = 2.37423 loss)
I0524 09:18:53.995718   849 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I0524 09:18:54.306898   849 solver.cpp:228] Iteration 14, loss = 2.40324
I0524 09:18:54.306972   849 solver.cpp:244]     Train net output #0: loss = 2.40324 (* 1 = 2.40324 loss)
I0524 09:18:54.306980   849 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I0524 09:18:54.635262   849 solver.cpp:228] Iteration 15, loss = 2.37409
I0524 09:18:54.635329   849 solver.cpp:244]     Train net output #0: loss = 2.37409 (* 1 = 2.37409 loss)
I0524 09:18:54.635337   849 sgd_solver.cpp:106] Iteration 15, lr = 0.001
I0524 09:18:54.960119   849 solver.cpp:228] Iteration 16, loss = 2.45273
I0524 09:18:54.960177   849 solver.cpp:244]     Train net output #0: loss = 2.45273 (* 1 = 2.45273 loss)
I0524 09:18:54.960188   849 sgd_solver.cpp:106] Iteration 16, lr = 0.001
I0524 09:18:55.278111   849 solver.cpp:228] Iteration 17, loss = 2.50964
I0524 09:18:55.278177   849 solver.cpp:244]     Train net output #0: loss = 2.50964 (* 1 = 2.50964 loss)
I0524 09:18:55.278190   849 sgd_solver.cpp:106] Iteration 17, lr = 0.001
I0524 09:18:55.602718   849 solver.cpp:228] Iteration 18, loss = 2.47165
I0524 09:18:55.602773   849 solver.cpp:244]     Train net output #0: loss = 2.47165 (* 1 = 2.47165 loss)
I0524 09:18:55.602784   849 sgd_solver.cpp:106] Iteration 18, lr = 0.001
I0524 09:18:55.927790   849 solver.cpp:228] Iteration 19, loss = 6.3687
I0524 09:18:55.927846   849 solver.cpp:244]     Train net output #0: loss = 6.3687 (* 1 = 6.3687 loss)
I0524 09:18:55.927855   849 sgd_solver.cpp:106] Iteration 19, lr = 0.001
I0524 09:18:56.239758   849 solver.cpp:228] Iteration 20, loss = 2.50128
I0524 09:18:56.239874   849 solver.cpp:244]     Train net output #0: loss = 2.50128 (* 1 = 2.50128 loss)
I0524 09:18:56.239886   849 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I0524 09:18:56.556668   849 solver.cpp:228] Iteration 21, loss = 2.47684
I0524 09:18:56.556725   849 solver.cpp:244]     Train net output #0: loss = 2.47684 (* 1 = 2.47684 loss)
I0524 09:18:56.556735   849 sgd_solver.cpp:106] Iteration 21, lr = 0.001
I0524 09:18:56.875950   849 solver.cpp:228] Iteration 22, loss = 2.53507
I0524 09:18:56.876006   849 solver.cpp:244]     Train net output #0: loss = 2.53507 (* 1 = 2.53507 loss)
I0524 09:18:56.876015   849 sgd_solver.cpp:106] Iteration 22, lr = 0.001
I0524 09:18:57.183948   849 solver.cpp:228] Iteration 23, loss = 6.05019
I0524 09:18:57.184017   849 solver.cpp:244]     Train net output #0: loss = 6.05019 (* 1 = 6.05019 loss)
I0524 09:18:57.184031   849 sgd_solver.cpp:106] Iteration 23, lr = 0.001
I0524 09:18:57.495977   849 solver.cpp:228] Iteration 24, loss = 2.38514
I0524 09:18:57.496026   849 solver.cpp:244]     Train net output #0: loss = 2.38514 (* 1 = 2.38514 loss)
I0524 09:18:57.496034   849 sgd_solver.cpp:106] Iteration 24, lr = 0.001
I0524 09:18:57.806226   849 solver.cpp:228] Iteration 25, loss = 2.47429
I0524 09:18:57.806277   849 solver.cpp:244]     Train net output #0: loss = 2.47429 (* 1 = 2.47429 loss)
I0524 09:18:57.806288   849 sgd_solver.cpp:106] Iteration 25, lr = 0.001
I0524 09:18:58.125902   849 solver.cpp:228] Iteration 26, loss = 2.3945
I0524 09:18:58.126140   849 solver.cpp:244]     Train net output #0: loss = 2.3945 (* 1 = 2.3945 loss)
I0524 09:18:58.126175   849 sgd_solver.cpp:106] Iteration 26, lr = 0.001
I0524 09:18:58.435951   849 solver.cpp:228] Iteration 27, loss = 2.35108
I0524 09:18:58.436003   849 solver.cpp:244]     Train net output #0: loss = 2.35108 (* 1 = 2.35108 loss)
I0524 09:18:58.436012   849 sgd_solver.cpp:106] Iteration 27, lr = 0.001
I0524 09:18:58.774418   849 solver.cpp:228] Iteration 28, loss = 2.39642
I0524 09:18:58.774471   849 solver.cpp:244]     Train net output #0: loss = 2.39642 (* 1 = 2.39642 loss)
I0524 09:18:58.774482   849 sgd_solver.cpp:106] Iteration 28, lr = 0.001
I0524 09:18:59.097506   849 solver.cpp:228] Iteration 29, loss = 6.11818
I0524 09:18:59.097559   849 solver.cpp:244]     Train net output #0: loss = 6.11818 (* 1 = 6.11818 loss)
I0524 09:18:59.097571   849 sgd_solver.cpp:106] Iteration 29, lr = 0.001
I0524 09:18:59.415336   849 solver.cpp:228] Iteration 30, loss = 2.40828
I0524 09:18:59.415395   849 solver.cpp:244]     Train net output #0: loss = 2.40828 (* 1 = 2.40828 loss)
I0524 09:18:59.415405   849 sgd_solver.cpp:106] Iteration 30, lr = 0.001
I0524 09:18:59.727282   849 solver.cpp:228] Iteration 31, loss = 2.37006
I0524 09:18:59.727339   849 solver.cpp:244]     Train net output #0: loss = 2.37006 (* 1 = 2.37006 loss)
I0524 09:18:59.727347   849 sgd_solver.cpp:106] Iteration 31, lr = 0.001
I0524 09:19:00.032109   849 solver.cpp:228] Iteration 32, loss = 2.364
I0524 09:19:00.032167   849 solver.cpp:244]     Train net output #0: loss = 2.364 (* 1 = 2.364 loss)
I0524 09:19:00.032177   849 sgd_solver.cpp:106] Iteration 32, lr = 0.001
I0524 09:19:00.353446   849 solver.cpp:228] Iteration 33, loss = 2.39613
I0524 09:19:00.353507   849 solver.cpp:244]     Train net output #0: loss = 2.39613 (* 1 = 2.39613 loss)
I0524 09:19:00.353518   849 sgd_solver.cpp:106] Iteration 33, lr = 0.001
I0524 09:19:00.736462   849 solver.cpp:228] Iteration 34, loss = 6.14344
I0524 09:19:00.736507   849 solver.cpp:244]     Train net output #0: loss = 6.14344 (* 1 = 6.14344 loss)
I0524 09:19:00.736516   849 sgd_solver.cpp:106] Iteration 34, lr = 0.001
I0524 09:19:01.055649   849 solver.cpp:228] Iteration 35, loss = 2.50495
I0524 09:19:01.055691   849 solver.cpp:244]     Train net output #0: loss = 2.50495 (* 1 = 2.50495 loss)
I0524 09:19:01.055701   849 sgd_solver.cpp:106] Iteration 35, lr = 0.001
I0524 09:19:01.371690   849 solver.cpp:228] Iteration 36, loss = 6.15658
I0524 09:19:01.371737   849 solver.cpp:244]     Train net output #0: loss = 6.15658 (* 1 = 6.15658 loss)
I0524 09:19:01.371798   849 sgd_solver.cpp:106] Iteration 36, lr = 0.001
I0524 09:19:01.695030   849 solver.cpp:228] Iteration 37, loss = 5.97987
I0524 09:19:01.695116   849 solver.cpp:244]     Train net output #0: loss = 5.97987 (* 1 = 5.97987 loss)
I0524 09:19:01.695129   849 sgd_solver.cpp:106] Iteration 37, lr = 0.001
I0524 09:19:02.033053   849 solver.cpp:228] Iteration 38, loss = 5.92388
I0524 09:19:02.033099   849 solver.cpp:244]     Train net output #0: loss = 5.92388 (* 1 = 5.92388 loss)
I0524 09:19:02.033107   849 sgd_solver.cpp:106] Iteration 38, lr = 0.001
I0524 09:19:02.349447   849 solver.cpp:228] Iteration 39, loss = 2.48174
I0524 09:19:02.349484   849 solver.cpp:244]     Train net output #0: loss = 2.48174 (* 1 = 2.48174 loss)
I0524 09:19:02.349493   849 sgd_solver.cpp:106] Iteration 39, lr = 0.001
I0524 09:19:02.664952   849 solver.cpp:228] Iteration 40, loss = 2.32622
I0524 09:19:02.665025   849 solver.cpp:244]     Train net output #0: loss = 2.32622 (* 1 = 2.32622 loss)
I0524 09:19:02.665038   849 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I0524 09:19:02.975169   849 solver.cpp:228] Iteration 41, loss = 2.51189
I0524 09:19:02.975217   849 solver.cpp:244]     Train net output #0: loss = 2.51189 (* 1 = 2.51189 loss)
I0524 09:19:02.975226   849 sgd_solver.cpp:106] Iteration 41, lr = 0.001
I0524 09:19:03.301285   849 solver.cpp:228] Iteration 42, loss = 2.31246
I0524 09:19:03.301343   849 solver.cpp:244]     Train net output #0: loss = 2.31246 (* 1 = 2.31246 loss)
I0524 09:19:03.301353   849 sgd_solver.cpp:106] Iteration 42, lr = 0.001
I0524 09:19:03.655899   849 solver.cpp:228] Iteration 43, loss = 2.70415
I0524 09:19:03.655947   849 solver.cpp:244]     Train net output #0: loss = 2.70415 (* 1 = 2.70415 loss)
I0524 09:19:03.655957   849 sgd_solver.cpp:106] Iteration 43, lr = 0.001
I0524 09:19:03.979753   849 solver.cpp:228] Iteration 44, loss = 2.35323
I0524 09:19:03.979812   849 solver.cpp:244]     Train net output #0: loss = 2.35323 (* 1 = 2.35323 loss)
I0524 09:19:03.979823   849 sgd_solver.cpp:106] Iteration 44, lr = 0.001
I0524 09:19:04.298698   849 solver.cpp:228] Iteration 45, loss = 2.24346
I0524 09:19:04.298758   849 solver.cpp:244]     Train net output #0: loss = 2.24346 (* 1 = 2.24346 loss)
I0524 09:19:04.298768   849 sgd_solver.cpp:106] Iteration 45, lr = 0.001
I0524 09:19:04.643337   849 solver.cpp:228] Iteration 46, loss = 2.32651
I0524 09:19:04.643390   849 solver.cpp:244]     Train net output #0: loss = 2.32651 (* 1 = 2.32651 loss)
I0524 09:19:04.643400   849 sgd_solver.cpp:106] Iteration 46, lr = 0.001
I0524 09:19:04.849047   857 blocking_queue.cpp:50] Waiting for data
I0524 09:19:04.948930   849 solver.cpp:228] Iteration 47, loss = 2.47826
I0524 09:19:04.948976   849 solver.cpp:244]     Train net output #0: loss = 2.47826 (* 1 = 2.47826 loss)
I0524 09:19:04.948985   849 sgd_solver.cpp:106] Iteration 47, lr = 0.001
I0524 09:19:05.238143   849 solver.cpp:228] Iteration 48, loss = 2.19653
I0524 09:19:05.238199   849 solver.cpp:244]     Train net output #0: loss = 2.19653 (* 1 = 2.19653 loss)
I0524 09:19:05.238209   849 sgd_solver.cpp:106] Iteration 48, lr = 0.001
I0524 09:19:05.535048   849 solver.cpp:228] Iteration 49, loss = 2.42316
I0524 09:19:05.535101   849 solver.cpp:244]     Train net output #0: loss = 2.42316 (* 1 = 2.42316 loss)
I0524 09:19:05.535114   849 sgd_solver.cpp:106] Iteration 49, lr = 0.001
I0524 09:19:05.867790   849 solver.cpp:228] Iteration 50, loss = 2.50573
I0524 09:19:05.867846   849 solver.cpp:244]     Train net output #0: loss = 2.50573 (* 1 = 2.50573 loss)
I0524 09:19:05.867864   849 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I0524 09:19:07.016335   849 solver.cpp:228] Iteration 51, loss = 2.61693
I0524 09:19:07.016476   849 solver.cpp:244]     Train net output #0: loss = 2.61693 (* 1 = 2.61693 loss)
I0524 09:19:07.016489   849 sgd_solver.cpp:106] Iteration 51, lr = 0.001
I0524 09:19:08.386319   849 solver.cpp:228] Iteration 52, loss = 2.25864
I0524 09:19:08.386376   849 solver.cpp:244]     Train net output #0: loss = 2.25864 (* 1 = 2.25864 loss)
I0524 09:19:08.386387   849 sgd_solver.cpp:106] Iteration 52, lr = 0.001
I0524 09:19:09.695822   849 solver.cpp:228] Iteration 53, loss = 2.52079
I0524 09:19:09.695869   849 solver.cpp:244]     Train net output #0: loss = 2.52079 (* 1 = 2.52079 loss)
I0524 09:19:09.695880   849 sgd_solver.cpp:106] Iteration 53, lr = 0.001
I0524 09:19:10.363427   849 solver.cpp:228] Iteration 54, loss = 2.41874
I0524 09:19:10.363472   849 solver.cpp:244]     Train net output #0: loss = 2.41874 (* 1 = 2.41874 loss)
I0524 09:19:10.363481   849 sgd_solver.cpp:106] Iteration 54, lr = 0.001
I0524 09:19:10.927968   849 solver.cpp:228] Iteration 55, loss = 2.58033
I0524 09:19:10.928019   849 solver.cpp:244]     Train net output #0: loss = 2.58033 (* 1 = 2.58033 loss)
I0524 09:19:10.928028   849 sgd_solver.cpp:106] Iteration 55, lr = 0.001
I0524 09:19:11.763025   849 solver.cpp:228] Iteration 56, loss = 2.37496
I0524 09:19:11.763104   849 solver.cpp:244]     Train net output #0: loss = 2.37496 (* 1 = 2.37496 loss)
I0524 09:19:11.763116   849 sgd_solver.cpp:106] Iteration 56, lr = 0.001
I0524 09:19:12.551175   849 solver.cpp:228] Iteration 57, loss = 2.52112
I0524 09:19:12.551239   849 solver.cpp:244]     Train net output #0: loss = 2.52112 (* 1 = 2.52112 loss)
I0524 09:19:12.551249   849 sgd_solver.cpp:106] Iteration 57, lr = 0.001
I0524 09:19:12.856160   849 solver.cpp:228] Iteration 58, loss = 2.4273
I0524 09:19:12.856220   849 solver.cpp:244]     Train net output #0: loss = 2.4273 (* 1 = 2.4273 loss)
I0524 09:19:12.856230   849 sgd_solver.cpp:106] Iteration 58, lr = 0.001
I0524 09:19:13.165920   849 solver.cpp:228] Iteration 59, loss = 2.55649
I0524 09:19:13.165966   849 solver.cpp:244]     Train net output #0: loss = 2.55649 (* 1 = 2.55649 loss)
I0524 09:19:13.165976   849 sgd_solver.cpp:106] Iteration 59, lr = 0.001
I0524 09:19:14.205380   849 solver.cpp:228] Iteration 60, loss = 2.33017
I0524 09:19:14.205425   849 solver.cpp:244]     Train net output #0: loss = 2.33017 (* 1 = 2.33017 loss)
I0524 09:19:14.205435   849 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I0524 09:19:15.389642   849 solver.cpp:228] Iteration 61, loss = 2.21495
I0524 09:19:15.389696   849 solver.cpp:244]     Train net output #0: loss = 2.21495 (* 1 = 2.21495 loss)
I0524 09:19:15.389706   849 sgd_solver.cpp:106] Iteration 61, lr = 0.001
I0524 09:19:16.230962   849 solver.cpp:228] Iteration 62, loss = 2.44127
I0524 09:19:16.231012   849 solver.cpp:244]     Train net output #0: loss = 2.44127 (* 1 = 2.44127 loss)
I0524 09:19:16.231021   849 sgd_solver.cpp:106] Iteration 62, lr = 0.001
I0524 09:19:17.141654   849 solver.cpp:228] Iteration 63, loss = 2.44057
I0524 09:19:17.141726   849 solver.cpp:244]     Train net output #0: loss = 2.44057 (* 1 = 2.44057 loss)
I0524 09:19:17.141743   849 sgd_solver.cpp:106] Iteration 63, lr = 0.001
I0524 09:19:18.123736   849 solver.cpp:228] Iteration 64, loss = 2.16732
I0524 09:19:18.123783   849 solver.cpp:244]     Train net output #0: loss = 2.16732 (* 1 = 2.16732 loss)
I0524 09:19:18.123793   849 sgd_solver.cpp:106] Iteration 64, lr = 0.001
I0524 09:19:19.351122   849 solver.cpp:228] Iteration 65, loss = 2.50087
I0524 09:19:19.351176   849 solver.cpp:244]     Train net output #0: loss = 2.50087 (* 1 = 2.50087 loss)
I0524 09:19:19.351187   849 sgd_solver.cpp:106] Iteration 65, lr = 0.001
I0524 09:19:19.934679   849 solver.cpp:228] Iteration 66, loss = 2.35595
I0524 09:19:19.934727   849 solver.cpp:244]     Train net output #0: loss = 2.35595 (* 1 = 2.35595 loss)
I0524 09:19:19.934736   849 sgd_solver.cpp:106] Iteration 66, lr = 0.001
I0524 09:19:20.264690   849 solver.cpp:228] Iteration 67, loss = 2.4671
I0524 09:19:20.264729   849 solver.cpp:244]     Train net output #0: loss = 2.4671 (* 1 = 2.4671 loss)
I0524 09:19:20.264778   849 sgd_solver.cpp:106] Iteration 67, lr = 0.001
I0524 09:19:20.687644   849 solver.cpp:228] Iteration 68, loss = 2.38283
I0524 09:19:20.687700   849 solver.cpp:244]     Train net output #0: loss = 2.38283 (* 1 = 2.38283 loss)
I0524 09:19:20.687711   849 sgd_solver.cpp:106] Iteration 68, lr = 0.001
I0524 09:19:21.860982   849 solver.cpp:228] Iteration 69, loss = 2.31455
I0524 09:19:21.861033   849 solver.cpp:244]     Train net output #0: loss = 2.31455 (* 1 = 2.31455 loss)
I0524 09:19:21.861047   849 sgd_solver.cpp:106] Iteration 69, lr = 0.001
I0524 09:19:23.520979   849 solver.cpp:228] Iteration 70, loss = 2.46086
I0524 09:19:23.521024   849 solver.cpp:244]     Train net output #0: loss = 2.46086 (* 1 = 2.46086 loss)
I0524 09:19:23.521034   849 sgd_solver.cpp:106] Iteration 70, lr = 0.001
I0524 09:19:24.026074   849 solver.cpp:228] Iteration 71, loss = 2.40073
I0524 09:19:24.026137   849 solver.cpp:244]     Train net output #0: loss = 2.40073 (* 1 = 2.40073 loss)
I0524 09:19:24.026152   849 sgd_solver.cpp:106] Iteration 71, lr = 0.001
I0524 09:19:25.171494   849 solver.cpp:228] Iteration 72, loss = 2.42663
I0524 09:19:25.171540   849 solver.cpp:244]     Train net output #0: loss = 2.42663 (* 1 = 2.42663 loss)
I0524 09:19:25.171550   849 sgd_solver.cpp:106] Iteration 72, lr = 0.001
I0524 09:19:26.610875   849 solver.cpp:228] Iteration 73, loss = 2.46644
I0524 09:19:26.610930   849 solver.cpp:244]     Train net output #0: loss = 2.46644 (* 1 = 2.46644 loss)
I0524 09:19:26.610947   849 sgd_solver.cpp:106] Iteration 73, lr = 0.001
I0524 09:19:28.059916   849 solver.cpp:228] Iteration 74, loss = 2.25046
I0524 09:19:28.059962   849 solver.cpp:244]     Train net output #0: loss = 2.25046 (* 1 = 2.25046 loss)
I0524 09:19:28.059975   849 sgd_solver.cpp:106] Iteration 74, lr = 0.001
I0524 09:19:29.298008   849 solver.cpp:228] Iteration 75, loss = 2.17235
I0524 09:19:29.298053   849 solver.cpp:244]     Train net output #0: loss = 2.17235 (* 1 = 2.17235 loss)
I0524 09:19:29.298066   849 sgd_solver.cpp:106] Iteration 75, lr = 0.001
I0524 09:19:30.230000   849 solver.cpp:228] Iteration 76, loss = 2.37101
I0524 09:19:30.230053   849 solver.cpp:244]     Train net output #0: loss = 2.37101 (* 1 = 2.37101 loss)
I0524 09:19:30.230067   849 sgd_solver.cpp:106] Iteration 76, lr = 0.001
I0524 09:19:30.546793   849 solver.cpp:228] Iteration 77, loss = 2.65505
I0524 09:19:30.546846   849 solver.cpp:244]     Train net output #0: loss = 2.65505 (* 1 = 2.65505 loss)
I0524 09:19:30.546857   849 sgd_solver.cpp:106] Iteration 77, lr = 0.001
I0524 09:19:31.124893   849 solver.cpp:228] Iteration 78, loss = 2.46857
I0524 09:19:31.124954   849 solver.cpp:244]     Train net output #0: loss = 2.46857 (* 1 = 2.46857 loss)
I0524 09:19:31.124963   849 sgd_solver.cpp:106] Iteration 78, lr = 0.001
I0524 09:19:31.643528   849 solver.cpp:228] Iteration 79, loss = 2.60033
I0524 09:19:31.643584   849 solver.cpp:244]     Train net output #0: loss = 2.60033 (* 1 = 2.60033 loss)
I0524 09:19:31.643594   849 sgd_solver.cpp:106] Iteration 79, lr = 0.001
I0524 09:19:32.401193   849 solver.cpp:228] Iteration 80, loss = 6.07547
I0524 09:19:32.401244   849 solver.cpp:244]     Train net output #0: loss = 6.07547 (* 1 = 6.07547 loss)
I0524 09:19:32.401252   849 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I0524 09:19:33.086791   849 solver.cpp:228] Iteration 81, loss = 2.58343
I0524 09:19:33.086848   849 solver.cpp:244]     Train net output #0: loss = 2.58343 (* 1 = 2.58343 loss)
I0524 09:19:33.086858   849 sgd_solver.cpp:106] Iteration 81, lr = 0.001
I0524 09:19:33.584782   849 solver.cpp:228] Iteration 82, loss = 2.3447
I0524 09:19:33.584839   849 solver.cpp:244]     Train net output #0: loss = 2.3447 (* 1 = 2.3447 loss)
I0524 09:19:33.584851   849 sgd_solver.cpp:106] Iteration 82, lr = 0.001
I0524 09:19:33.980087   849 solver.cpp:228] Iteration 83, loss = 2.54932
I0524 09:19:33.980129   849 solver.cpp:244]     Train net output #0: loss = 2.54932 (* 1 = 2.54932 loss)
I0524 09:19:33.980141   849 sgd_solver.cpp:106] Iteration 83, lr = 0.001
I0524 09:19:35.014263   849 solver.cpp:228] Iteration 84, loss = 2.33955
I0524 09:19:35.014304   849 solver.cpp:244]     Train net output #0: loss = 2.33955 (* 1 = 2.33955 loss)
I0524 09:19:35.014314   849 sgd_solver.cpp:106] Iteration 84, lr = 0.001
I0524 09:19:35.696156   849 solver.cpp:228] Iteration 85, loss = 2.4784
I0524 09:19:35.696199   849 solver.cpp:244]     Train net output #0: loss = 2.4784 (* 1 = 2.4784 loss)
I0524 09:19:35.696208   849 sgd_solver.cpp:106] Iteration 85, lr = 0.001
I0524 09:19:36.945719   849 solver.cpp:228] Iteration 86, loss = 2.41371
I0524 09:19:36.945766   849 solver.cpp:244]     Train net output #0: loss = 2.41371 (* 1 = 2.41371 loss)
I0524 09:19:36.945775   849 sgd_solver.cpp:106] Iteration 86, lr = 0.001
I0524 09:19:38.754623   849 solver.cpp:228] Iteration 87, loss = 2.51043
I0524 09:19:38.767109   849 solver.cpp:244]     Train net output #0: loss = 2.51043 (* 1 = 2.51043 loss)
I0524 09:19:38.767122   849 sgd_solver.cpp:106] Iteration 87, lr = 0.001
I0524 09:19:40.237443   849 solver.cpp:228] Iteration 88, loss = 2.41158
I0524 09:19:40.237489   849 solver.cpp:244]     Train net output #0: loss = 2.41158 (* 1 = 2.41158 loss)
I0524 09:19:40.237498   849 sgd_solver.cpp:106] Iteration 88, lr = 0.001
I0524 09:19:41.711832   849 solver.cpp:228] Iteration 89, loss = 2.38026
I0524 09:19:41.711899   849 solver.cpp:244]     Train net output #0: loss = 2.38026 (* 1 = 2.38026 loss)
I0524 09:19:41.711911   849 sgd_solver.cpp:106] Iteration 89, lr = 0.001
I0524 09:19:42.899315   849 solver.cpp:228] Iteration 90, loss = 2.37989
I0524 09:19:42.899370   849 solver.cpp:244]     Train net output #0: loss = 2.37989 (* 1 = 2.37989 loss)
I0524 09:19:42.899389   849 sgd_solver.cpp:106] Iteration 90, lr = 0.001
I0524 09:19:43.863060   849 solver.cpp:228] Iteration 91, loss = 5.92481
I0524 09:19:43.863118   849 solver.cpp:244]     Train net output #0: loss = 5.92481 (* 1 = 5.92481 loss)
I0524 09:19:43.863131   849 sgd_solver.cpp:106] Iteration 91, lr = 0.001
I0524 09:19:45.471724   849 solver.cpp:228] Iteration 92, loss = 2.23323
I0524 09:19:45.471786   849 solver.cpp:244]     Train net output #0: loss = 2.23323 (* 1 = 2.23323 loss)
I0524 09:19:45.471802   849 sgd_solver.cpp:106] Iteration 92, lr = 0.001
I0524 09:19:46.104126   849 solver.cpp:228] Iteration 93, loss = 5.81513
I0524 09:19:46.104198   849 solver.cpp:244]     Train net output #0: loss = 5.81513 (* 1 = 5.81513 loss)
I0524 09:19:46.104218   849 sgd_solver.cpp:106] Iteration 93, lr = 0.001
I0524 09:19:46.646842   849 solver.cpp:228] Iteration 94, loss = 6.06106
I0524 09:19:46.646919   849 solver.cpp:244]     Train net output #0: loss = 6.06106 (* 1 = 6.06106 loss)
I0524 09:19:46.646936   849 sgd_solver.cpp:106] Iteration 94, lr = 0.001
I0524 09:19:47.509944   849 solver.cpp:228] Iteration 95, loss = 2.66563
I0524 09:19:47.509994   849 solver.cpp:244]     Train net output #0: loss = 2.66563 (* 1 = 2.66563 loss)
I0524 09:19:47.510004   849 sgd_solver.cpp:106] Iteration 95, lr = 0.001
I0524 09:19:48.501333   849 solver.cpp:228] Iteration 96, loss = 2.28693
I0524 09:19:48.501384   849 solver.cpp:244]     Train net output #0: loss = 2.28693 (* 1 = 2.28693 loss)
I0524 09:19:48.501396   849 sgd_solver.cpp:106] Iteration 96, lr = 0.001
I0524 09:19:49.592741   849 solver.cpp:228] Iteration 97, loss = 2.47987
I0524 09:19:49.592788   849 solver.cpp:244]     Train net output #0: loss = 2.47987 (* 1 = 2.47987 loss)
I0524 09:19:49.592798   849 sgd_solver.cpp:106] Iteration 97, lr = 0.001
I0524 09:19:50.259124   849 solver.cpp:228] Iteration 98, loss = 5.85447
I0524 09:19:50.259168   849 solver.cpp:244]     Train net output #0: loss = 5.85447 (* 1 = 5.85447 loss)
I0524 09:19:50.259177   849 sgd_solver.cpp:106] Iteration 98, lr = 0.001
I0524 09:19:52.239941   849 solver.cpp:228] Iteration 99, loss = 2.52713
I0524 09:19:52.240006   849 solver.cpp:244]     Train net output #0: loss = 2.52713 (* 1 = 2.52713 loss)
I0524 09:19:52.240021   849 sgd_solver.cpp:106] Iteration 99, lr = 0.001
I0524 09:19:53.636116   849 solver.cpp:228] Iteration 100, loss = 2.34894
I0524 09:19:53.636175   849 solver.cpp:244]     Train net output #0: loss = 2.34894 (* 1 = 2.34894 loss)
I0524 09:19:53.636188   849 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0524 09:19:54.842618   849 solver.cpp:228] Iteration 101, loss = 5.97237
I0524 09:19:54.842682   849 solver.cpp:244]     Train net output #0: loss = 5.97237 (* 1 = 5.97237 loss)
I0524 09:19:54.842695   849 sgd_solver.cpp:106] Iteration 101, lr = 0.001
I0524 09:19:55.335942   849 solver.cpp:228] Iteration 102, loss = 2.42174
I0524 09:19:55.335986   849 solver.cpp:244]     Train net output #0: loss = 2.42174 (* 1 = 2.42174 loss)
I0524 09:19:55.335995   849 sgd_solver.cpp:106] Iteration 102, lr = 0.001
I0524 09:19:55.803460   849 solver.cpp:228] Iteration 103, loss = 2.3558
I0524 09:19:55.803519   849 solver.cpp:244]     Train net output #0: loss = 2.3558 (* 1 = 2.3558 loss)
I0524 09:19:55.803575   849 sgd_solver.cpp:106] Iteration 103, lr = 0.001
I0524 09:19:56.631235   849 solver.cpp:228] Iteration 104, loss = 2.3405
I0524 09:19:56.631300   849 solver.cpp:244]     Train net output #0: loss = 2.3405 (* 1 = 2.3405 loss)
I0524 09:19:56.631312   849 sgd_solver.cpp:106] Iteration 104, lr = 0.001
I0524 09:19:57.354519   849 solver.cpp:228] Iteration 105, loss = 2.40236
I0524 09:19:57.354562   849 solver.cpp:244]     Train net output #0: loss = 2.40236 (* 1 = 2.40236 loss)
I0524 09:19:57.354573   849 sgd_solver.cpp:106] Iteration 105, lr = 0.001
I0524 09:19:58.417861   849 solver.cpp:228] Iteration 106, loss = 5.86305
I0524 09:19:58.417907   849 solver.cpp:244]     Train net output #0: loss = 5.86305 (* 1 = 5.86305 loss)
I0524 09:19:58.417918   849 sgd_solver.cpp:106] Iteration 106, lr = 0.001
I0524 09:19:59.133359   849 solver.cpp:228] Iteration 107, loss = 2.28652
I0524 09:19:59.133400   849 solver.cpp:244]     Train net output #0: loss = 2.28652 (* 1 = 2.28652 loss)
I0524 09:19:59.133410   849 sgd_solver.cpp:106] Iteration 107, lr = 0.001
